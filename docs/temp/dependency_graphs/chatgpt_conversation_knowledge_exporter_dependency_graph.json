{
  "apps.review-api.src.review_api.addons.Addon": {
    "id": "apps.review-api.src.review_api.addons.Addon",
    "name": "Addon",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [],
    "source_code": "class Addon(ABC):\n    \"\"\"Base class for addons.\"\"\"\n\n    @property\n    @abstractmethod\n    def addon_id(self) -> str:\n        \"\"\"Unique identifier for this addon.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Human-readable name.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def description(self) -> str:\n        \"\"\"Description of what this addon does.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def capabilities(self) -> List[str]:\n        \"\"\"List of capabilities: 'exporter', 'panel', etc.\"\"\"\n        pass",
    "start_line": 15,
    "end_line": 40,
    "has_docstring": true,
    "docstring": "Base class for addons.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "ABC"
    ],
    "class_name": null,
    "display_name": "class Addon",
    "component_id": "apps.review-api.src.review_api.addons.Addon"
  },
  "apps.review-api.src.review_api.addons.ExporterAddon": {
    "id": "apps.review-api.src.review_api.addons.ExporterAddon",
    "name": "ExporterAddon",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.Addon"
    ],
    "source_code": "class ExporterAddon(Addon):\n    \"\"\"Addon that exports additional files for bundles.\"\"\"\n\n    @abstractmethod\n    def export_topic(self, topic_id: int, store: KnowledgeStore) -> Dict[str, bytes]:\n        \"\"\"\n        Export files for a topic.\n\n        Returns:\n            Dict mapping file paths (relative to bundle root) to file contents (bytes)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def export_conversation(self, conversation_id: str, store: KnowledgeStore) -> Dict[str, bytes]:\n        \"\"\"\n        Export files for a conversation.\n\n        Returns:\n            Dict mapping file paths (relative to bundle root) to file contents (bytes)\n        \"\"\"\n        pass",
    "start_line": 43,
    "end_line": 64,
    "has_docstring": true,
    "docstring": "Addon that exports additional files for bundles.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Addon"
    ],
    "class_name": null,
    "display_name": "class ExporterAddon",
    "component_id": "apps.review-api.src.review_api.addons.ExporterAddon"
  },
  "apps.review-api.src.review_api.addons.PanelAddon": {
    "id": "apps.review-api.src.review_api.addons.PanelAddon",
    "name": "PanelAddon",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.Addon"
    ],
    "source_code": "class PanelAddon(Addon):\n    \"\"\"Addon that provides UI panel data.\"\"\"\n\n    @abstractmethod\n    def get_panel_data_topic(self, topic_id: int, store: KnowledgeStore) -> Dict[str, Any]:\n        \"\"\"Get panel data for a topic.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_panel_data_conversation(self, conversation_id: str, store: KnowledgeStore) -> Dict[str, Any]:\n        \"\"\"Get panel data for a conversation.\"\"\"\n        pass",
    "start_line": 67,
    "end_line": 78,
    "has_docstring": true,
    "docstring": "Addon that provides UI panel data.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Addon"
    ],
    "class_name": null,
    "display_name": "class PanelAddon",
    "component_id": "apps.review-api.src.review_api.addons.PanelAddon"
  },
  "apps.review-api.src.review_api.addons.TopicBriefExporter": {
    "id": "apps.review-api.src.review_api.addons.TopicBriefExporter",
    "name": "TopicBriefExporter",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.ExporterAddon"
    ],
    "source_code": "class TopicBriefExporter(ExporterAddon):\n    \"\"\"Export a markdown brief summarizing a topic.\"\"\"\n\n    @property\n    def addon_id(self) -> str:\n        return \"topic-brief\"\n\n    @property\n    def name(self) -> str:\n        return \"Topic Brief\"\n\n    @property\n    def description(self) -> str:\n        return \"Generate a markdown brief summarizing the topic and its conversations\"\n\n    @property\n    def capabilities(self) -> List[str]:\n        return [\"exporter\"]\n\n    def export_topic(self, topic_id: int, store: KnowledgeStore) -> Dict[str, bytes]:\n        \"\"\"Generate topic brief.\"\"\"\n        topic = store.topics.get(topic_id)\n        if not topic:\n            return {}\n\n        conv_ids = store.get_topic_conversation_ids(topic_id)\n        conversations = []\n        total_atoms = 0\n\n        for conv_id in conv_ids:\n            assignment = store.assignments.get(conv_id)\n            if assignment:\n                conversations.append(assignment)\n                total_atoms += assignment.atom_count\n\n        brief = f\"\"\"# Topic: {topic.name}\n\n## Description\n\n{topic.description}\n\n## Keywords\n\n{', '.join(topic.keywords[:10])}\n\n## Statistics\n\n- **Conversations**: {len(conversations)}\n- **Total Atoms**: {total_atoms}\n- **Representative Conversations**: {len(topic.representative_conversations)}\n\n## Conversations\n\n\"\"\"\n        for conv in conversations[:20]:  # Limit to first 20\n            brief += f\"- **{conv.title}** ({conv.atom_count} atoms)\\n\"\n            if conv.project_name:\n                brief += f\"  - Project: {conv.project_name}\\n\"\n            brief += \"\\n\"\n\n        return {\"topic-brief.md\": brief.encode(\"utf-8\")}\n\n    def export_conversation(self, conversation_id: str, store: KnowledgeStore) -> Dict[str, bytes]:\n        \"\"\"Not applicable for conversation exports.\"\"\"\n        return {}",
    "start_line": 81,
    "end_line": 145,
    "has_docstring": true,
    "docstring": "Export a markdown brief summarizing a topic.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "ExporterAddon"
    ],
    "class_name": null,
    "display_name": "class TopicBriefExporter",
    "component_id": "apps.review-api.src.review_api.addons.TopicBriefExporter"
  },
  "apps.review-api.src.review_api.addons.AtomsCSVExporter": {
    "id": "apps.review-api.src.review_api.addons.AtomsCSVExporter",
    "name": "AtomsCSVExporter",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.ExporterAddon"
    ],
    "source_code": "class AtomsCSVExporter(ExporterAddon):\n    \"\"\"Export atoms as CSV.\"\"\"\n\n    @property\n    def addon_id(self) -> str:\n        return \"atoms-csv\"\n\n    @property\n    def name(self) -> str:\n        return \"Atoms CSV\"\n\n    @property\n    def description(self) -> str:\n        return \"Export atoms as CSV for spreadsheet analysis\"\n\n    @property\n    def capabilities(self) -> List[str]:\n        return [\"exporter\"]\n\n    def export_topic(self, topic_id: int, store: KnowledgeStore) -> Dict[str, bytes]:\n        \"\"\"Export all atoms from topic conversations as CSV.\"\"\"\n        import csv\n        import io\n\n        conv_ids = store.get_topic_conversation_ids(topic_id)\n        all_atoms = []\n\n        for conv_id in conv_ids:\n            detail = store.get_conversation_detail(conv_id)\n            if detail:\n                for atom in detail.get(\"facts\", []) + detail.get(\"decisions\", []) + detail.get(\"questions\", []):\n                    all_atoms.append({\n                        \"conversation_id\": conv_id,\n                        \"type\": atom.get(\"type\", \"\"),\n                        \"topic\": atom.get(\"topic\", \"\"),\n                        \"statement\": atom.get(\"statement\", atom.get(\"question\", \"\")),\n                        \"status\": atom.get(\"status\", \"\"),\n                        \"extracted_at\": atom.get(\"extracted_at\", \"\"),\n                    })\n\n        output = io.StringIO()\n        if all_atoms:\n            writer = csv.DictWriter(output, fieldnames=[\"conversation_id\", \"type\", \"topic\", \"statement\", \"status\", \"extracted_at\"])\n            writer.writeheader()\n            writer.writerows(all_atoms)\n\n        return {\"atoms.csv\": output.getvalue().encode(\"utf-8\")}\n\n    def export_conversation(self, conversation_id: str, store: KnowledgeStore) -> Dict[str, bytes]:\n        \"\"\"Export conversation atoms as CSV.\"\"\"\n        import csv\n        import io\n\n        detail = store.get_conversation_detail(conversation_id)\n        if not detail:\n            return {}\n\n        all_atoms = []\n        for atom in detail.get(\"facts\", []) + detail.get(\"decisions\", []) + detail.get(\"questions\", []):\n            all_atoms.append({\n                \"type\": atom.get(\"type\", \"\"),\n                \"topic\": atom.get(\"topic\", \"\"),\n                \"statement\": atom.get(\"statement\", atom.get(\"question\", \"\")),\n                \"status\": atom.get(\"status\", \"\"),\n                \"extracted_at\": atom.get(\"extracted_at\", \"\"),\n            })\n\n        output = io.StringIO()\n        if all_atoms:\n            writer = csv.DictWriter(output, fieldnames=[\"type\", \"topic\", \"statement\", \"status\", \"extracted_at\"])\n            writer.writeheader()\n            writer.writerows(all_atoms)\n\n        return {\"atoms.csv\": output.getvalue().encode(\"utf-8\")}",
    "start_line": 148,
    "end_line": 221,
    "has_docstring": true,
    "docstring": "Export atoms as CSV.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "ExporterAddon"
    ],
    "class_name": null,
    "display_name": "class AtomsCSVExporter",
    "component_id": "apps.review-api.src.review_api.addons.AtomsCSVExporter"
  },
  "apps.review-api.src.review_api.addons.TopicStatsPanel": {
    "id": "apps.review-api.src.review_api.addons.TopicStatsPanel",
    "name": "TopicStatsPanel",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.PanelAddon"
    ],
    "source_code": "class TopicStatsPanel(PanelAddon):\n    \"\"\"Panel showing topic statistics.\"\"\"\n\n    @property\n    def addon_id(self) -> str:\n        return \"topic-stats\"\n\n    @property\n    def name(self) -> str:\n        return \"Topic Statistics\"\n\n    @property\n    def description(self) -> str:\n        return \"Display statistics and aggregations for a topic\"\n\n    @property\n    def capabilities(self) -> List[str]:\n        return [\"panel\"]\n\n    def get_panel_data_topic(self, topic_id: int, store: KnowledgeStore) -> Dict[str, Any]:\n        \"\"\"Get stats for a topic.\"\"\"\n        topic = store.topics.get(topic_id)\n        if not topic:\n            return {}\n\n        conv_ids = store.get_topic_conversation_ids(topic_id)\n        atom_types = {}\n        atom_topics = {}\n        status_counts = {\"active\": 0, \"deprecated\": 0, \"uncertain\": 0}\n\n        for conv_id in conv_ids:\n            detail = store.get_conversation_detail(conv_id)\n            if detail:\n                for atom in detail.get(\"facts\", []) + detail.get(\"decisions\", []) + detail.get(\"questions\", []):\n                    atom_type = atom.get(\"type\", \"unknown\")\n                    atom_types[atom_type] = atom_types.get(atom_type, 0) + 1\n                    atom_topic = atom.get(\"topic\", \"uncategorized\")\n                    atom_topics[atom_topic] = atom_topics.get(atom_topic, 0) + 1\n                    status = atom.get(\"status\", \"active\")\n                    if status in status_counts:\n                        status_counts[status] += 1\n\n        return {\n            \"conversation_count\": len(conv_ids),\n            \"atom_type_distribution\": atom_types,\n            \"atom_topic_distribution\": dict(sorted(atom_topics.items(), key=lambda x: x[1], reverse=True)[:10]),\n            \"status_distribution\": status_counts,\n        }\n\n    def get_panel_data_conversation(self, conversation_id: str, store: KnowledgeStore) -> Dict[str, Any]:\n        \"\"\"Not applicable for conversations.\"\"\"\n        return {}",
    "start_line": 224,
    "end_line": 275,
    "has_docstring": true,
    "docstring": "Panel showing topic statistics.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "PanelAddon"
    ],
    "class_name": null,
    "display_name": "class TopicStatsPanel",
    "component_id": "apps.review-api.src.review_api.addons.TopicStatsPanel"
  },
  "apps.review-api.src.review_api.addons.AddonRegistry": {
    "id": "apps.review-api.src.review_api.addons.AddonRegistry",
    "name": "AddonRegistry",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/addons.py",
    "relative_path": "apps/review-api/src/review_api/addons.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.AtomsCSVExporter",
      "apps.review-api.src.review_api.addons.TopicStatsPanel",
      "apps.review-api.src.review_api.addons.TopicBriefExporter"
    ],
    "source_code": "class AddonRegistry:\n    \"\"\"Registry of available addons.\"\"\"\n\n    def __init__(self):\n        self.addons: Dict[str, Addon] = {}\n        self._register_builtin()\n\n    def _register_builtin(self) -> None:\n        \"\"\"Register built-in addons.\"\"\"\n        self.register(TopicBriefExporter())\n        self.register(AtomsCSVExporter())\n        self.register(TopicStatsPanel())\n\n    def register(self, addon: Addon) -> None:\n        \"\"\"Register an addon.\"\"\"\n        self.addons[addon.addon_id] = addon\n\n    def get(self, addon_id: str) -> Optional[Addon]:\n        \"\"\"Get an addon by ID.\"\"\"\n        return self.addons.get(addon_id)\n\n    def list_all(self) -> List[Dict[str, Any]]:\n        \"\"\"List all registered addons.\"\"\"\n        return [\n            {\n                \"addon_id\": addon.addon_id,\n                \"name\": addon.name,\n                \"description\": addon.description,\n                \"capabilities\": addon.capabilities,\n            }\n            for addon in self.addons.values()\n        ]\n\n    def get_exporters(self) -> List[ExporterAddon]:\n        \"\"\"Get all exporter addons.\"\"\"\n        return [addon for addon in self.addons.values() if isinstance(addon, ExporterAddon)]\n\n    def get_panels(self) -> List[PanelAddon]:\n        \"\"\"Get all panel addons.\"\"\"\n        return [addon for addon in self.addons.values() if isinstance(addon, PanelAddon)]",
    "start_line": 278,
    "end_line": 317,
    "has_docstring": true,
    "docstring": "Registry of available addons.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AddonRegistry",
    "component_id": "apps.review-api.src.review_api.addons.AddonRegistry"
  },
  "apps.review-api.src.review_api.bundler.ZipBundler": {
    "id": "apps.review-api.src.review_api.bundler.ZipBundler",
    "name": "ZipBundler",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/bundler.py",
    "relative_path": "apps/review-api/src/review_api/bundler.py",
    "depends_on": [
      "apps.review-api.src.review_api.addons.AddonRegistry"
    ],
    "source_code": "class ZipBundler:\n    \"\"\"Generate zip bundles for topics and conversations.\"\"\"\n\n    def __init__(self, store: KnowledgeStore, addon_registry: Optional[AddonRegistry] = None):\n        self.store = store\n        self.addon_registry = addon_registry or AddonRegistry()\n\n    def bundle_conversation(self, conversation_id: str) -> Optional[bytes]:\n        \"\"\"Create a zip bundle for a single conversation.\"\"\"\n        assignment = self.store.assignments.get(conversation_id)\n        if not assignment:\n            return None\n\n        import io\n\n        zip_buffer = io.BytesIO()\n        with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n            # Add manifest\n            manifest = {\n                \"kind\": \"conversation\",\n                \"conversation_id\": conversation_id,\n                \"title\": assignment.title,\n                \"project_id\": assignment.project_id,\n                \"project_name\": assignment.project_name,\n                \"topics\": [t.model_dump() for t in assignment.topics],\n                \"atom_count\": assignment.atom_count,\n                \"review_flag\": assignment.review_flag,\n                \"generated_at\": datetime.now().isoformat(),\n            }\n            zip_file.writestr(\"manifest.json\", json.dumps(manifest, indent=2))\n\n            # Get file paths\n            paths = self.store.get_conversation_file_paths(conversation_id)\n\n            # Add docs\n            if \"docs\" in paths:\n                docs_dir = paths[\"docs\"]\n                for md_file in docs_dir.glob(\"*.md\"):\n                    arc_name = f\"docs/{md_file.name}\"\n                    zip_file.write(md_file, arc_name)\n\n            # Add ADRs\n            if \"adrs\" in paths:\n                adr_dir = paths[\"adrs\"]\n                for md_file in adr_dir.glob(\"*.md\"):\n                    arc_name = f\"adrs/{md_file.name}\"\n                    zip_file.write(md_file, arc_name)\n\n            # Add atoms\n            if \"atoms\" in paths:\n                atoms_dir = paths[\"atoms\"]\n                for jsonl_file in atoms_dir.glob(\"*.jsonl\"):\n                    arc_name = f\"atoms/{jsonl_file.name}\"\n                    zip_file.write(jsonl_file, arc_name)\n            elif \"atoms_consolidated\" in paths:\n                # Slice from consolidated files\n                consolidated_dir = paths[\"atoms_consolidated\"]\n                for file_name in [\"atoms.jsonl\", \"decisions.jsonl\", \"open_questions.jsonl\"]:\n                    source_file = consolidated_dir / file_name\n                    if source_file.exists():\n                        # Filter by conversation_id\n                        filtered_lines = []\n                        with source_file.open(\"r\", encoding=\"utf-8\") as f:\n                            for line in f:\n                                line = line.strip()\n                                if not line:\n                                    continue\n                                try:\n                                    obj = json.loads(line)\n                                    if obj.get(\"source_conversation_id\") == conversation_id:\n                                        filtered_lines.append(line)\n                                except json.JSONDecodeError:\n                                    continue\n                        if filtered_lines:\n                            arc_name = f\"atoms/{file_name}\"\n                            zip_file.writestr(arc_name, \"\\n\".join(filtered_lines) + \"\\n\")\n\n            # Add evidence\n            if \"evidence\" in paths:\n                evidence_file = paths[\"evidence\"]\n                zip_file.write(evidence_file, \"evidence/conversation.md\")\n\n            # Add addon exports\n            for exporter in self.addon_registry.get_exporters():\n                try:\n                    addon_files = exporter.export_conversation(conversation_id, self.store)\n                    for file_path, content in addon_files.items():\n                        zip_file.writestr(f\"addons/{exporter.addon_id}/{file_path}\", content)\n                except Exception as e:\n                    # Skip addon if it fails\n                    continue\n\n        zip_buffer.seek(0)\n        return zip_buffer.read()\n\n    def bundle_topic(self, topic_id: int) -> Optional[bytes]:\n        \"\"\"Create a zip bundle for a topic (all primary conversations).\"\"\"\n        topic = self.store.topics.get(topic_id)\n        if not topic:\n            return None\n\n        conv_ids = self.store.get_topic_conversation_ids(topic_id)\n        if not conv_ids:\n            return None\n\n        import io\n\n        zip_buffer = io.BytesIO()\n        with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n            # Add manifest\n            manifest = {\n                \"kind\": \"topic\",\n                \"topic_id\": topic_id,\n                \"topic_name\": topic.name,\n                \"topic_description\": topic.description,\n                \"keywords\": topic.keywords,\n                \"conversation_count\": len(conv_ids),\n                \"conversation_ids\": conv_ids,\n                \"generated_at\": datetime.now().isoformat(),\n            }\n            zip_file.writestr(\"manifest.json\", json.dumps(manifest, indent=2))\n\n            # Bundle each conversation\n            for conv_id in conv_ids:\n                conv_zip = self.bundle_conversation(conv_id)\n                if conv_zip:\n                    # Extract and add to main zip\n                    import io as io_module\n                    import zipfile as zipfile_module\n\n                    conv_zip_buffer = io_module.BytesIO(conv_zip)\n                    with zipfile_module.ZipFile(conv_zip_buffer, \"r\") as conv_zip_file:\n                        for item in conv_zip_file.namelist():\n                            # Prefix with conversation_id\n                            new_name = f\"conversations/{conv_id}/{item}\"\n                            zip_file.writestr(new_name, conv_zip_file.read(item))\n\n            # Add topic-level addon exports\n            for exporter in self.addon_registry.get_exporters():\n                try:\n                    addon_files = exporter.export_topic(topic_id, self.store)\n                    for file_path, content in addon_files.items():\n                        zip_file.writestr(f\"addons/{exporter.addon_id}/{file_path}\", content)\n                except Exception as e:\n                    # Skip addon if it fails\n                    continue\n\n        zip_buffer.seek(0)\n        return zip_buffer.read()",
    "start_line": 17,
    "end_line": 165,
    "has_docstring": true,
    "docstring": "Generate zip bundles for topics and conversations.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ZipBundler",
    "component_id": "apps.review-api.src.review_api.bundler.ZipBundler"
  },
  "apps.review-api.src.review_api.main.list_topics": {
    "id": "apps.review-api.src.review_api.main.list_topics",
    "name": "list_topics",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def list_topics():\n    \"\"\"List all topics with counts.\"\"\"\n    return {\"topics\": store.get_topics_summary()}",
    "start_line": 44,
    "end_line": 46,
    "has_docstring": true,
    "docstring": "List all topics with counts.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function list_topics",
    "component_id": "apps.review-api.src.review_api.main.list_topics"
  },
  "apps.review-api.src.review_api.main.get_topic": {
    "id": "apps.review-api.src.review_api.main.get_topic",
    "name": "get_topic",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def get_topic(topic_id: int):\n    \"\"\"Get topic details with conversations.\"\"\"\n    detail = store.get_topic_detail(topic_id)\n    if not detail:\n        raise HTTPException(status_code=404, detail=\"Topic not found\")\n    return detail",
    "start_line": 50,
    "end_line": 55,
    "has_docstring": true,
    "docstring": "Get topic details with conversations.",
    "parameters": [
      "topic_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_topic",
    "component_id": "apps.review-api.src.review_api.main.get_topic"
  },
  "apps.review-api.src.review_api.main.get_conversation": {
    "id": "apps.review-api.src.review_api.main.get_conversation",
    "name": "get_conversation",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def get_conversation(conversation_id: str):\n    \"\"\"Get conversation details with atoms and docs.\"\"\"\n    detail = store.get_conversation_detail(conversation_id)\n    if not detail:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n    return detail",
    "start_line": 59,
    "end_line": 64,
    "has_docstring": true,
    "docstring": "Get conversation details with atoms and docs.",
    "parameters": [
      "conversation_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_conversation",
    "component_id": "apps.review-api.src.review_api.main.get_conversation"
  },
  "apps.review-api.src.review_api.main.get_doc": {
    "id": "apps.review-api.src.review_api.main.get_doc",
    "name": "get_doc",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def get_doc(conversation_id: str, doc_name: str):\n    \"\"\"Get markdown content for a doc.\"\"\"\n    content = store.get_doc_content(conversation_id, doc_name)\n    if content is None:\n        raise HTTPException(status_code=404, detail=\"Doc not found\")\n    return Response(content=content, media_type=\"text/markdown\")",
    "start_line": 68,
    "end_line": 73,
    "has_docstring": true,
    "docstring": "Get markdown content for a doc.",
    "parameters": [
      "conversation_id",
      "doc_name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_doc",
    "component_id": "apps.review-api.src.review_api.main.get_doc"
  },
  "apps.review-api.src.review_api.main.download_topic_bundle": {
    "id": "apps.review-api.src.review_api.main.download_topic_bundle",
    "name": "download_topic_bundle",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def download_topic_bundle(topic_id: int):\n    \"\"\"Download zip bundle for a topic.\"\"\"\n    zip_data = bundler.bundle_topic(topic_id)\n    if not zip_data:\n        raise HTTPException(status_code=404, detail=\"Topic not found or no conversations\")\n\n    topic = store.topics.get(topic_id)\n    filename = f\"topic-{topic_id}-{topic.name.replace(' ', '-') if topic else 'unknown'}.zip\"\n\n    return StreamingResponse(\n        iter([zip_data]),\n        media_type=\"application/zip\",\n        headers={\"Content-Disposition\": f'attachment; filename=\"{filename}\"'},\n    )",
    "start_line": 77,
    "end_line": 90,
    "has_docstring": true,
    "docstring": "Download zip bundle for a topic.",
    "parameters": [
      "topic_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function download_topic_bundle",
    "component_id": "apps.review-api.src.review_api.main.download_topic_bundle"
  },
  "apps.review-api.src.review_api.main.download_conversation_bundle": {
    "id": "apps.review-api.src.review_api.main.download_conversation_bundle",
    "name": "download_conversation_bundle",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def download_conversation_bundle(conversation_id: str):\n    \"\"\"Download zip bundle for a conversation.\"\"\"\n    zip_data = bundler.bundle_conversation(conversation_id)\n    if not zip_data:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    assignment = store.assignments.get(conversation_id)\n    title_safe = (assignment.title if assignment else conversation_id).replace(\" \", \"-\")[:50]\n    filename = f\"conversation-{conversation_id[:8]}-{title_safe}.zip\"\n\n    return StreamingResponse(\n        iter([zip_data]),\n        media_type=\"application/zip\",\n        headers={\"Content-Disposition\": f'attachment; filename=\"{filename}\"'},\n    )",
    "start_line": 94,
    "end_line": 108,
    "has_docstring": true,
    "docstring": "Download zip bundle for a conversation.",
    "parameters": [
      "conversation_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function download_conversation_bundle",
    "component_id": "apps.review-api.src.review_api.main.download_conversation_bundle"
  },
  "apps.review-api.src.review_api.main.get_review_queue": {
    "id": "apps.review-api.src.review_api.main.get_review_queue",
    "name": "get_review_queue",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [
      "apps.review-api.src.review_api.main.get_review_queue"
    ],
    "source_code": "async def get_review_queue():\n    \"\"\"Get review queue items.\"\"\"\n    return {\"items\": store.get_review_queue()}",
    "start_line": 112,
    "end_line": 114,
    "has_docstring": true,
    "docstring": "Get review queue items.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_review_queue",
    "component_id": "apps.review-api.src.review_api.main.get_review_queue"
  },
  "apps.review-api.src.review_api.main.get_conversation_atoms": {
    "id": "apps.review-api.src.review_api.main.get_conversation_atoms",
    "name": "get_conversation_atoms",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def get_conversation_atoms(\n    conversation_id: str,\n    atom_type: str | None = None,\n    status: str | None = None,\n    atom_topic: str | None = None,\n    page: int = 1,\n    page_size: int = 50,\n):\n    \"\"\"Get filtered atoms for a conversation with pagination.\"\"\"\n    detail = store.get_conversation_detail(conversation_id)\n    if not detail:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    # Combine all atoms\n    all_atoms = detail[\"facts\"] + detail[\"decisions\"] + detail[\"questions\"]\n\n    # Apply filters\n    filtered = all_atoms\n    if atom_type:\n        filtered = [a for a in filtered if a.get(\"type\") == atom_type]\n    if status:\n        filtered = [a for a in filtered if a.get(\"status\") == status]\n    if atom_topic:\n        filtered = [a for a in filtered if a.get(\"topic\") == atom_topic]\n\n    # Paginate\n    start = (page - 1) * page_size\n    end = start + page_size\n    paginated = filtered[start:end]\n\n    return {\n        \"atoms\": paginated,\n        \"total\": len(filtered),\n        \"page\": page,\n        \"page_size\": page_size,\n        \"total_pages\": (len(filtered) + page_size - 1) // page_size,\n    }",
    "start_line": 118,
    "end_line": 154,
    "has_docstring": true,
    "docstring": "Get filtered atoms for a conversation with pagination.",
    "parameters": [
      "conversation_id",
      "atom_type",
      "status",
      "atom_topic",
      "page",
      "page_size"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_conversation_atoms",
    "component_id": "apps.review-api.src.review_api.main.get_conversation_atoms"
  },
  "apps.review-api.src.review_api.main.export_selected_atoms": {
    "id": "apps.review-api.src.review_api.main.export_selected_atoms",
    "name": "export_selected_atoms",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def export_selected_atoms(conversation_id: str, atom_ids: list[str] | None = None):\n    \"\"\"Export selected atoms as JSONL.\"\"\"\n    detail = store.get_conversation_detail(conversation_id)\n    if not detail:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    # Combine all atoms\n    all_atoms = detail[\"facts\"] + detail[\"decisions\"] + detail[\"questions\"]\n\n    # Filter by IDs if provided\n    if atom_ids:\n        # Create a simple ID based on index for now (could be improved with actual IDs)\n        selected = []\n        for idx, atom in enumerate(all_atoms):\n            atom_id = f\"{conversation_id}-{idx}\"\n            if atom_id in atom_ids:\n                selected.append(atom)\n        atoms_to_export = selected\n    else:\n        atoms_to_export = all_atoms\n\n    # Convert to JSONL\n    jsonl_lines = [json.dumps(atom, ensure_ascii=False) for atom in atoms_to_export]\n    jsonl_content = \"\\n\".join(jsonl_lines)\n\n    return Response(\n        content=jsonl_content,\n        media_type=\"application/x-ndjson\",\n        headers={\n            \"Content-Disposition\": f'attachment; filename=\"atoms-{conversation_id[:8]}.jsonl\"'\n        },\n    )",
    "start_line": 158,
    "end_line": 189,
    "has_docstring": true,
    "docstring": "Export selected atoms as JSONL.",
    "parameters": [
      "conversation_id",
      "atom_ids"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function export_selected_atoms",
    "component_id": "apps.review-api.src.review_api.main.export_selected_atoms"
  },
  "apps.review-api.src.review_api.main.search": {
    "id": "apps.review-api.src.review_api.main.search",
    "name": "search",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [
      "apps.review-api.src.review_api.main.search"
    ],
    "source_code": "async def search(q: str = Query(..., min_length=1), limit: int = Query(20, ge=1, le=100)):\n    \"\"\"Full-text search across topics, conversations, atoms, and docs.\"\"\"\n    results = search_index.search(q, limit=limit)\n    return results",
    "start_line": 193,
    "end_line": 196,
    "has_docstring": true,
    "docstring": "Full-text search across topics, conversations, atoms, and docs.",
    "parameters": [
      "q",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function search",
    "component_id": "apps.review-api.src.review_api.main.search"
  },
  "apps.review-api.src.review_api.main.list_addons": {
    "id": "apps.review-api.src.review_api.main.list_addons",
    "name": "list_addons",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def list_addons():\n    \"\"\"List all available addons.\"\"\"\n    return {\"addons\": addon_registry.list_all()}",
    "start_line": 200,
    "end_line": 202,
    "has_docstring": true,
    "docstring": "List all available addons.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function list_addons",
    "component_id": "apps.review-api.src.review_api.main.list_addons"
  },
  "apps.review-api.src.review_api.main.run_addon": {
    "id": "apps.review-api.src.review_api.main.run_addon",
    "name": "run_addon",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def run_addon(addon_id: str, target: Dict[str, Any]):\n    \"\"\"Run an addon on a topic or conversation.\"\"\"\n    addon = addon_registry.get(addon_id)\n    if not addon:\n        raise HTTPException(status_code=404, detail=\"Addon not found\")\n\n    kind = target.get(\"kind\")\n    target_id = target.get(\"id\")\n\n    if kind == \"topic\":\n        if not isinstance(addon, PanelAddon):\n            raise HTTPException(status_code=400, detail=\"Addon does not support topic panels\")\n        data = addon.get_panel_data_topic(int(target_id), store)\n        return {\"addon_id\": addon_id, \"data\": data}\n    elif kind == \"conversation\":\n        if not isinstance(addon, PanelAddon):\n            raise HTTPException(status_code=400, detail=\"Addon does not support conversation panels\")\n        data = addon.get_panel_data_conversation(str(target_id), store)\n        return {\"addon_id\": addon_id, \"data\": data}\n    else:\n        raise HTTPException(status_code=400, detail=\"Invalid target kind\")",
    "start_line": 206,
    "end_line": 226,
    "has_docstring": true,
    "docstring": "Run an addon on a topic or conversation.",
    "parameters": [
      "addon_id",
      "target"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function run_addon",
    "component_id": "apps.review-api.src.review_api.main.run_addon"
  },
  "apps.review-api.src.review_api.main.refresh_index": {
    "id": "apps.review-api.src.review_api.main.refresh_index",
    "name": "refresh_index",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [
      "apps.review-api.src.review_api.store.KnowledgeStore",
      "apps.review-api.src.review_api.bundler.ZipBundler",
      "apps.review-api.src.review_api.search.SearchIndex"
    ],
    "source_code": "async def refresh_index():\n    \"\"\"Rebuild search index and reload store data.\"\"\"\n    global store, search_index, bundler\n    store = KnowledgeStore(BASE_PATH)\n    bundler = ZipBundler(store, addon_registry)\n    search_index = SearchIndex(store)\n    search_index.build_index()\n    return {\n        \"status\": \"ok\",\n        \"topics_loaded\": len(store.topics),\n        \"conversations_loaded\": len(store.assignments),\n        \"index_built\": True,\n    }",
    "start_line": 230,
    "end_line": 242,
    "has_docstring": true,
    "docstring": "Rebuild search index and reload store data.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function refresh_index",
    "component_id": "apps.review-api.src.review_api.main.refresh_index"
  },
  "apps.review-api.src.review_api.main.health": {
    "id": "apps.review-api.src.review_api.main.health",
    "name": "health",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "async def health():\n    \"\"\"Health check.\"\"\"\n    return {\"status\": \"ok\", \"topics_loaded\": len(store.topics), \"conversations_loaded\": len(store.assignments)}",
    "start_line": 246,
    "end_line": 248,
    "has_docstring": true,
    "docstring": "Health check.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function health",
    "component_id": "apps.review-api.src.review_api.main.health"
  },
  "apps.review-api.src.review_api.main.main": {
    "id": "apps.review-api.src.review_api.main.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/main.py",
    "relative_path": "apps/review-api/src/review_api/main.py",
    "depends_on": [],
    "source_code": "def main():\n    \"\"\"Run the API server.\"\"\"\n    import uvicorn\n\n    port = int(os.getenv(\"PORT\", \"8001\"))\n    uvicorn.run(app, host=\"0.0.0.0\", port=port)",
    "start_line": 251,
    "end_line": 256,
    "has_docstring": true,
    "docstring": "Run the API server.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "apps.review-api.src.review_api.main.main"
  },
  "apps.review-api.src.review_api.search.SearchIndex": {
    "id": "apps.review-api.src.review_api.search.SearchIndex",
    "name": "SearchIndex",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/search.py",
    "relative_path": "apps/review-api/src/review_api/search.py",
    "depends_on": [],
    "source_code": "class SearchIndex:\n    \"\"\"Simple full-text search index.\"\"\"\n\n    def __init__(self, store: KnowledgeStore):\n        self.store = store\n        self.index: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n\n    def build_index(self) -> None:\n        \"\"\"Build search index from store data.\"\"\"\n        self.index.clear()\n\n        # Index topics\n        for topic_id, topic in self.store.topics.items():\n            text = f\"{topic.name} {topic.description} {' '.join(topic.keywords)}\"\n            self._index_text(\"topic\", topic_id, text, {\"topic_id\": topic_id, \"name\": topic.name})\n\n        # Index conversations\n        for conv_id, assignment in self.store.assignments.items():\n            text = f\"{assignment.title}\"\n            if assignment.project_name:\n                text += f\" {assignment.project_name}\"\n            # Add atom statements if available\n            detail = self.store.get_conversation_detail(conv_id)\n            if detail:\n                for fact in detail.get(\"facts\", []):\n                    text += f\" {fact.get('statement', '')}\"\n                for decision in detail.get(\"decisions\", []):\n                    text += f\" {decision.get('statement', '')}\"\n                for question in detail.get(\"questions\", []):\n                    text += f\" {question.get('question', '')}\"\n\n            self._index_text(\n                \"conversation\",\n                conv_id,\n                text,\n                {\n                    \"conversation_id\": conv_id,\n                    \"title\": assignment.title,\n                    \"project_name\": assignment.project_name,\n                },\n            )\n\n        # Index atoms\n        for conv_id, assignment in self.store.assignments.items():\n            detail = self.store.get_conversation_detail(conv_id)\n            if detail:\n                for i, fact in enumerate(detail.get(\"facts\", [])):\n                    text = f\"{fact.get('statement', '')} {fact.get('topic', '')}\"\n                    self._index_text(\n                        \"atom\",\n                        f\"{conv_id}-fact-{i}\",\n                        text,\n                        {\n                            \"conversation_id\": conv_id,\n                            \"type\": \"fact\",\n                            \"statement\": fact.get(\"statement\", \"\"),\n                            \"topic\": fact.get(\"topic\", \"\"),\n                        },\n                    )\n                for i, decision in enumerate(detail.get(\"decisions\", [])):\n                    text = f\"{decision.get('statement', '')} {decision.get('topic', '')} {decision.get('rationale', '')}\"\n                    self._index_text(\n                        \"atom\",\n                        f\"{conv_id}-decision-{i}\",\n                        text,\n                        {\n                            \"conversation_id\": conv_id,\n                            \"type\": \"decision\",\n                            \"statement\": decision.get(\"statement\", \"\"),\n                            \"topic\": decision.get(\"topic\", \"\"),\n                        },\n                    )\n                for i, question in enumerate(detail.get(\"questions\", [])):\n                    text = f\"{question.get('question', '')} {question.get('topic', '')} {question.get('context', '')}\"\n                    self._index_text(\n                        \"atom\",\n                        f\"{conv_id}-question-{i}\",\n                        text,\n                        {\n                            \"conversation_id\": conv_id,\n                            \"type\": \"question\",\n                            \"question\": question.get(\"question\", \"\"),\n                            \"topic\": question.get(\"topic\", \"\"),\n                        },\n                    )\n\n        # Index doc content (markdown)\n        for conv_id in self.store.assignments.keys():\n            docs = self.store._list_conversation_docs(conv_id)\n            for doc in docs:\n                content = self.store.get_doc_content(conv_id, doc[\"name\"])\n                if content:\n                    self._index_text(\n                        \"doc\",\n                        f\"{conv_id}/{doc['name']}\",\n                        content,\n                        {\n                            \"conversation_id\": conv_id,\n                            \"doc_name\": doc[\"name\"],\n                            \"path\": doc[\"path\"],\n                        },\n                    )\n\n    def _index_text(self, doc_type: str, doc_id: str, text: str, metadata: Dict[str, Any]) -> None:\n        \"\"\"Index a text document.\"\"\"\n        # Simple word-based indexing (could be improved with stemming, etc.)\n        words = text.lower().split()\n        for word in words:\n            if len(word) > 2:  # Skip very short words\n                self.index[word].append({\"type\": doc_type, \"id\": doc_id, \"metadata\": metadata})\n\n    def search(self, query: str, limit: int = 20) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Search across all indexed content.\"\"\"\n        query_words = query.lower().split()\n        if not query_words:\n            return {\"topics\": [], \"conversations\": [], \"atoms\": [], \"docs\": []}\n\n        # Score results by word frequency\n        scores: Dict[tuple, float] = defaultdict(float)\n        result_metadata: Dict[tuple, Dict[str, Any]] = {}\n\n        for word in query_words:\n            if len(word) > 2 and word in self.index:\n                for hit in self.index[word]:\n                    key = (hit[\"type\"], hit[\"id\"])\n                    scores[key] += 1.0\n                    result_metadata[key] = hit[\"metadata\"]\n\n        # Sort by score and group by type\n        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:limit]\n\n        results: Dict[str, List[Dict[str, Any]]] = {\n            \"topics\": [],\n            \"conversations\": [],\n            \"atoms\": [],\n            \"docs\": [],\n        }\n\n        for (doc_type, doc_id), score in sorted_results:\n            metadata = result_metadata.get((doc_type, doc_id), {})\n            result_item = {**metadata, \"score\": score, \"id\": doc_id}\n            if doc_type == \"topic\":\n                results[\"topics\"].append(result_item)\n            elif doc_type == \"conversation\":\n                results[\"conversations\"].append(result_item)\n            elif doc_type == \"atom\":\n                results[\"atoms\"].append(result_item)\n            elif doc_type == \"doc\":\n                results[\"docs\"].append(result_item)\n\n        return results",
    "start_line": 11,
    "end_line": 161,
    "has_docstring": true,
    "docstring": "Simple full-text search index.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class SearchIndex",
    "component_id": "apps.review-api.src.review_api.search.SearchIndex"
  },
  "apps.review-api.src.review_api.store.KnowledgeStore": {
    "id": "apps.review-api.src.review_api.store.KnowledgeStore",
    "name": "KnowledgeStore",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-api/src/review_api/store.py",
    "relative_path": "apps/review-api/src/review_api/store.py",
    "depends_on": [
      "src.ck_exporter.pipeline.consolidate._read_jsonl",
      "src.ck_exporter.core.models.topics.TopicRegistry",
      "src.ck_exporter.core.models.topics.ConversationTopics"
    ],
    "source_code": "class KnowledgeStore:\n    \"\"\"Read-only store that loads and indexes knowledge artifacts.\"\"\"\n\n    def __init__(self, base_path: Path):\n        \"\"\"\n        Initialize store from base directory containing output/, docs/, _atoms/, _evidence/.\n\n        Args:\n            base_path: Root directory of the project (should contain output/, docs/, etc.)\n        \"\"\"\n        self.base_path = Path(base_path).resolve()\n        self.output_dir = self.base_path / \"output\"\n        self.docs_dir = self.base_path / \"docs\"\n        self.atoms_dir = self.base_path / \"_atoms\"\n        self.evidence_dir = self.base_path / \"_evidence\"\n\n        # In-memory indexes\n        self.topics: Dict[int, Topic] = {}\n        self.topic_registry: Optional[TopicRegistry] = None\n        self.assignments: Dict[str, ConversationTopics] = {}\n        self.review_queue: List[Dict[str, Any]] = []\n        self.conversation_index: Dict[str, Dict[str, Any]] = {}\n        self.topic_to_conversations: Dict[int, List[str]] = defaultdict(list)\n\n        self._load_all()\n\n    def _load_all(self) -> None:\n        \"\"\"Load all data sources and build indexes.\"\"\"\n        # Load topic registry\n        registry_path = self.output_dir / \"topic_registry.json\"\n        if registry_path.exists():\n            with registry_path.open(\"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n                self.topic_registry = TopicRegistry(**data)\n                for topic in self.topic_registry.topics:\n                    self.topics[topic.topic_id] = topic\n\n        # Load assignments\n        assignments_path = self.output_dir / \"assignments.jsonl\"\n        if assignments_path.exists():\n            with assignments_path.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        data = json.loads(line)\n                        assignment = ConversationTopics(**data)\n                        self.assignments[assignment.conversation_id] = assignment\n\n                        # Build topic -> conversations index\n                        for topic_assignment in assignment.topics:\n                            if topic_assignment.rank == \"primary\":\n                                self.topic_to_conversations[topic_assignment.topic_id].append(\n                                    assignment.conversation_id\n                                )\n                    except (json.JSONDecodeError, Exception) as e:\n                        continue\n\n        # Load review queue\n        review_path = self.output_dir / \"review_queue.jsonl\"\n        if review_path.exists():\n            with review_path.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        self.review_queue.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        continue\n\n        # Build conversation index\n        for conv_id, assignment in self.assignments.items():\n            self.conversation_index[conv_id] = {\n                \"title\": assignment.title,\n                \"project_id\": assignment.project_id,\n                \"project_name\": assignment.project_name,\n                \"topics\": [t.model_dump() for t in assignment.topics],\n                \"atom_count\": assignment.atom_count,\n                \"review_flag\": assignment.review_flag,\n            }\n\n    def get_topics_summary(self) -> List[Dict[str, Any]]:\n        \"\"\"Get list of topics with counts (without embeddings).\"\"\"\n        result = []\n        for topic_id, topic in self.topics.items():\n            conv_ids = self.topic_to_conversations.get(topic_id, [])\n            flagged_count = sum(\n                1\n                for conv_id in conv_ids\n                if self.assignments.get(conv_id, ConversationTopics(\n                    conversation_id=conv_id, title=\"\", topics=[], atom_count=0\n                )).review_flag\n            )\n\n            # Count atoms for this topic's conversations\n            atom_count = sum(\n                self.assignments.get(conv_id, ConversationTopics(\n                    conversation_id=conv_id, title=\"\", topics=[], atom_count=0\n                )).atom_count\n                for conv_id in conv_ids\n            )\n\n            result.append({\n                \"topic_id\": topic.topic_id,\n                \"name\": topic.name,\n                \"description\": topic.description,\n                \"keywords\": topic.keywords[:10],  # Limit for UI\n                \"conversation_count\": len(conv_ids),\n                \"atom_count\": atom_count,\n                \"flagged_count\": flagged_count,\n            })\n        return sorted(result, key=lambda x: x[\"conversation_count\"], reverse=True)\n\n    def get_topic_detail(self, topic_id: int) -> Optional[Dict[str, Any]]:\n        \"\"\"Get topic details with conversations.\"\"\"\n        topic = self.topics.get(topic_id)\n        if not topic:\n            return None\n\n        conv_ids = self.topic_to_conversations.get(topic_id, [])\n        conversations = []\n        for conv_id in conv_ids:\n            assignment = self.assignments.get(conv_id)\n            if assignment:\n                conversations.append({\n                    \"conversation_id\": assignment.conversation_id,\n                    \"title\": assignment.title,\n                    \"project_id\": assignment.project_id,\n                    \"project_name\": assignment.project_name,\n                    \"atom_count\": assignment.atom_count,\n                    \"review_flag\": assignment.review_flag,\n                    \"topics\": [\n                        {\n                            \"topic_id\": t.topic_id,\n                            \"name\": t.name,\n                            \"score\": t.score,\n                            \"rank\": t.rank,\n                        }\n                        for t in assignment.topics\n                    ],\n                })\n\n        return {\n            \"topic_id\": topic.topic_id,\n            \"name\": topic.name,\n            \"description\": topic.description,\n            \"keywords\": topic.keywords,\n            \"representative_conversations\": topic.representative_conversations,\n            \"conversations\": conversations,\n        }\n\n    def get_conversation_detail(self, conversation_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get conversation details with atoms and doc list.\"\"\"\n        assignment = self.assignments.get(conversation_id)\n        if not assignment:\n            return None\n\n        # Load atoms (prefer per-conversation, fallback to consolidated)\n        atoms = self._load_conversation_atoms(conversation_id)\n        # Filter by type (atoms are dicts, not Pydantic models at this point)\n        facts = [a for a in atoms if isinstance(a, dict) and a.get(\"type\") != \"decision\" and a.get(\"type\") != \"question\"]\n        decisions = [a for a in atoms if isinstance(a, dict) and a.get(\"type\") == \"decision\"]\n        questions = [a for a in atoms if isinstance(a, dict) and a.get(\"type\") == \"question\"]\n\n        # List available docs\n        docs = self._list_conversation_docs(conversation_id)\n\n        return {\n            \"conversation_id\": assignment.conversation_id,\n            \"title\": assignment.title,\n            \"project_id\": assignment.project_id,\n            \"project_name\": assignment.project_name,\n            \"topics\": [t.model_dump() for t in assignment.topics],\n            \"atom_count\": assignment.atom_count,\n            \"review_flag\": assignment.review_flag,\n            \"facts\": facts,  # Already dicts\n            \"decisions\": decisions,  # Already dicts\n            \"questions\": questions,  # Already dicts\n            \"docs\": docs,\n        }\n\n    def _load_conversation_atoms(self, conversation_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Load atoms for a conversation, preferring per-conversation files.\"\"\"\n        atoms = []\n\n        # Try per-conversation atoms first\n        conv_atoms_dir = self.atoms_dir / conversation_id\n        if conv_atoms_dir.exists():\n            for file_name in [\"facts.jsonl\", \"decisions.jsonl\", \"open_questions.jsonl\"]:\n                file_path = conv_atoms_dir / file_name\n                if file_path.exists():\n                    atoms.extend(self._read_jsonl(file_path))\n        else:\n            # Fallback: slice from consolidated files\n            consolidated_dir = self.output_dir / \"project\"\n            for file_name in [\"atoms.jsonl\", \"decisions.jsonl\", \"open_questions.jsonl\"]:\n                file_path = consolidated_dir / file_name\n                if file_path.exists():\n                    for obj in self._read_jsonl(file_path):\n                        if obj.get(\"source_conversation_id\") == conversation_id:\n                            atoms.append(obj)\n\n        return atoms\n\n    def _read_jsonl(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Read JSONL file.\"\"\"\n        result = []\n        if not file_path.exists():\n            return result\n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    result.append(json.loads(line))\n                except json.JSONDecodeError:\n                    continue\n        return result\n\n    def _list_conversation_docs(self, conversation_id: str) -> List[Dict[str, str]]:\n        \"\"\"List available markdown docs for a conversation.\"\"\"\n        docs = []\n        conv_docs_dir = self.docs_dir / conversation_id\n        if conv_docs_dir.exists():\n            for md_file in conv_docs_dir.glob(\"*.md\"):\n                docs.append({\n                    \"name\": md_file.name,\n                    \"path\": str(md_file.relative_to(self.base_path)),\n                })\n\n        # Also check ADRs\n        adr_dir = self.docs_dir / \"decisions\" / conversation_id\n        if adr_dir.exists():\n            for md_file in adr_dir.glob(\"*.md\"):\n                docs.append({\n                    \"name\": f\"adrs/{md_file.name}\",\n                    \"path\": str(md_file.relative_to(self.base_path)),\n                })\n\n        return docs\n\n    def get_doc_content(self, conversation_id: str, doc_name: str) -> Optional[str]:\n        \"\"\"Get markdown content for a doc.\"\"\"\n        # Handle ADR paths\n        if doc_name.startswith(\"adrs/\"):\n            adr_path = self.docs_dir / \"decisions\" / conversation_id / doc_name.replace(\"adrs/\", \"\")\n            if adr_path.exists():\n                return adr_path.read_text(encoding=\"utf-8\")\n        else:\n            doc_path = self.docs_dir / conversation_id / doc_name\n            if doc_path.exists():\n                return doc_path.read_text(encoding=\"utf-8\")\n        return None\n\n    def get_review_queue(self) -> List[Dict[str, Any]]:\n        \"\"\"Get review queue items.\"\"\"\n        return self.review_queue\n\n    def get_conversation_file_paths(self, conversation_id: str) -> Dict[str, Path]:\n        \"\"\"Get file paths for bundling a conversation.\"\"\"\n        paths = {}\n\n        # Docs\n        conv_docs_dir = self.docs_dir / conversation_id\n        if conv_docs_dir.exists():\n            paths[\"docs\"] = conv_docs_dir\n\n        # ADRs\n        adr_dir = self.docs_dir / \"decisions\" / conversation_id\n        if adr_dir.exists():\n            paths[\"adrs\"] = adr_dir\n\n        # Atoms\n        conv_atoms_dir = self.atoms_dir / conversation_id\n        if conv_atoms_dir.exists():\n            paths[\"atoms\"] = conv_atoms_dir\n        else:\n            # Will need to slice from consolidated\n            paths[\"atoms_consolidated\"] = self.output_dir / \"project\"\n            paths[\"conversation_id\"] = conversation_id  # Marker for slicing\n\n        # Evidence\n        evidence_path = self.evidence_dir / conversation_id / \"conversation.md\"\n        if evidence_path.exists():\n            paths[\"evidence\"] = evidence_path\n\n        return paths\n\n    def get_topic_conversation_ids(self, topic_id: int) -> List[str]:\n        \"\"\"Get all conversation IDs for a topic (primary assignments).\"\"\"\n        return self.topic_to_conversations.get(topic_id, [])",
    "start_line": 16,
    "end_line": 309,
    "has_docstring": true,
    "docstring": "Read-only store that loads and indexes knowledge artifacts.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class KnowledgeStore",
    "component_id": "apps.review-api.src.review_api.store.KnowledgeStore"
  },
  "apps.review-ui.src.pages.ConversationDetail.ConversationDetail": {
    "id": "apps.review-ui.src.pages.ConversationDetail.ConversationDetail",
    "name": "ConversationDetail",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/pages/ConversationDetail.tsx",
    "relative_path": "apps/review-ui/src/pages/ConversationDetail.tsx",
    "depends_on": [
      "src.ck_exporter.core.models.atoms.Atom",
      "src.ck_exporter.core.models.atoms.DecisionAtom",
      "src.ck_exporter.core.models.atoms.OpenQuestion"
    ],
    "source_code": "function ConversationDetail() {\n  const { conversationId } = useParams<{ conversationId: string }>()\n  const navigate = useNavigate()\n  const [conversation, setConversation] = useState<ConversationDetailType | null>(null)\n  const [loading, setLoading] = useState(true)\n  const [activeTab, setActiveTab] = useState<'docs' | 'facts' | 'decisions' | 'questions'>('docs')\n  const [selectedDoc, setSelectedDoc] = useState<DocInfo | null>(null)\n  const [docContent, setDocContent] = useState<string>('')\n  const [filters, setFilters] = useState<{\n    atomType?: string\n    status?: string\n    atomTopic?: string\n  }>({})\n  const [selectedAtomIds, setSelectedAtomIds] = useState<Set<string>>(new Set())\n\n  useEffect(() => {\n    if (!conversationId) return\n    fetch(`/api/conversations/${conversationId}`)\n      .then(res => res.json())\n      .then(data => {\n        setConversation(data)\n        setLoading(false)\n        // Auto-select first doc\n        if (data.docs && data.docs.length > 0) {\n          setSelectedDoc(data.docs[0])\n        }\n      })\n      .catch(err => {\n        console.error('Failed to load conversation:', err)\n        setLoading(false)\n      })\n  }, [conversationId])\n\n  useEffect(() => {\n    if (!selectedDoc || !conversationId) return\n    fetch(`/api/conversations/${conversationId}/doc/${selectedDoc.name}`)\n      .then(res => res.text())\n      .then(text => setDocContent(text))\n      .catch(err => {\n        console.error('Failed to load doc:', err)\n        setDocContent('Failed to load document')\n      })\n  }, [selectedDoc, conversationId])\n\n  const handleDownload = async () => {\n    if (!conversationId) return\n    try {\n      const response = await fetch(`/api/bundles/conversation/${conversationId}`, { method: 'POST' })\n      const blob = await response.blob()\n      const url = window.URL.createObjectURL(blob)\n      const a = document.createElement('a')\n      a.href = url\n      const title = conversation?.title.replace(/\\s+/g, '-') || 'conversation'\n      a.download = `conversation-${conversationId.substring(0, 8)}-${title}.zip`\n      document.body.appendChild(a)\n      a.click()\n      window.URL.revokeObjectURL(url)\n      document.body.removeChild(a)\n    } catch (err) {\n      console.error('Failed to download bundle:', err)\n      alert('Failed to download bundle')\n    }\n  }\n\n  const handleExportSelectedAtoms = async () => {\n    if (!conversationId || selectedAtomIds.size === 0) return\n    try {\n      const response = await fetch(`/api/conversations/${conversationId}/atoms/export`, {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ atom_ids: Array.from(selectedAtomIds) }),\n      })\n      const blob = await response.blob()\n      const url = window.URL.createObjectURL(blob)\n      const a = document.createElement('a')\n      a.href = url\n      a.download = `atoms-${conversationId.substring(0, 8)}.jsonl`\n      document.body.appendChild(a)\n      a.click()\n      window.URL.revokeObjectURL(url)\n      document.body.removeChild(a)\n    } catch (err) {\n      console.error('Failed to export atoms:', err)\n      alert('Failed to export atoms')\n    }\n  }\n\n  const getFilteredAtoms = (atoms: (Atom | DecisionAtom | OpenQuestion)[]) => {\n    return atoms.filter(atom => {\n      if (filters.atomType && atom.type !== filters.atomType) return false\n      if (filters.status && atom.status !== filters.status) return false\n      if (filters.atomTopic && atom.topic !== filters.atomTopic) return false\n      return true\n    })\n  }\n\n  const getUniqueAtomTopics = (atoms: (Atom | DecisionAtom | OpenQuestion)[]) => {\n    return Array.from(new Set(atoms.map(a => a.topic))).sort()\n  }\n\n  if (loading) {\n    return <div className=\"container\">Loading conversation...</div>\n  }\n\n  if (!conversation) {\n    return <div className=\"container\">Conversation not found</div>\n  }\n\n  return (\n    <div className=\"container\">\n      <div className=\"page-header\">\n        <button onClick={() => navigate(-1)} className=\"btn btn-secondary\">\n           Back\n        </button>\n        <button onClick={handleDownload} className=\"btn\">\n           Download Bundle\n        </button>\n      </div>\n      <div className=\"card\">\n        <h1 className=\"page-title\">{conversation.title || 'Untitled Conversation'}</h1>\n        {conversation.project_name && (\n          <p className=\"page-subtitle\">Project: {conversation.project_name}</p>\n        )}",
    "start_line": 7,
    "end_line": 129,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function ConversationDetail",
    "component_id": "apps.review-ui.src.pages.ConversationDetail.ConversationDetail"
  },
  "apps.review-ui.src.types.TopicSummary": {
    "id": "apps.review-ui.src.types.TopicSummary",
    "name": "TopicSummary",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [],
    "source_code": "interface TopicSummary {\n  topic_id: number;\n  name: string;\n  description: string;\n  keywords: string[];\n  conversation_count: number;\n  atom_count: number;\n  flagged_count: number;\n}",
    "start_line": 1,
    "end_line": 9,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface TopicSummary",
    "component_id": "apps.review-ui.src.types.TopicSummary"
  },
  "apps.review-ui.src.types.TopicDetail": {
    "id": "apps.review-ui.src.types.TopicDetail",
    "name": "TopicDetail",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [
      "apps.review-ui.src.types.ConversationSummary"
    ],
    "source_code": "interface TopicDetail extends TopicSummary {\n  representative_conversations: string[];\n  conversations: ConversationSummary[];\n}",
    "start_line": 11,
    "end_line": 14,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface TopicDetail",
    "component_id": "apps.review-ui.src.types.TopicDetail"
  },
  "apps.review-ui.src.types.ConversationSummary": {
    "id": "apps.review-ui.src.types.ConversationSummary",
    "name": "ConversationSummary",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [
      "apps.review-ui.src.types.TopicAssignment"
    ],
    "source_code": "interface ConversationSummary {\n  conversation_id: string;\n  title: string;\n  project_id?: string;\n  project_name?: string;\n  atom_count: number;\n  review_flag: boolean;\n  topics: TopicAssignment[];\n}",
    "start_line": 16,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface ConversationSummary",
    "component_id": "apps.review-ui.src.types.ConversationSummary"
  },
  "apps.review-ui.src.types.TopicAssignment": {
    "id": "apps.review-ui.src.types.TopicAssignment",
    "name": "TopicAssignment",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [],
    "source_code": "interface TopicAssignment {\n  topic_id: number;\n  name: string;\n  score: number;\n  rank: 'primary' | 'secondary';\n}",
    "start_line": 26,
    "end_line": 31,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface TopicAssignment",
    "component_id": "apps.review-ui.src.types.TopicAssignment"
  },
  "apps.review-ui.src.types.ConversationDetail": {
    "id": "apps.review-ui.src.types.ConversationDetail",
    "name": "ConversationDetail",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [
      "apps.review-ui.src.types.DecisionAtom",
      "apps.review-ui.src.types.DocInfo",
      "apps.review-ui.src.types.Atom",
      "apps.review-ui.src.types.OpenQuestion"
    ],
    "source_code": "interface ConversationDetail extends ConversationSummary {\n  facts: Atom[];\n  decisions: DecisionAtom[];\n  questions: OpenQuestion[];\n  docs: DocInfo[];\n}",
    "start_line": 33,
    "end_line": 38,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface ConversationDetail",
    "component_id": "apps.review-ui.src.types.ConversationDetail"
  },
  "apps.review-ui.src.types.Atom": {
    "id": "apps.review-ui.src.types.Atom",
    "name": "Atom",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [
      "apps.review-ui.src.types.Evidence"
    ],
    "source_code": "interface Atom {\n  type: string;\n  topic: string;\n  statement: string;\n  status: 'active' | 'deprecated' | 'uncertain';\n  evidence: Evidence[];\n  extracted_at: string;\n}",
    "start_line": 40,
    "end_line": 47,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface Atom",
    "component_id": "apps.review-ui.src.types.Atom"
  },
  "apps.review-ui.src.types.DecisionAtom": {
    "id": "apps.review-ui.src.types.DecisionAtom",
    "name": "DecisionAtom",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [],
    "source_code": "interface DecisionAtom extends Atom {\n  type: 'decision';\n  alternatives: string[];\n  rationale?: string;\n  consequences?: string;\n}",
    "start_line": 49,
    "end_line": 54,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface DecisionAtom",
    "component_id": "apps.review-ui.src.types.DecisionAtom"
  },
  "apps.review-ui.src.types.OpenQuestion": {
    "id": "apps.review-ui.src.types.OpenQuestion",
    "name": "OpenQuestion",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [
      "apps.review-ui.src.types.Evidence"
    ],
    "source_code": "interface OpenQuestion {\n  question: string;\n  topic: string;\n  context?: string;\n  evidence: Evidence[];\n  extracted_at: string;\n}",
    "start_line": 56,
    "end_line": 62,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface OpenQuestion",
    "component_id": "apps.review-ui.src.types.OpenQuestion"
  },
  "apps.review-ui.src.types.Evidence": {
    "id": "apps.review-ui.src.types.Evidence",
    "name": "Evidence",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [],
    "source_code": "interface Evidence {\n  message_id?: string;\n  time_iso?: string;\n  text_snippet?: string;\n}",
    "start_line": 64,
    "end_line": 68,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface Evidence",
    "component_id": "apps.review-ui.src.types.Evidence"
  },
  "apps.review-ui.src.types.DocInfo": {
    "id": "apps.review-ui.src.types.DocInfo",
    "name": "DocInfo",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [],
    "source_code": "interface DocInfo {\n  name: string;\n  path: string;\n}",
    "start_line": 70,
    "end_line": 73,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface DocInfo",
    "component_id": "apps.review-ui.src.types.DocInfo"
  },
  "apps.review-ui.src.types.ReviewQueueItem": {
    "id": "apps.review-ui.src.types.ReviewQueueItem",
    "name": "ReviewQueueItem",
    "component_type": "interface",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/src/types.ts",
    "relative_path": "apps/review-ui/src/types.ts",
    "depends_on": [],
    "source_code": "interface ReviewQueueItem {\n  conversation_id: string;\n  title: string;\n  project_id?: string;\n  project_name?: string;\n  primary_topic: string;\n  primary_score: number;\n  reason: 'low_confidence' | 'ambiguous';\n}",
    "start_line": 75,
    "end_line": 83,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "interface",
    "base_classes": [],
    "class_name": null,
    "display_name": "interface ReviewQueueItem",
    "component_id": "apps.review-ui.src.types.ReviewQueueItem"
  },
  "apps.review-ui.vite.config.defineConfig": {
    "id": "apps.review-ui.vite.config.defineConfig",
    "name": "defineConfig",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/apps/review-ui/vite.config.ts",
    "relative_path": "apps/review-ui/vite.config.ts",
    "depends_on": [
      "apps.review-ui.vite.config.defineConfig"
    ],
    "source_code": "export default defineConfig({\n  plugins: [react()],\n  server: {\n    port: 5173,\n    proxy: {\n      '/api': {\n        target: 'http://localhost:8001',\n        changeOrigin: true,\n      },\n    },\n  },\n})",
    "start_line": 4,
    "end_line": 15,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "export_default_call",
    "base_classes": null,
    "class_name": null,
    "display_name": "export default defineConfig(...)",
    "component_id": "apps.review-ui.vite.config.defineConfig"
  },
  "scripts.organize_examples.classify": {
    "id": "scripts.organize_examples.classify",
    "name": "classify",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/scripts/organize_examples.py",
    "relative_path": "scripts/organize_examples.py",
    "depends_on": [],
    "source_code": "def classify(obj: Any) -> str:\n    \"\"\"Classify JSON object as chatgpt_export_list, chatgpt_single, claude, or other.\"\"\"\n    if isinstance(obj, list):\n        return \"chatgpt_export_list\"\n    if isinstance(obj, dict):\n        if \"chat_messages\" in obj and obj.get(\"platform\") == \"CLAUDE_AI\":\n            return \"claude\"\n        if \"mapping\" in obj and \"current_node\" in obj:\n            return \"chatgpt_single\"\n    return \"other\"",
    "start_line": 12,
    "end_line": 21,
    "has_docstring": true,
    "docstring": "Classify JSON object as chatgpt_export_list, chatgpt_single, claude, or other.",
    "parameters": [
      "obj"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function classify",
    "component_id": "scripts.organize_examples.classify"
  },
  "scripts.organize_examples.main": {
    "id": "scripts.organize_examples.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/scripts/organize_examples.py",
    "relative_path": "scripts/organize_examples.py",
    "depends_on": [
      "scripts.organize_examples.classify"
    ],
    "source_code": "def main() -> None:\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--root\", default=\"data-examples\")\n    ap.add_argument(\"--apply\", action=\"store_true\", help=\"Actually move files (otherwise dry-run).\")\n    ap.add_argument(\"--copy\", action=\"store_true\", help=\"Copy instead of move.\")\n    ap.add_argument(\n        \"--write-merged-chatgpt-export\",\n        action=\"store_true\",\n        help=\"Write chatgpt_export/merged_chatgpt_export.json with generated ids.\",\n    )\n    args = ap.parse_args()\n\n    root = Path(args.root).resolve()\n    files = sorted(p for p in root.glob(\"*.json\") if p.is_file())\n\n    out_chatgpt_single = root / \"chatgpt_single\"\n    out_claude = root / \"claude\"\n    out_export = root / \"chatgpt_export\"\n    out_other = root / \"other\"\n\n    moves: list[tuple[Path, Path]] = []\n    merged: list[dict[str, Any]] = []\n\n    for p in files:\n        try:\n            obj = json.loads(p.read_text(encoding=\"utf-8\"))\n        except Exception:\n            bucket = \"other\"\n            obj = None\n        else:\n            bucket = classify(obj)\n\n        if bucket == \"chatgpt_single\":\n            dest_dir = out_chatgpt_single\n            if args.write_merged_chatgpt_export and isinstance(obj, dict):\n                # Ensure the current CLI can work: needs conversation[\"id\"] (or \"conversation_id\")\n                if not obj.get(\"id\") and not obj.get(\"conversation_id\"):\n                    obj[\"id\"] = p.stem  # recommendation: filename-based id\n                merged.append(obj)\n        elif bucket == \"claude\":\n            dest_dir = out_claude\n        elif bucket == \"chatgpt_export_list\":\n            dest_dir = out_export\n        else:\n            dest_dir = out_other\n\n        dest = dest_dir / p.name\n        moves.append((p, dest))\n\n    # Show plan\n    print(f\"Root: {root}\")\n    print(f\"\\nOrganization plan:\")\n    for src, dst in moves:\n        action = \"COPY\" if args.copy else \"MOVE\"\n        print(f\"  {action} {src.name} -> {dst.relative_to(root)}\")\n\n    if not args.apply:\n        print(\"\\nDry-run only. Re-run with --apply to perform changes.\")\n        return\n\n    # Apply moves/copies\n    for d in [out_chatgpt_single, out_claude, out_export, out_other]:\n        d.mkdir(parents=True, exist_ok=True)\n\n    for src, dst in moves:\n        if dst.exists():\n            print(f\"  Skipping (exists): {dst.relative_to(root)}\")\n            continue\n        if args.copy:\n            shutil.copy2(src, dst)\n            print(f\" Copied: {src.name} -> {dst.relative_to(root)}\")\n        else:\n            src.rename(dst)\n            print(f\" Moved: {src.name} -> {dst.relative_to(root)}\")\n\n    if args.write_merged_chatgpt_export and merged:\n        out_export.mkdir(parents=True, exist_ok=True)\n        merged_path = out_export / \"merged_chatgpt_export.json\"\n        merged_path.write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n        print(f\"\\n Wrote merged export: {merged_path.relative_to(root)} ({len(merged)} conversations)\")",
    "start_line": 24,
    "end_line": 103,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "scripts.organize_examples.main"
  },
  "src.ck_exporter.adapters.dspy_atom_refiner.DspyAtomRefiner": {
    "id": "src.ck_exporter.adapters.dspy_atom_refiner.DspyAtomRefiner",
    "name": "DspyAtomRefiner",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/dspy_atom_refiner.py",
    "relative_path": "src/ck_exporter/adapters/dspy_atom_refiner.py",
    "depends_on": [
      "src.ck_exporter.programs.dspy.refine_atoms.create_refine_atoms_program",
      "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_refinement"
    ],
    "source_code": "class DspyAtomRefiner:\n    \"\"\"DSPy-backed implementation of atom refinement logic.\"\"\"\n\n    def __init__(self, use_openrouter: bool = True, lm: Any = None):\n        \"\"\"\n        Initialize DSPy atom refiner.\n\n        Args:\n            use_openrouter: Whether to use OpenRouter\n            lm: Optional pre-configured DSPy LM (for testing)\n        \"\"\"\n        try:\n            import dspy\n        except ImportError:\n            raise ImportError(\n                \"dspy-ai is not installed. Install it with: uv sync --extra dspy\"\n            ) from None\n\n        if lm is None:\n            lm = get_dspy_lm_for_refinement(use_openrouter=use_openrouter)\n\n        self.program = create_refine_atoms_program(lm)\n\n    def refine_atoms(\n        self,\n        all_candidates: dict[str, list[dict[str, Any]]],\n        conversation_id: str,\n        conversation_title: str | None,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Refine and consolidate candidate atoms (Pass 2).\n\n        Args:\n            all_candidates: Dict with \"facts\", \"decisions\", \"open_questions\" lists\n            conversation_id: Conversation identifier\n            conversation_title: Optional conversation title\n\n        Returns:\n            Dict with refined/deduplicated atoms\n        \"\"\"\n        # Prepare candidates JSON\n        candidates_json = json.dumps(all_candidates, ensure_ascii=False, indent=2)\n\n        try:\n            # Call DSPy program\n            result = self.program(\n                conversation_id=conversation_id,\n                conversation_title=conversation_title or \"Unknown\",\n                candidates_json=candidates_json,\n            )\n\n            # Parse JSON outputs\n            facts_json = result.get(\"facts_json\", \"[]\")\n            decisions_json = result.get(\"decisions_json\", \"[]\")\n            open_questions_json = result.get(\"open_questions_json\", \"[]\")\n\n            # Validate and parse\n            try:\n                facts = json.loads(facts_json)\n                decisions = json.loads(decisions_json)\n                open_questions = json.loads(open_questions_json)\n\n                # Ensure they are lists\n                if not isinstance(facts, list):\n                    logger.warning(\n                        \"DSPy returned non-list facts, using candidates as-is\",\n                        extra={\n                            \"event\": \"refiner.dspy.invalid_type\",\n                            \"conversation_id\": conversation_id,\n                            \"type\": \"facts\",\n                        },\n                    )\n                    return all_candidates\n                if not isinstance(decisions, list):\n                    logger.warning(\n                        \"DSPy returned non-list decisions, using candidates as-is\",\n                        extra={\n                            \"event\": \"refiner.dspy.invalid_type\",\n                            \"conversation_id\": conversation_id,\n                            \"type\": \"decisions\",\n                        },\n                    )\n                    return all_candidates\n                if not isinstance(open_questions, list):\n                    logger.warning(\n                        \"DSPy returned non-list questions, using candidates as-is\",\n                        extra={\n                            \"event\": \"refiner.dspy.invalid_type\",\n                            \"conversation_id\": conversation_id,\n                            \"type\": \"questions\",\n                        },\n                    )\n                    return all_candidates\n\n                return {\n                    \"facts\": facts,\n                    \"decisions\": decisions,\n                    \"open_questions\": open_questions,\n                }\n\n            except json.JSONDecodeError as e:\n                logger.warning(\n                    \"Failed to parse DSPy JSON output, using candidates as-is\",\n                    extra={\n                        \"event\": \"refiner.dspy.json_parse_error\",\n                        \"conversation_id\": conversation_id,\n                    },\n                    exc_info=True,\n                )\n                return all_candidates\n\n        except Exception as e:\n            logger.exception(\n                \"Error in DSPy refinement, falling back to candidates\",\n                extra={\n                    \"event\": \"refiner.dspy.error\",\n                    \"conversation_id\": conversation_id,\n                },\n            )\n            return all_candidates",
    "start_line": 13,
    "end_line": 132,
    "has_docstring": true,
    "docstring": "DSPy-backed implementation of atom refinement logic.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DspyAtomRefiner",
    "component_id": "src.ck_exporter.adapters.dspy_atom_refiner.DspyAtomRefiner"
  },
  "src.ck_exporter.adapters.dspy_lm.configure_dspy_lm": {
    "id": "src.ck_exporter.adapters.dspy_lm.configure_dspy_lm",
    "name": "configure_dspy_lm",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/dspy_lm.py",
    "relative_path": "src/ck_exporter/adapters/dspy_lm.py",
    "depends_on": [],
    "source_code": "def configure_dspy_lm(model: str, use_openrouter: bool = True):\n    \"\"\"\n    Configure DSPy language model for OpenRouter.\n\n    Args:\n        model: Model identifier (e.g., \"z-ai/glm-4.7\")\n        use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n\n    Returns:\n        Configured DSPy LM instance\n\n    Raises:\n        ImportError: If dspy-ai is not installed\n        ValueError: If API key is missing\n    \"\"\"\n    try:\n        import dspy\n    except ImportError:\n        raise ImportError(\n            \"dspy-ai is not installed. Install it with: uv sync --extra dspy\"\n        ) from None\n\n    if use_openrouter:\n        api_key = os.getenv(\"OPENROUTER_API_KEY\") or os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"OPENROUTER_API_KEY or OPENAI_API_KEY must be set in environment\")\n\n        # Configure OpenRouter base URL\n        base_url = \"https://openrouter.ai/api/v1\"\n\n        # Optional attribution headers\n        extra_headers = {}\n        http_referer = os.getenv(\"OPENROUTER_HTTP_REFERER\")\n        x_title = os.getenv(\"OPENROUTER_X_TITLE\", \"ChatGPT Conversation Knowledge Exporter\")\n        if http_referer:\n            extra_headers[\"HTTP-Referer\"] = http_referer\n        if x_title:\n            extra_headers[\"X-Title\"] = x_title\n\n        # DSPy/LiteLLM requires \"openrouter/\" prefix for OpenRouter models\n        if not model.startswith(\"openrouter/\"):\n            dspy_model = f\"openrouter/{model}\"\n        else:\n            dspy_model = model\n\n        # Create DSPy LM with OpenRouter\n        lm = dspy.LM(\n            model=dspy_model,\n            api_key=api_key,\n            api_base=base_url,\n            extra_headers=extra_headers if extra_headers else None,\n        )\n    else:\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"OPENAI_API_KEY must be set in environment\")\n\n        lm = dspy.LM(model=model, api_key=api_key)\n\n    return lm",
    "start_line": 11,
    "end_line": 70,
    "has_docstring": true,
    "docstring": "Configure DSPy language model for OpenRouter.\n\nArgs:\n    model: Model identifier (e.g., \"z-ai/glm-4.7\")\n    use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n\nReturns:\n    Configured DSPy LM instance\n\nRaises:\n    ImportError: If dspy-ai is not installed\n    ValueError: If API key is missing",
    "parameters": [
      "model",
      "use_openrouter"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function configure_dspy_lm",
    "component_id": "src.ck_exporter.adapters.dspy_lm.configure_dspy_lm"
  },
  "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_labeling": {
    "id": "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_labeling",
    "name": "get_dspy_lm_for_labeling",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/dspy_lm.py",
    "relative_path": "src/ck_exporter/adapters/dspy_lm.py",
    "depends_on": [
      "src.ck_exporter.adapters.dspy_lm.configure_dspy_lm"
    ],
    "source_code": "def get_dspy_lm_for_labeling(use_openrouter: bool = True) -> \"dspy.LM\":\n    \"\"\"\n    Get DSPy LM configured for topic labeling.\n\n    Args:\n        use_openrouter: Whether to use OpenRouter\n\n    Returns:\n        Configured DSPy LM instance\n    \"\"\"\n    model = os.getenv(\"CKX_DSPY_LABEL_MODEL\", \"z-ai/glm-4.7\")\n    return configure_dspy_lm(model, use_openrouter=use_openrouter)",
    "start_line": 73,
    "end_line": 84,
    "has_docstring": true,
    "docstring": "Get DSPy LM configured for topic labeling.\n\nArgs:\n    use_openrouter: Whether to use OpenRouter\n\nReturns:\n    Configured DSPy LM instance",
    "parameters": [
      "use_openrouter"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_dspy_lm_for_labeling",
    "component_id": "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_labeling"
  },
  "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_refinement": {
    "id": "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_refinement",
    "name": "get_dspy_lm_for_refinement",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/dspy_lm.py",
    "relative_path": "src/ck_exporter/adapters/dspy_lm.py",
    "depends_on": [
      "src.ck_exporter.adapters.dspy_lm.configure_dspy_lm"
    ],
    "source_code": "def get_dspy_lm_for_refinement(use_openrouter: bool = True) -> \"dspy.LM\":\n    \"\"\"\n    Get DSPy LM configured for atom refinement.\n\n    Args:\n        use_openrouter: Whether to use OpenRouter\n\n    Returns:\n        Configured DSPy LM instance\n    \"\"\"\n    model = os.getenv(\"CKX_DSPY_REFINE_MODEL\", \"z-ai/glm-4.7\")\n    return configure_dspy_lm(model, use_openrouter=use_openrouter)",
    "start_line": 87,
    "end_line": 98,
    "has_docstring": true,
    "docstring": "Get DSPy LM configured for atom refinement.\n\nArgs:\n    use_openrouter: Whether to use OpenRouter\n\nReturns:\n    Configured DSPy LM instance",
    "parameters": [
      "use_openrouter"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_dspy_lm_for_refinement",
    "component_id": "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_refinement"
  },
  "src.ck_exporter.adapters.dspy_topic_labeler.DspyTopicLabeler": {
    "id": "src.ck_exporter.adapters.dspy_topic_labeler.DspyTopicLabeler",
    "name": "DspyTopicLabeler",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/dspy_topic_labeler.py",
    "relative_path": "src/ck_exporter/adapters/dspy_topic_labeler.py",
    "depends_on": [
      "src.ck_exporter.programs.dspy.label_topic.create_label_topic_program",
      "src.ck_exporter.adapters.dspy_lm.get_dspy_lm_for_labeling"
    ],
    "source_code": "class DspyTopicLabeler:\n    \"\"\"DSPy-backed implementation of TopicLabeler.\"\"\"\n\n    def __init__(self, use_openrouter: bool = True, lm: Any = None):\n        \"\"\"\n        Initialize DSPy topic labeler.\n\n        Args:\n            use_openrouter: Whether to use OpenRouter\n            lm: Optional pre-configured DSPy LM (for testing)\n        \"\"\"\n        try:\n            import dspy\n        except ImportError:\n            raise ImportError(\n                \"dspy-ai is not installed. Install it with: uv sync --extra dspy\"\n            ) from None\n\n        if lm is None:\n            lm = get_dspy_lm_for_labeling(use_openrouter=use_openrouter)\n\n        self.program = create_label_topic_program(lm)\n\n    def label_topic(\n        self,\n        topic_id: int,\n        representative_docs: list[tuple[str, str]],\n        keywords: list[str],\n    ) -> dict[str, str]:\n        \"\"\"\n        Generate a label (name + description) for a topic cluster.\n\n        Args:\n            topic_id: Numeric topic identifier\n            representative_docs: List of (conversation_id, doc_text) tuples\n            keywords: Top keywords from topic model\n\n        Returns:\n            Dict with \"name\" and \"description\" keys\n        \"\"\"\n        # Format representative docs\n        doc_samples = \"\\n\\n---\\n\\n\".join(\n            [f\"Conversation ID: {conv_id}\\n\\n{doc_text[:500]}...\" for conv_id, doc_text in representative_docs]\n        )\n\n        # Format keywords\n        keywords_str = \", \".join(keywords[:10])\n\n        try:\n            # Call DSPy program\n            result = self.program(representative_docs=doc_samples, keywords=keywords_str)\n\n            # Validate result\n            name = result.get(\"name\", \"\").strip()\n            description = result.get(\"description\", \"\").strip()\n\n            if not name:\n                logger.warning(\n                    \"Empty name from DSPy, using fallback\",\n                    extra={\n                        \"event\": \"labeler.dspy.empty_name\",\n                        \"topic_id\": topic_id,\n                    },\n                )\n                name = f\"Topic {topic_id}\"\n\n            if not description:\n                logger.warning(\n                    \"Empty description from DSPy, using fallback\",\n                    extra={\n                        \"event\": \"labeler.dspy.empty_description\",\n                        \"topic_id\": topic_id,\n                    },\n                )\n                description = \"No description available\"\n\n            return {\"name\": name, \"description\": description}\n\n        except Exception as e:\n            logger.warning(\n                \"Failed to label topic with DSPy\",\n                extra={\n                    \"event\": \"labeler.dspy.error\",\n                    \"topic_id\": topic_id,\n                },\n                exc_info=True,\n            )\n            return {\"name\": f\"Topic {topic_id}\", \"description\": \"No description available\"}",
    "start_line": 14,
    "end_line": 101,
    "has_docstring": true,
    "docstring": "DSPy-backed implementation of TopicLabeler.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DspyTopicLabeler",
    "component_id": "src.ck_exporter.adapters.dspy_topic_labeler.DspyTopicLabeler"
  },
  "src.ck_exporter.adapters.fs_jsonl.read_jsonl": {
    "id": "src.ck_exporter.adapters.fs_jsonl.read_jsonl",
    "name": "read_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/fs_jsonl.py",
    "relative_path": "src/ck_exporter/adapters/fs_jsonl.py",
    "depends_on": [],
    "source_code": "def read_jsonl(path: Path) -> Iterable[dict[str, Any]]:\n    \"\"\"\n    Read JSONL file and yield each JSON object.\n\n    Args:\n        path: Path to JSONL file\n\n    Yields:\n        Dict objects parsed from each line\n    \"\"\"\n    if not path.exists():\n        return\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            if isinstance(obj, dict):\n                yield obj",
    "start_line": 8,
    "end_line": 30,
    "has_docstring": true,
    "docstring": "Read JSONL file and yield each JSON object.\n\nArgs:\n    path: Path to JSONL file\n\nYields:\n    Dict objects parsed from each line",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function read_jsonl",
    "component_id": "src.ck_exporter.adapters.fs_jsonl.read_jsonl"
  },
  "src.ck_exporter.adapters.fs_jsonl.write_jsonl": {
    "id": "src.ck_exporter.adapters.fs_jsonl.write_jsonl",
    "name": "write_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/fs_jsonl.py",
    "relative_path": "src/ck_exporter/adapters/fs_jsonl.py",
    "depends_on": [],
    "source_code": "def write_jsonl(path: Path, rows: list[dict[str, Any]]) -> None:\n    \"\"\"\n    Write list of dicts to JSONL file (atomic write via temp file).\n\n    Args:\n        path: Output path\n        rows: List of dict objects to write\n    \"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Write to temp file first, then rename (atomic write)\n    temp_path = path.with_suffix(path.suffix + \".tmp\")\n    with temp_path.open(\"w\", encoding=\"utf-8\") as f:\n        for row in rows:\n            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n    temp_path.replace(path)",
    "start_line": 33,
    "end_line": 48,
    "has_docstring": true,
    "docstring": "Write list of dicts to JSONL file (atomic write via temp file).\n\nArgs:\n    path: Output path\n    rows: List of dict objects to write",
    "parameters": [
      "path",
      "rows"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function write_jsonl",
    "component_id": "src.ck_exporter.adapters.fs_jsonl.write_jsonl"
  },
  "src.ck_exporter.adapters.fs_jsonl.load_atoms_jsonl": {
    "id": "src.ck_exporter.adapters.fs_jsonl.load_atoms_jsonl",
    "name": "load_atoms_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/fs_jsonl.py",
    "relative_path": "src/ck_exporter/adapters/fs_jsonl.py",
    "depends_on": [],
    "source_code": "def load_atoms_jsonl(file_path: Path) -> list[dict[str, Any]]:\n    \"\"\"\n    Load atoms from a JSONL file.\n\n    Args:\n        file_path: Path to JSONL file\n\n    Returns:\n        List of atom dicts\n    \"\"\"\n    if not file_path.exists():\n        return []\n\n    atoms = []\n    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                try:\n                    atoms.append(json.loads(line))\n                except json.JSONDecodeError:\n                    continue\n\n    return atoms",
    "start_line": 51,
    "end_line": 74,
    "has_docstring": true,
    "docstring": "Load atoms from a JSONL file.\n\nArgs:\n    file_path: Path to JSONL file\n\nReturns:\n    List of atom dicts",
    "parameters": [
      "file_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function load_atoms_jsonl",
    "component_id": "src.ck_exporter.adapters.fs_jsonl.load_atoms_jsonl"
  },
  "src.ck_exporter.adapters.hybrid_atom_extractor.HybridAtomExtractor": {
    "id": "src.ck_exporter.adapters.hybrid_atom_extractor.HybridAtomExtractor",
    "name": "HybridAtomExtractor",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/hybrid_atom_extractor.py",
    "relative_path": "src/ck_exporter/adapters/hybrid_atom_extractor.py",
    "depends_on": [],
    "source_code": "class HybridAtomExtractor:\n    \"\"\"\n    Hybrid atom extractor that combines OpenRouter (Pass 1) and DSPy (Pass 2).\n\n    This allows using DSPy for refinement while keeping the proven OpenRouter\n    extraction logic for chunk processing.\n    \"\"\"\n\n    def __init__(\n        self,\n        openrouter_extractor: OpenRouterAtomExtractor,\n        dspy_refiner: DspyAtomRefiner,\n    ):\n        \"\"\"\n        Initialize hybrid extractor.\n\n        Args:\n            openrouter_extractor: OpenRouter extractor for Pass 1\n            dspy_refiner: DSPy refiner for Pass 2\n        \"\"\"\n        self.openrouter_extractor = openrouter_extractor\n        self.dspy_refiner = dspy_refiner\n\n    def extract_from_chunk(self, chunk_text: str) -> dict[str, Any]:\n        \"\"\"\n        Extract candidate atoms from a conversation chunk (Pass 1).\n\n        Delegates to OpenRouter extractor.\n\n        Args:\n            chunk_text: Formatted conversation chunk text\n\n        Returns:\n            Dict with keys: \"facts\", \"decisions\", \"open_questions\"\n        \"\"\"\n        return self.openrouter_extractor.extract_from_chunk(chunk_text)\n\n    def refine_atoms(\n        self,\n        all_candidates: dict[str, list[dict[str, Any]]],\n        conversation_id: str,\n        conversation_title: str | None,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Refine and consolidate candidate atoms (Pass 2).\n\n        Delegates to DSPy refiner.\n\n        Args:\n            all_candidates: Dict with \"facts\", \"decisions\", \"open_questions\" lists\n            conversation_id: Conversation identifier\n            conversation_title: Optional conversation title\n\n        Returns:\n            Dict with refined/deduplicated atoms\n        \"\"\"\n        return self.dspy_refiner.refine_atoms(all_candidates, conversation_id, conversation_title)",
    "start_line": 10,
    "end_line": 66,
    "has_docstring": true,
    "docstring": "Hybrid atom extractor that combines OpenRouter (Pass 1) and DSPy (Pass 2).\n\nThis allows using DSPy for refinement while keeping the proven OpenRouter\nextraction logic for chunk processing.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class HybridAtomExtractor",
    "component_id": "src.ck_exporter.adapters.hybrid_atom_extractor.HybridAtomExtractor"
  },
  "src.ck_exporter.adapters.openrouter_atom_extractor.OpenRouterAtomExtractor": {
    "id": "src.ck_exporter.adapters.openrouter_atom_extractor.OpenRouterAtomExtractor",
    "name": "OpenRouterAtomExtractor",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_atom_extractor.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_atom_extractor.py",
    "depends_on": [
      "src.ck_exporter.utils.atom_candidates.deduplicate_candidates",
      "src.ck_exporter.programs.json_extract.extract_json_from_text"
    ],
    "source_code": "class OpenRouterAtomExtractor:\n    \"\"\"OpenRouter-backed implementation of AtomExtractor.\"\"\"\n\n    def __init__(\n        self,\n        fast_llm: LLMClient,\n        big_llm: LLMClient,\n        fast_model: str = \"z-ai/glm-4.7\",\n        big_model: str = \"z-ai/glm-4.7\",\n    ):\n        \"\"\"\n        Initialize atom extractor.\n\n        Args:\n            fast_llm: LLM client for Pass 1 (fast extraction)\n            big_llm: LLM client for Pass 2 (refinement)\n            fast_model: Model identifier for Pass 1\n            big_model: Model identifier for Pass 2\n        \"\"\"\n        self.fast_llm = fast_llm\n        self.big_llm = big_llm\n        self.fast_model = fast_model\n        self.big_model = big_model\n\n    def extract_from_chunk(self, chunk_text: str) -> dict[str, Any]:\n        \"\"\"\n        Extract candidate atoms from a conversation chunk (Pass 1).\n\n        Args:\n            chunk_text: Formatted conversation chunk text\n\n        Returns:\n            Dict with keys: \"facts\", \"decisions\", \"open_questions\"\n        \"\"\"\n        prompt = PASS1_EXTRACTION_PROMPT.format(chunk_text=chunk_text)\n\n        try:\n            # Try with json_object=True first (structured output reduces repair calls)\n            try:\n                content = self.fast_llm.chat(\n                    model=self.fast_model,\n                    system=\"You are a knowledge extraction assistant. Return only valid JSON, no markdown, no code blocks.\",\n                    user=prompt,\n                    temperature=0.3,\n                    json_object=True,\n                )\n            except Exception as e:\n                # If json_object is not supported (e.g., 400 Bad Request), retry without it\n                error_str = str(e).lower()\n                if \"response_format\" in error_str or \"json_object\" in error_str or \"400\" in error_str:\n                    logger.debug(\n                        \"json_object not supported, falling back to regular mode\",\n                        extra={\"event\": \"extractor.pass1.json_object_fallback\", \"error\": str(e)},\n                    )\n                    content = self.fast_llm.chat(\n                        model=self.fast_model,\n                        system=\"You are a knowledge extraction assistant. Return only valid JSON, no markdown, no code blocks.\",\n                        user=prompt,\n                        temperature=0.3,\n                        json_object=False,\n                    )\n                else:\n                    # Re-raise if it's a different error\n                    raise\n\n            if not content:\n                return {\"facts\": [], \"decisions\": [], \"open_questions\": []}\n\n            # Try parsing JSON directly\n            result = json.loads(content)\n            if isinstance(result, dict):\n                return {\n                    \"facts\": result.get(\"facts\", []),\n                    \"decisions\": result.get(\"decisions\", []),\n                    \"open_questions\": result.get(\"open_questions\", []),\n                }\n\n            # Try extracting from markdown code blocks\n            result = extract_json_from_text(content)\n            if result:\n                return {\n                    \"facts\": result.get(\"facts\", []),\n                    \"decisions\": result.get(\"decisions\", []),\n                    \"open_questions\": result.get(\"open_questions\", []),\n                }\n\n            # Last resort: retry with repair prompt\n            logger.warning(\n                \"JSON parse failed, attempting repair\",\n                extra={\"event\": \"extractor.pass1.json_repair\"},\n            )\n            repair_content = self.fast_llm.chat(\n                model=self.fast_model,\n                system=\"You are a JSON repair assistant. Extract and return ONLY valid JSON, no other text.\",\n                user=f\"Repair this JSON output to be valid:\\n\\n{content}\",\n                temperature=0.1,\n            )\n            if repair_content:\n                result = extract_json_from_text(repair_content) or json.loads(repair_content)\n                if result:\n                    return {\n                        \"facts\": result.get(\"facts\", []),\n                        \"decisions\": result.get(\"decisions\", []),\n                        \"open_questions\": result.get(\"open_questions\", []),\n                    }\n\n            logger.error(\n                \"Failed to parse JSON after repair\",\n                extra={\n                    \"event\": \"extractor.pass1.json_parse_failed\",\n                    \"response_preview\": content[:200] if content else None,\n                },\n            )\n            return {\"facts\": [], \"decisions\": [], \"open_questions\": []}\n\n        except Exception as e:\n            logger.exception(\n                \"Error in fast extraction\",\n                extra={\"event\": \"extractor.pass1.error\"},\n            )\n            return {\"facts\": [], \"decisions\": [], \"open_questions\": []}\n\n    def refine_atoms(\n        self,\n        all_candidates: dict[str, list[dict[str, Any]]],\n        conversation_id: str,\n        conversation_title: str | None,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Refine and consolidate candidate atoms (Pass 2).\n\n        Args:\n            all_candidates: Dict with \"facts\", \"decisions\", \"open_questions\" lists\n            conversation_id: Conversation identifier\n            conversation_title: Optional conversation title\n\n        Returns:\n            Dict with refined/deduplicated atoms\n        \"\"\"\n        # Pre-deduplicate candidates locally to reduce payload size\n        deduplicated = deduplicate_candidates(all_candidates)\n\n        # Prepare candidates JSON for prompt (compact format to reduce tokens)\n        candidates_json = json.dumps(\n            deduplicated, ensure_ascii=False, separators=(\",\", \":\")\n        )\n\n        prompt = PASS2_REFINEMENT_PROMPT.format(\n            conversation_id=conversation_id,\n            conversation_title=conversation_title or \"Unknown\",\n            all_candidates_json=candidates_json,\n        )\n\n        try:\n            content = self.big_llm.chat(\n                model=self.big_model,\n                system=\"You are a knowledge refinement assistant. Return only valid JSON matching the schema, no markdown, no code blocks.\",\n                user=prompt,\n                temperature=0.2,\n                json_object=True,\n            )\n\n            if not content:\n                logger.warning(\n                    \"Empty response from refinement, using candidates as-is\",\n                    extra={\n                        \"event\": \"extractor.pass2.empty_response\",\n                        \"conversation_id\": conversation_id,\n                    },\n                )\n                return all_candidates\n\n            result = json.loads(content)\n            if not isinstance(result, dict):\n                logger.warning(\n                    \"Invalid response format, using candidates as-is\",\n                    extra={\n                        \"event\": \"extractor.pass2.invalid_format\",\n                        \"conversation_id\": conversation_id,\n                    },\n                )\n                return all_candidates\n\n            # Ensure all keys are present\n            return {\n                \"facts\": result.get(\"facts\", []),\n                \"decisions\": result.get(\"decisions\", []),\n                \"open_questions\": result.get(\"open_questions\", []),\n            }\n\n        except Exception as e:\n            logger.exception(\n                \"Error in refinement, falling back to candidates\",\n                extra={\n                    \"event\": \"extractor.pass2.error\",\n                    \"conversation_id\": conversation_id,\n                },\n            )\n            return all_candidates",
    "start_line": 19,
    "end_line": 217,
    "has_docstring": true,
    "docstring": "OpenRouter-backed implementation of AtomExtractor.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OpenRouterAtomExtractor",
    "component_id": "src.ck_exporter.adapters.openrouter_atom_extractor.OpenRouterAtomExtractor"
  },
  "src.ck_exporter.adapters.openrouter_client.make_openrouter_client": {
    "id": "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
    "name": "make_openrouter_client",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_client.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_client.py",
    "depends_on": [],
    "source_code": "def make_openrouter_client(use_openrouter: bool = True) -> OpenAI:\n    \"\"\"\n    Create an OpenAI-compatible client for OpenRouter or standard OpenAI.\n\n    Args:\n        use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n\n    Returns:\n        Configured OpenAI client\n    \"\"\"\n    if use_openrouter:\n        api_key = os.getenv(\"OPENROUTER_API_KEY\") or os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"OPENROUTER_API_KEY or OPENAI_API_KEY must be set in environment\")\n\n        # Optional attribution headers (recommended by OpenRouter)\n        extra_headers = {}\n        http_referer = os.getenv(\"OPENROUTER_HTTP_REFERER\")\n        x_title = os.getenv(\"OPENROUTER_X_TITLE\", \"ChatGPT Conversation Knowledge Exporter\")\n        if http_referer:\n            extra_headers[\"HTTP-Referer\"] = http_referer\n        if x_title:\n            extra_headers[\"X-Title\"] = x_title\n\n        return OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n            default_headers=extra_headers if extra_headers else None,\n        )\n    else:\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise ValueError(\"OPENAI_API_KEY must be set in environment\")\n        return OpenAI(api_key=api_key)",
    "start_line": 12,
    "end_line": 45,
    "has_docstring": true,
    "docstring": "Create an OpenAI-compatible client for OpenRouter or standard OpenAI.\n\nArgs:\n    use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n\nReturns:\n    Configured OpenAI client",
    "parameters": [
      "use_openrouter"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function make_openrouter_client",
    "component_id": "src.ck_exporter.adapters.openrouter_client.make_openrouter_client"
  },
  "src.ck_exporter.adapters.openrouter_embedder.OpenRouterEmbedder": {
    "id": "src.ck_exporter.adapters.openrouter_embedder.OpenRouterEmbedder",
    "name": "OpenRouterEmbedder",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_embedder.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_embedder.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.utils.chunking.chunk_text"
    ],
    "source_code": "class OpenRouterEmbedder:\n    \"\"\"OpenRouter-backed implementation of Embedder.\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"openai/text-embedding-3-small\",\n        use_openrouter: bool = True,\n        client: Optional[OpenAI] = None,\n    ):\n        \"\"\"\n        Initialize embedding client.\n\n        Args:\n            model: Model identifier (OpenRouter format)\n            use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n            client: Optional pre-configured OpenAI client (for testing/sharing)\n        \"\"\"\n        self.model = model\n        self._client = client if client is not None else make_openrouter_client(use_openrouter)\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    def _get_embeddings_batch(self, texts: list[str]) -> np.ndarray:\n        \"\"\"\n        Get embeddings for a batch of texts (up to 100).\n\n        Args:\n            texts: List of text strings to embed\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        if not texts:\n            return np.array([])\n\n        try:\n            response = self._client.embeddings.create(\n                model=self.model,\n                input=texts,\n            )\n\n            embeddings = [item.embedding for item in response.data]\n            return np.array(embeddings)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to get embeddings: {e}\") from e\n\n    def embed(self, texts: list[str], batch_size: int = 100) -> np.ndarray:\n        \"\"\"\n        Get embeddings for multiple texts, batching as needed.\n\n        Args:\n            texts: List of text strings to embed\n            batch_size: Maximum texts per API call (default 100)\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        if not texts:\n            return np.array([])\n\n        all_embeddings = []\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i : i + batch_size]\n            batch_embeddings = self._get_embeddings_batch(batch)\n            all_embeddings.append(batch_embeddings)\n\n        if not all_embeddings:\n            return np.array([])\n\n        return np.vstack(all_embeddings)\n\n    def _get_cache_key(self, text: str, pooling_version: str = POOLING_VERSION) -> str:\n        \"\"\"Generate cache key for a text chunk.\"\"\"\n        key_string = f\"{self.model}:{pooling_version}:{text}\"\n        return hashlib.sha256(key_string.encode(\"utf-8\")).hexdigest()\n\n    def _load_from_cache(self, cache_dir: Path, cache_key: str) -> Optional[np.ndarray]:\n        \"\"\"Load embedding from cache if it exists.\"\"\"\n        if cache_dir is None:\n            return None\n        cache_file = cache_dir / f\"{cache_key}.npy\"\n        if cache_file.exists():\n            try:\n                return np.load(cache_file)\n            except Exception:\n                return None\n        return None\n\n    def _save_to_cache(self, cache_dir: Path, cache_key: str, embedding: np.ndarray) -> None:\n        \"\"\"Save embedding to cache.\"\"\"\n        if cache_dir is None:\n            return\n        try:\n            cache_dir.mkdir(parents=True, exist_ok=True)\n            cache_file = cache_dir / f\"{cache_key}.npy\"\n            np.save(cache_file, embedding)\n        except Exception:\n            # Cache failures shouldn't break the pipeline\n            pass\n\n    def _normalized_mean_pool(self, chunk_embeddings: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Pool chunk embeddings using normalized mean pooling.\n\n        Args:\n            chunk_embeddings: Array of shape (n_chunks, embedding_dim)\n\n        Returns:\n            Pooled vector of shape (embedding_dim,)\n        \"\"\"\n        if len(chunk_embeddings) == 0:\n            raise ValueError(\"Cannot pool empty chunk embeddings\")\n        if len(chunk_embeddings) == 1:\n            # Single chunk: just normalize it\n            vec = chunk_embeddings[0]\n            norm = np.linalg.norm(vec)\n            return vec / norm if norm > 0 else vec\n\n        # L2-normalize each chunk vector\n        norms = np.linalg.norm(chunk_embeddings, axis=1, keepdims=True)\n        # Avoid division by zero\n        norms = np.where(norms == 0, 1.0, norms)\n        normalized = chunk_embeddings / norms\n\n        # Mean pool\n        pooled = np.mean(normalized, axis=0)\n\n        # L2-normalize the final pooled vector\n        pooled_norm = np.linalg.norm(pooled)\n        if pooled_norm > 0:\n            pooled = pooled / pooled_norm\n\n        return pooled\n\n    def _map_embedding_model_to_tokenizer(self, embedding_model: str) -> str:\n        \"\"\"\n        Map embedding model name to a tiktoken-compatible model name.\n\n        Args:\n            embedding_model: Embedding model identifier (e.g., \"openai/text-embedding-3-small\")\n\n        Returns:\n            Tiktoken-compatible model name\n        \"\"\"\n        # OpenAI embedding models use cl100k_base encoding (same as GPT-3.5/GPT-4)\n        if \"openai\" in embedding_model.lower() or \"text-embedding\" in embedding_model.lower():\n            return \"gpt-4\"\n        # Default fallback\n        return \"gpt-4\"\n\n    def embed_pooled(\n        self,\n        texts: list[str],\n        chunk_tokens: int = 600,\n        overlap_tokens: int = 80,\n        pooling: str = \"mean\",\n        cache_dir: Optional[Path] = None,\n        batch_size: int = 100,\n    ) -> np.ndarray:\n        \"\"\"\n        Get pooled embeddings for multiple texts by chunking, embedding chunks, and pooling.\n\n        Args:\n            texts: List of text strings to embed\n            chunk_tokens: Maximum tokens per chunk (default 600)\n            overlap_tokens: Token overlap between chunks (default 80)\n            pooling: Pooling method (\"mean\" for normalized mean pooling)\n            cache_dir: Optional directory for caching embeddings\n            batch_size: Maximum chunks per API call (default 100)\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        if not texts:\n            return np.array([])\n\n        if pooling != \"mean\":\n            raise ValueError(f\"Unsupported pooling method: {pooling}. Only 'mean' is supported.\")\n\n        tokenizer_model = self._map_embedding_model_to_tokenizer(self.model)\n\n        # Chunk all texts\n        all_chunks = []\n        chunk_to_doc_idx = []  # Maps chunk index to original document index\n\n        for doc_idx, text in enumerate(texts):\n            chunks = chunk_text(\n                text,\n                max_tokens=chunk_tokens,\n                overlap_tokens=overlap_tokens,\n                model=tokenizer_model,\n            )\n            # Filter out empty chunks\n            chunks = [chunk for chunk in chunks if chunk.strip()]\n            if not chunks:\n                # Empty text: skip this document (will handle later)\n                chunks = []\n            all_chunks.extend(chunks)\n            chunk_to_doc_idx.extend([doc_idx] * len(chunks))\n\n        if not all_chunks:\n            return np.array([])\n\n        # Get embeddings for all chunks (with caching)\n        chunk_embeddings_list = []\n        chunks_to_embed = []\n        chunk_indices_to_embed = []\n\n        for chunk_idx, chunk in enumerate(all_chunks):\n            cache_key = self._get_cache_key(chunk)\n            cached_embedding = self._load_from_cache(cache_dir, cache_key)\n\n            if cached_embedding is not None:\n                chunk_embeddings_list.append((chunk_idx, cached_embedding))\n            else:\n                chunks_to_embed.append(chunk)\n                chunk_indices_to_embed.append(chunk_idx)\n\n        # Embed uncached chunks in batches\n        if chunks_to_embed:\n            all_new_embeddings = []\n            for i in range(0, len(chunks_to_embed), batch_size):\n                batch = chunks_to_embed[i : i + batch_size]\n                batch_indices = chunk_indices_to_embed[i : i + batch_size]\n                batch_embeddings = self._get_embeddings_batch(batch)\n\n                # Save to cache\n                for chunk_idx, embedding in zip(batch_indices, batch_embeddings):\n                    chunk = all_chunks[chunk_idx]\n                    cache_key = self._get_cache_key(chunk)\n                    self._save_to_cache(cache_dir, cache_key, embedding)\n\n                all_new_embeddings.append(batch_embeddings)\n\n            if all_new_embeddings:\n                new_embeddings = np.vstack(all_new_embeddings)\n                for chunk_idx, embedding in zip(chunk_indices_to_embed, new_embeddings):\n                    chunk_embeddings_list.append((chunk_idx, embedding))\n\n        # Sort by chunk index to maintain order\n        chunk_embeddings_list.sort(key=lambda x: x[0])\n        chunk_embeddings_array = np.array([emb for _, emb in chunk_embeddings_list])\n\n        # Group chunks by document and pool\n        num_docs = len(texts)\n        pooled_embeddings = []\n\n        # Get embedding dimension from first chunk (if available)\n        embedding_dim = None\n        if len(chunk_embeddings_array) > 0:\n            embedding_dim = chunk_embeddings_array.shape[1]\n\n        for doc_idx in range(num_docs):\n            # Find all chunks belonging to this document\n            doc_chunk_indices = [\n                i for i, orig_doc_idx in enumerate(chunk_to_doc_idx) if orig_doc_idx == doc_idx\n            ]\n            if not doc_chunk_indices:\n                # No chunks for this doc (empty or filtered out)\n                # Use a zero vector with same dimension as other embeddings\n                if embedding_dim is not None:\n                    zero_vec = np.zeros(embedding_dim)\n                    pooled_embeddings.append(zero_vec)\n                else:\n                    # No embeddings at all - this is an edge case\n                    # We'll need to get dimension from API, but for now skip\n                    # This document will be missing from results\n                    continue\n            else:\n                doc_chunk_embeddings = chunk_embeddings_array[doc_chunk_indices]\n                pooled = self._normalized_mean_pool(doc_chunk_embeddings)\n                pooled_embeddings.append(pooled)\n\n        if not pooled_embeddings:\n            return np.array([])\n\n        return np.array(pooled_embeddings)",
    "start_line": 19,
    "end_line": 294,
    "has_docstring": true,
    "docstring": "OpenRouter-backed implementation of Embedder.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OpenRouterEmbedder",
    "component_id": "src.ck_exporter.adapters.openrouter_embedder.OpenRouterEmbedder"
  },
  "src.ck_exporter.adapters.openrouter_embedder.cosine_similarity": {
    "id": "src.ck_exporter.adapters.openrouter_embedder.cosine_similarity",
    "name": "cosine_similarity",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_embedder.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_embedder.py",
    "depends_on": [],
    "source_code": "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"\n    Compute cosine similarity between two vectors.\n\n    Args:\n        a: First vector (1D array)\n        b: Second vector (1D array)\n\n    Returns:\n        Cosine similarity score between 0 and 1\n    \"\"\"\n    if a.shape != b.shape:\n        raise ValueError(f\"Vectors must have same shape, got {a.shape} and {b.shape}\")\n\n    dot_product = np.dot(a, b)\n    norm_a = np.linalg.norm(a)\n    norm_b = np.linalg.norm(b)\n\n    if norm_a == 0 or norm_b == 0:\n        return 0.0\n\n    similarity = dot_product / (norm_a * norm_b)\n    # Clamp to [0, 1] in case of floating point errors\n    return max(0.0, min(1.0, similarity))",
    "start_line": 297,
    "end_line": 320,
    "has_docstring": true,
    "docstring": "Compute cosine similarity between two vectors.\n\nArgs:\n    a: First vector (1D array)\n    b: Second vector (1D array)\n\nReturns:\n    Cosine similarity score between 0 and 1",
    "parameters": [
      "a",
      "b"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cosine_similarity",
    "component_id": "src.ck_exporter.adapters.openrouter_embedder.cosine_similarity"
  },
  "src.ck_exporter.adapters.openrouter_llm._get_inflight_semaphore": {
    "id": "src.ck_exporter.adapters.openrouter_llm._get_inflight_semaphore",
    "name": "_get_inflight_semaphore",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_llm.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_llm.py",
    "depends_on": [],
    "source_code": "def _get_inflight_semaphore() -> threading.BoundedSemaphore:\n    \"\"\"Get or create the global in-flight request semaphore.\"\"\"\n    global _inflight_semaphore\n    if _inflight_semaphore is None:\n        with _inflight_lock:\n            if _inflight_semaphore is None:\n                # Default: derive from CKX_MAX_CONCURRENCY or use conservative default\n                max_concurrency = int(os.getenv(\"CKX_MAX_CONCURRENCY\", \"8\"))\n                max_inflight = int(os.getenv(\"CKX_LLM_MAX_INFLIGHT\", str(max_concurrency * 4)))\n                _inflight_semaphore = threading.BoundedSemaphore(max_inflight)\n    return _inflight_semaphore",
    "start_line": 18,
    "end_line": 28,
    "has_docstring": true,
    "docstring": "Get or create the global in-flight request semaphore.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _get_inflight_semaphore",
    "component_id": "src.ck_exporter.adapters.openrouter_llm._get_inflight_semaphore"
  },
  "src.ck_exporter.adapters.openrouter_llm.OpenRouterLLMClient": {
    "id": "src.ck_exporter.adapters.openrouter_llm.OpenRouterLLMClient",
    "name": "OpenRouterLLMClient",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_llm.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_llm.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.adapters.openrouter_llm._get_inflight_semaphore"
    ],
    "source_code": "class OpenRouterLLMClient:\n    \"\"\"OpenRouter-backed implementation of LLMClient.\"\"\"\n\n    def __init__(self, use_openrouter: bool = True, client: Optional[OpenAI] = None):\n        \"\"\"\n        Initialize OpenRouter LLM client.\n\n        Args:\n            use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n            client: Optional pre-configured OpenAI client (for testing/sharing)\n        \"\"\"\n        self._client = client if client is not None else make_openrouter_client(use_openrouter)\n        self._semaphore = _get_inflight_semaphore()\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    def chat(\n        self,\n        model: str,\n        system: str,\n        user: str,\n        *,\n        temperature: float = 0.3,\n        json_object: bool = False,\n    ) -> str:\n        \"\"\"\n        Generate a chat completion.\n\n        Args:\n            model: Model identifier (e.g., \"z-ai/glm-4.7\")\n            system: System message content\n            user: User message content\n            temperature: Sampling temperature (default 0.3)\n            json_object: If True, request JSON object response format\n\n        Returns:\n            Generated text content\n        \"\"\"\n        kwargs = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system},\n                {\"role\": \"user\", \"content\": user},\n            ],\n            \"temperature\": temperature,\n        }\n        if json_object:\n            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n\n        # Acquire semaphore to limit in-flight requests\n        self._semaphore.acquire()\n        try:\n            response = self._client.chat.completions.create(**kwargs)\n            content = response.choices[0].message.content\n            return content if content else \"\"\n        finally:\n            self._semaphore.release()",
    "start_line": 31,
    "end_line": 86,
    "has_docstring": true,
    "docstring": "OpenRouter-backed implementation of LLMClient.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OpenRouterLLMClient",
    "component_id": "src.ck_exporter.adapters.openrouter_llm.OpenRouterLLMClient"
  },
  "src.ck_exporter.adapters.openrouter_topic_labeler.OpenRouterTopicLabeler": {
    "id": "src.ck_exporter.adapters.openrouter_topic_labeler.OpenRouterTopicLabeler",
    "name": "OpenRouterTopicLabeler",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/adapters/openrouter_topic_labeler.py",
    "relative_path": "src/ck_exporter/adapters/openrouter_topic_labeler.py",
    "depends_on": [
      "src.ck_exporter.programs.json_extract.extract_json_from_text"
    ],
    "source_code": "class OpenRouterTopicLabeler:\n    \"\"\"OpenRouter-backed implementation of TopicLabeler.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLMClient,\n        model: str = \"z-ai/glm-4.7\",\n    ):\n        \"\"\"\n        Initialize topic labeler.\n\n        Args:\n            llm: LLM client for labeling\n            model: Model identifier for labeling\n        \"\"\"\n        self.llm = llm\n        self.model = model\n\n    def label_topic(\n        self,\n        topic_id: int,\n        representative_docs: list[tuple[str, str]],\n        keywords: list[str],\n    ) -> dict[str, str]:\n        \"\"\"\n        Generate a label (name + description) for a topic cluster.\n\n        Args:\n            topic_id: Numeric topic identifier\n            representative_docs: List of (conversation_id, doc_text) tuples\n            keywords: Top keywords from topic model\n\n        Returns:\n            Dict with \"name\" and \"description\" keys\n        \"\"\"\n        # Build prompt for LLM labeling\n        doc_samples = \"\\n\\n---\\n\\n\".join(\n            [f\"Conversation ID: {conv_id}\\n\\n{doc_text[:500]}...\" for conv_id, doc_text in representative_docs]\n        )\n\n        prompt = f\"\"\"You are analyzing a topic cluster discovered from conversation data.\n\nHere are 3 representative conversations from this topic cluster:\n\n{doc_samples}\n\nBased on these conversations, generate:\n1. A short topic name (3-5 words) that captures the main theme\n2. A 1-2 sentence description of what this topic is about\n\nReturn ONLY valid JSON with this structure:\n{{\n  \"name\": \"Topic Name Here\",\n  \"description\": \"Description here.\"\n}}\"\"\"\n\n        try:\n            content = self.llm.chat(\n                model=self.model,\n                system=\"You are a topic labeling assistant. Return only valid JSON.\",\n                user=prompt,\n                temperature=0.3,\n            )\n\n            if content:\n                # Try to extract JSON\n                if \"```json\" in content:\n                    start = content.find(\"```json\") + 7\n                    end = content.find(\"```\", start)\n                    if end > start:\n                        content = content[start:end].strip()\n                elif \"```\" in content:\n                    start = content.find(\"```\") + 3\n                    end = content.find(\"```\", start)\n                    if end > start:\n                        content = content[start:end].strip()\n\n                label_data = extract_json_from_text(content) or json.loads(content)\n                name = label_data.get(\"name\", f\"Topic {topic_id}\")\n                description = label_data.get(\"description\", \"No description available\")\n            else:\n                name = f\"Topic {topic_id}\"\n                description = \"No description available\"\n        except Exception as e:\n            logger.warning(\n                \"Failed to label topic\",\n                extra={\n                    \"event\": \"labeler.topic.error\",\n                    \"topic_id\": topic_id,\n                },\n                exc_info=True,\n            )\n            name = f\"Topic {topic_id}\"\n            description = \"No description available\"\n\n        return {\"name\": name, \"description\": description}",
    "start_line": 14,
    "end_line": 109,
    "has_docstring": true,
    "docstring": "OpenRouter-backed implementation of TopicLabeler.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OpenRouterTopicLabeler",
    "component_id": "src.ck_exporter.adapters.openrouter_topic_labeler.OpenRouterTopicLabeler"
  },
  "src.ck_exporter.bootstrap.build_llm_client": {
    "id": "src.ck_exporter.bootstrap.build_llm_client",
    "name": "build_llm_client",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/bootstrap.py",
    "relative_path": "src/ck_exporter/bootstrap.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.adapters.openrouter_llm.OpenRouterLLMClient"
    ],
    "source_code": "def build_llm_client(use_openrouter: bool = True, client: Optional[object] = None) -> LLMClient:\n    \"\"\"\n    Build an LLM client adapter.\n\n    Args:\n        use_openrouter: Whether to use OpenRouter (default True)\n        client: Optional pre-configured OpenAI client (for sharing)\n\n    Returns:\n        Configured LLMClient instance\n    \"\"\"\n    if client is None:\n        client = make_openrouter_client(use_openrouter)\n    return OpenRouterLLMClient(use_openrouter=use_openrouter, client=client)",
    "start_line": 27,
    "end_line": 40,
    "has_docstring": true,
    "docstring": "Build an LLM client adapter.\n\nArgs:\n    use_openrouter: Whether to use OpenRouter (default True)\n    client: Optional pre-configured OpenAI client (for sharing)\n\nReturns:\n    Configured LLMClient instance",
    "parameters": [
      "use_openrouter",
      "client"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function build_llm_client",
    "component_id": "src.ck_exporter.bootstrap.build_llm_client"
  },
  "src.ck_exporter.bootstrap.build_embedder": {
    "id": "src.ck_exporter.bootstrap.build_embedder",
    "name": "build_embedder",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/bootstrap.py",
    "relative_path": "src/ck_exporter/bootstrap.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.adapters.openrouter_embedder.OpenRouterEmbedder"
    ],
    "source_code": "def build_embedder(\n    model: str = \"openai/text-embedding-3-small\",\n    use_openrouter: bool = True,\n    client: Optional[object] = None,\n) -> Embedder:\n    \"\"\"\n    Build an embedder adapter.\n\n    Args:\n        model: Embedding model identifier\n        use_openrouter: Whether to use OpenRouter (default True)\n        client: Optional pre-configured OpenAI client (for sharing)\n\n    Returns:\n        Configured Embedder instance\n    \"\"\"\n    if client is None:\n        client = make_openrouter_client(use_openrouter)\n    return OpenRouterEmbedder(model=model, use_openrouter=use_openrouter, client=client)",
    "start_line": 43,
    "end_line": 61,
    "has_docstring": true,
    "docstring": "Build an embedder adapter.\n\nArgs:\n    model: Embedding model identifier\n    use_openrouter: Whether to use OpenRouter (default True)\n    client: Optional pre-configured OpenAI client (for sharing)\n\nReturns:\n    Configured Embedder instance",
    "parameters": [
      "model",
      "use_openrouter",
      "client"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function build_embedder",
    "component_id": "src.ck_exporter.bootstrap.build_embedder"
  },
  "src.ck_exporter.bootstrap.build_atom_extractor": {
    "id": "src.ck_exporter.bootstrap.build_atom_extractor",
    "name": "build_atom_extractor",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/bootstrap.py",
    "relative_path": "src/ck_exporter/bootstrap.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.adapters.openrouter_atom_extractor.OpenRouterAtomExtractor",
      "src.ck_exporter.adapters.hybrid_atom_extractor.HybridAtomExtractor",
      "src.ck_exporter.bootstrap.build_llm_client",
      "src.ck_exporter.adapters.dspy_atom_refiner.DspyAtomRefiner"
    ],
    "source_code": "def build_atom_extractor(\n    fast_model: Optional[str] = None,\n    big_model: Optional[str] = None,\n    use_openrouter: bool = True,\n    shared_client: Optional[object] = None,\n) -> AtomExtractor:\n    \"\"\"\n    Build an atom extractor adapter based on environment configuration.\n\n    Reads `CKX_ATOM_REFINER_IMPL` to choose between:\n    - `openrouter`: OpenRouter for both Pass 1 and Pass 2 (default)\n    - `dspy`: OpenRouter for Pass 1, DSPy for Pass 2 (hybrid)\n\n    Args:\n        fast_model: Model for Pass 1 (defaults to env or \"z-ai/glm-4.7\")\n        big_model: Model for Pass 2 (defaults to env or \"z-ai/glm-4.7\")\n        use_openrouter: Whether to use OpenRouter (default True)\n        shared_client: Optional pre-configured OpenAI client (for efficiency)\n\n    Returns:\n        Configured AtomExtractor instance\n    \"\"\"\n    # Set defaults from environment or hardcoded defaults\n    fast_model = fast_model or os.getenv(\"CKX_FAST_MODEL\", \"z-ai/glm-4.7\")\n    big_model = big_model or os.getenv(\"CKX_BIG_MODEL\", \"z-ai/glm-4.7\")\n\n    # Create shared client if not provided\n    if shared_client is None:\n        shared_client = make_openrouter_client(use_openrouter)\n\n    # Create LLM clients\n    fast_llm = build_llm_client(use_openrouter=use_openrouter, client=shared_client)\n    big_llm = build_llm_client(use_openrouter=use_openrouter, client=shared_client)\n\n    # Check if DSPy refinement is requested\n    refiner_impl = os.getenv(\"CKX_ATOM_REFINER_IMPL\", \"openrouter\").lower()\n\n    if refiner_impl == \"dspy\":\n        try:\n            # Create OpenRouter extractor for Pass 1\n            openrouter_extractor = OpenRouterAtomExtractor(\n                fast_llm=fast_llm,\n                big_llm=big_llm,  # Not used in hybrid mode, but required for constructor\n                fast_model=fast_model,\n                big_model=big_model,\n            )\n\n            # Create DSPy refiner for Pass 2\n            dspy_refiner = DspyAtomRefiner(use_openrouter=use_openrouter)\n\n            # Wrap in hybrid extractor\n            return HybridAtomExtractor(\n                openrouter_extractor=openrouter_extractor,\n                dspy_refiner=dspy_refiner,\n            )\n        except ImportError as e:\n            logger.warning(\n                \"DSPy atom refiner requested but not available, falling back to OpenRouter\",\n                extra={\n                    \"event\": \"bootstrap.dspy_refiner.unavailable\",\n                    \"error\": str(e),\n                },\n            )\n            # Fall through to default OpenRouter implementation\n\n    # Default: OpenRouter for both passes\n    return OpenRouterAtomExtractor(\n        fast_llm=fast_llm,\n        big_llm=big_llm,\n        fast_model=fast_model,\n        big_model=big_model,\n    )",
    "start_line": 64,
    "end_line": 135,
    "has_docstring": true,
    "docstring": "Build an atom extractor adapter based on environment configuration.\n\nReads `CKX_ATOM_REFINER_IMPL` to choose between:\n- `openrouter`: OpenRouter for both Pass 1 and Pass 2 (default)\n- `dspy`: OpenRouter for Pass 1, DSPy for Pass 2 (hybrid)\n\nArgs:\n    fast_model: Model for Pass 1 (defaults to env or \"z-ai/glm-4.7\")\n    big_model: Model for Pass 2 (defaults to env or \"z-ai/glm-4.7\")\n    use_openrouter: Whether to use OpenRouter (default True)\n    shared_client: Optional pre-configured OpenAI client (for efficiency)\n\nReturns:\n    Configured AtomExtractor instance",
    "parameters": [
      "fast_model",
      "big_model",
      "use_openrouter",
      "shared_client"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function build_atom_extractor",
    "component_id": "src.ck_exporter.bootstrap.build_atom_extractor"
  },
  "src.ck_exporter.bootstrap.build_topic_labeler": {
    "id": "src.ck_exporter.bootstrap.build_topic_labeler",
    "name": "build_topic_labeler",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/bootstrap.py",
    "relative_path": "src/ck_exporter/bootstrap.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.adapters.dspy_topic_labeler.DspyTopicLabeler",
      "src.ck_exporter.bootstrap.build_llm_client",
      "src.ck_exporter.adapters.openrouter_topic_labeler.OpenRouterTopicLabeler"
    ],
    "source_code": "def build_topic_labeler(\n    label_model: Optional[str] = None,\n    use_openrouter: bool = True,\n    shared_client: Optional[object] = None,\n) -> TopicLabeler:\n    \"\"\"\n    Build a topic labeler adapter based on environment configuration.\n\n    Reads `CKX_TOPIC_LABELER_IMPL` to choose between:\n    - `openrouter`: OpenRouter-based labeler (default)\n    - `dspy`: DSPy-based labeler\n\n    Args:\n        label_model: Model for labeling (defaults to env or \"z-ai/glm-4.7\")\n        use_openrouter: Whether to use OpenRouter (default True)\n        shared_client: Optional pre-configured OpenAI client (for efficiency)\n\n    Returns:\n        Configured TopicLabeler instance\n    \"\"\"\n    if label_model is None:\n        label_model = os.getenv(\"CKX_DSPY_LABEL_MODEL\", \"z-ai/glm-4.7\")\n\n    # Choose labeler implementation based on env flag\n    labeler_impl = os.getenv(\"CKX_TOPIC_LABELER_IMPL\", \"openrouter\").lower()\n\n    if labeler_impl == \"dspy\":\n        try:\n            return DspyTopicLabeler(use_openrouter=use_openrouter)\n        except ImportError as e:\n            logger.warning(\n                \"DSPy topic labeler requested but not available, falling back to OpenRouter\",\n                extra={\n                    \"event\": \"bootstrap.dspy_labeler.unavailable\",\n                    \"error\": str(e),\n                },\n            )\n            # Fall through to default OpenRouter implementation\n\n    # Default: OpenRouter\n    if shared_client is None:\n        shared_client = make_openrouter_client(use_openrouter)\n    llm = build_llm_client(use_openrouter=use_openrouter, client=shared_client)\n    return OpenRouterTopicLabeler(llm=llm, model=label_model)",
    "start_line": 138,
    "end_line": 181,
    "has_docstring": true,
    "docstring": "Build a topic labeler adapter based on environment configuration.\n\nReads `CKX_TOPIC_LABELER_IMPL` to choose between:\n- `openrouter`: OpenRouter-based labeler (default)\n- `dspy`: DSPy-based labeler\n\nArgs:\n    label_model: Model for labeling (defaults to env or \"z-ai/glm-4.7\")\n    use_openrouter: Whether to use OpenRouter (default True)\n    shared_client: Optional pre-configured OpenAI client (for efficiency)\n\nReturns:\n    Configured TopicLabeler instance",
    "parameters": [
      "label_model",
      "use_openrouter",
      "shared_client"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function build_topic_labeler",
    "component_id": "src.ck_exporter.bootstrap.build_topic_labeler"
  },
  "src.ck_exporter.cli.app.main": {
    "id": "src.ck_exporter.cli.app.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/app.py",
    "relative_path": "src/ck_exporter/cli/app.py",
    "depends_on": [
      "src.ck_exporter.logging.configure_logging"
    ],
    "source_code": "def main(\n    log_mode: str = typer.Option(\n        os.getenv(\"CKX_LOG_MODE\", \"auto\"),\n        \"--log-mode\",\n        help=\"Logging mode: human (Rich console), hybrid (Rich console + JSON file), machine (JSON file only), or auto (default: human if TTY, hybrid if log-file set)\",\n    ),\n    log_format: str = typer.Option(\n        os.getenv(\"CKX_LOG_FORMAT\", \"json\"),\n        \"--log-format\",\n        help=\"Log format: json, rich, or plain (used when mode is not specified, for backward compatibility)\",\n    ),\n    log_level: str = typer.Option(\n        os.getenv(\"CKX_LOG_LEVEL\", \"INFO\"),\n        \"--log-level\",\n        help=\"Log level: DEBUG, INFO, WARNING, ERROR\",\n    ),\n    log_file: Optional[Path] = typer.Option(\n        os.getenv(\"CKX_LOG_FILE\") if os.getenv(\"CKX_LOG_FILE\") else None,\n        \"--log-file\",\n        help=\"Optional file path to write logs to (JSON format)\",\n    ),\n    third_party_log_level: str = typer.Option(\n        os.getenv(\"CKX_THIRD_PARTY_LOG_LEVEL\", \"WARNING\"),\n        \"--third-party-log-level\",\n        help=\"Log level for third-party libraries: WARNING, ERROR, INFO\",\n    ),\n    hybrid_ui: bool = typer.Option(\n        os.getenv(\"CKX_HYBRID_UI\", \"true\").lower() == \"true\",\n        \"--hybrid-ui/--no-hybrid-ui\",\n        help=\"Enable hybrid UI mode (Rich banners + structured logs)\",\n    ),\n) -> None:\n    \"\"\"Configure logging before command execution.\"\"\"\n    configure_logging(\n        level=log_level,\n        format=log_format,\n        log_file=log_file,\n        third_party_level=third_party_log_level,\n        hybrid_ui=hybrid_ui,\n        mode=log_mode,  # type: ignore\n    )",
    "start_line": 24,
    "end_line": 64,
    "has_docstring": true,
    "docstring": "Configure logging before command execution.",
    "parameters": [
      "log_mode",
      "log_format",
      "log_level",
      "log_file",
      "third_party_log_level",
      "hybrid_ui"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "src.ck_exporter.cli.app.main"
  },
  "src.ck_exporter.cli.commands.compile.compile_command": {
    "id": "src.ck_exporter.cli.commands.compile.compile_command",
    "name": "compile_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/compile.py",
    "relative_path": "src/ck_exporter/cli/commands/compile.py",
    "depends_on": [
      "src.ck_exporter.pipeline.compile.compile_docs"
    ],
    "source_code": "def compile_command(\n    atoms: Path = typer.Option(Path(\"_atoms\"), \"--atoms\", \"-a\", help=\"Atoms directory\"),\n    out: Path = typer.Option(Path(\"docs\"), \"--out\", \"-o\", help=\"Output directory for docs\"),\n) -> None:\n    \"\"\"Compile documentation from knowledge atoms.\"\"\"\n    compile_docs(atoms, out)",
    "start_line": 10,
    "end_line": 15,
    "has_docstring": true,
    "docstring": "Compile documentation from knowledge atoms.",
    "parameters": [
      "atoms",
      "out"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function compile_command",
    "component_id": "src.ck_exporter.cli.commands.compile.compile_command"
  },
  "src.ck_exporter.cli.commands.consolidate.consolidate_command": {
    "id": "src.ck_exporter.cli.commands.consolidate.consolidate_command",
    "name": "consolidate_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/consolidate.py",
    "relative_path": "src/ck_exporter/cli/commands/consolidate.py",
    "depends_on": [
      "src.ck_exporter.pipeline.consolidate.consolidate_project"
    ],
    "source_code": "def consolidate_command(\n    atoms: Path = typer.Option(Path(\"_atoms\"), \"--atoms\", \"-a\", help=\"Atoms directory (per-conversation)\"),\n    docs: Path = typer.Option(Path(\"docs\"), \"--docs\", \"-d\", help=\"Docs directory (per-conversation)\"),\n    out: Path = typer.Option(Path(\"output\"), \"--out\", \"-o\", help=\"Output directory\"),\n    include_docs: bool = typer.Option(True, \"--include-docs/--no-include-docs\", help=\"Concatenate docs into bundles\"),\n) -> None:\n    \"\"\"Aggregate per-conversation outputs into a single project-wide knowledge packet (exact dedupe only).\"\"\"\n    stats = consolidate_project(atoms_dir=atoms, docs_dir=docs, out_dir=out, include_docs=include_docs)\n    console.print(\n        f\"[bold green] Consolidated[/bold green] \"\n        f\"atoms {stats.atoms_in}{stats.atoms_out}, \"\n        f\"decisions {stats.decisions_in}{stats.decisions_out}, \"\n        f\"questions {stats.questions_in}{stats.questions_out} \"\n        f\"into {out / 'project'}\"\n    )",
    "start_line": 13,
    "end_line": 27,
    "has_docstring": true,
    "docstring": "Aggregate per-conversation outputs into a single project-wide knowledge packet (exact dedupe only).",
    "parameters": [
      "atoms",
      "docs",
      "out",
      "include_docs"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function consolidate_command",
    "component_id": "src.ck_exporter.cli.commands.consolidate.consolidate_command"
  },
  "src.ck_exporter.cli.commands.extract.extract_command": {
    "id": "src.ck_exporter.cli.commands.extract.extract_command",
    "name": "extract_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/extract.py",
    "relative_path": "src/ck_exporter/cli/commands/extract.py",
    "depends_on": [
      "src.ck_exporter.pipeline.extract.extract_export"
    ],
    "source_code": "def extract_command(\n    input: Path = typer.Option(\n        ...,\n        \"--input\",\n        \"-i\",\n        help=\"Path to ChatGPT export JSON (list), single-conversation JSON (mapping/current_node), Claude export JSON, or a directory of per-conversation .json files\",\n    ),\n    evidence: Path = typer.Option(Path(\"_evidence\"), \"--evidence\", \"-e\", help=\"Evidence directory\"),\n    out: Path = typer.Option(Path(\"_atoms\"), \"--out\", \"-o\", help=\"Output directory for atoms\"),\n    fast_model: str = typer.Option(None, \"--fast-model\", help=\"Fast model for Pass 1 (default: z-ai/glm-4.7)\"),\n    big_model: str = typer.Option(None, \"--big-model\", help=\"Big model for Pass 2 (default: z-ai/glm-4.7)\"),\n    max_concurrency: int = typer.Option(None, \"--max-concurrency\", help=\"Max concurrent conversations (default: 8)\"),\n    skip_existing: bool = typer.Option(True, \"--skip-existing/--no-skip-existing\", help=\"Skip conversations with existing outputs\"),\n    use_openrouter: bool = typer.Option(True, \"--openrouter/--no-openrouter\", help=\"Use OpenRouter API (default: True)\"),\n    conversation_id: str = typer.Option(None, \"--conversation-id\", \"-c\", help=\"Process only this conversation ID (for testing)\"),\n    limit: Optional[int] = typer.Option(None, \"--limit\", \"-n\", help=\"Limit number of conversations to process (deterministic: first N by sorted filename)\"),\n) -> None:\n    \"\"\"Extract knowledge atoms from conversations using two-pass OpenRouter pipeline.\"\"\"\n    if not input.exists():\n        console.print(f\"[red]Input path not found: {input}[/red]\")\n        raise typer.Exit(1)\n\n    extract_export(\n        input,\n        evidence,\n        out,\n        fast_model=fast_model,\n        big_model=big_model,\n        max_concurrency=max_concurrency,\n        skip_existing=skip_existing,\n        use_openrouter=use_openrouter,\n        conversation_id=conversation_id,\n        limit=limit,\n    )",
    "start_line": 14,
    "end_line": 47,
    "has_docstring": true,
    "docstring": "Extract knowledge atoms from conversations using two-pass OpenRouter pipeline.",
    "parameters": [
      "input",
      "evidence",
      "out",
      "fast_model",
      "big_model",
      "max_concurrency",
      "skip_existing",
      "use_openrouter",
      "conversation_id",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_command",
    "component_id": "src.ck_exporter.cli.commands.extract.extract_command"
  },
  "src.ck_exporter.cli.commands.linearize.linearize_command": {
    "id": "src.ck_exporter.cli.commands.linearize.linearize_command",
    "name": "linearize_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/linearize.py",
    "relative_path": "src/ck_exporter/cli/commands/linearize.py",
    "depends_on": [
      "src.ck_exporter.pipeline.linearize.linearize_export"
    ],
    "source_code": "def linearize_command(\n    input: Path = typer.Option(\n        ...,\n        \"--input\",\n        \"-i\",\n        help=\"Path to ChatGPT export JSON (list), single-conversation JSON (mapping/current_node), Claude export JSON, or a directory of per-conversation .json files\",\n    ),\n    out: Path = typer.Option(Path(\"_evidence\"), \"--out\", \"-o\", help=\"Output directory for evidence\"),\n    limit: Optional[int] = typer.Option(None, \"--limit\", \"-n\", help=\"Limit number of conversations to process (deterministic: first N by sorted filename)\"),\n) -> None:\n    \"\"\"Linearize conversations from export JSON into markdown.\"\"\"\n    if not input.exists():\n        console.print(f\"[red]Input path not found: {input}[/red]\")\n        raise typer.Exit(1)\n\n    linearize_export(input, out, limit=limit)",
    "start_line": 14,
    "end_line": 29,
    "has_docstring": true,
    "docstring": "Linearize conversations from export JSON into markdown.",
    "parameters": [
      "input",
      "out",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function linearize_command",
    "component_id": "src.ck_exporter.cli.commands.linearize.linearize_command"
  },
  "src.ck_exporter.cli.commands.run_all.run_all_command": {
    "id": "src.ck_exporter.cli.commands.run_all.run_all_command",
    "name": "run_all_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/run_all.py",
    "relative_path": "src/ck_exporter/cli/commands/run_all.py",
    "depends_on": [
      "src.ck_exporter.pipeline.extract.extract_export",
      "src.ck_exporter.ui.dashboard.PipelineDashboard",
      "src.ck_exporter.logging.get_current_log_mode",
      "src.ck_exporter.pipeline.compile.compile_docs",
      "src.ck_exporter.pipeline.linearize.linearize_export"
    ],
    "source_code": "def run_all_command(\n    input: Path = typer.Option(\n        ...,\n        \"--input\",\n        \"-i\",\n        help=\"Path to ChatGPT export JSON (list), single-conversation JSON (mapping/current_node), Claude export JSON, or a directory of per-conversation .json files\",\n    ),\n    evidence_dir: Path = typer.Option(Path(\"_evidence\"), \"--evidence\", \"-e\", help=\"Evidence directory\"),\n    atoms_dir: Path = typer.Option(Path(\"_atoms\"), \"--atoms\", \"-a\", help=\"Atoms directory\"),\n    docs_dir: Path = typer.Option(Path(\"docs\"), \"--docs\", \"-d\", help=\"Docs directory\"),\n    fast_model: str = typer.Option(None, \"--fast-model\", help=\"Fast model for Pass 1 (default: z-ai/glm-4.7)\"),\n    big_model: str = typer.Option(None, \"--big-model\", help=\"Big model for Pass 2 (default: z-ai/glm-4.7)\"),\n    max_concurrency: int = typer.Option(None, \"--max-concurrency\", help=\"Max concurrent conversations (default: 8)\"),\n    skip_existing: bool = typer.Option(True, \"--skip-existing/--no-skip-existing\", help=\"Skip conversations with existing outputs\"),\n    use_openrouter: bool = typer.Option(True, \"--openrouter/--no-openrouter\", help=\"Use OpenRouter API (default: True)\"),\n    conversation_id: str = typer.Option(None, \"--conversation-id\", \"-c\", help=\"Process only this conversation ID (for testing)\"),\n    limit: Optional[int] = typer.Option(None, \"--limit\", \"-n\", help=\"Limit number of conversations to process (deterministic: first N by sorted filename)\"),\n    dashboard: Optional[bool] = typer.Option(\n        None,\n        \"--dashboard/--no-dashboard\",\n        help=\"Enable/disable persistent dashboard (default: auto-detect based on TTY and log mode)\",\n    ),\n    dashboard_log_lines: int = typer.Option(50, \"--dashboard-log-lines\", help=\"Number of log lines to show in dashboard tail (default: 50)\"),\n) -> None:\n    \"\"\"Run the full pipeline: linearize  extract  compile.\"\"\"\n    if not input.exists():\n        console.print(f\"[red]Input path not found: {input}[/red]\")\n        raise typer.Exit(1)\n\n    # Determine if dashboard should be enabled\n    current_log_mode = get_current_log_mode()\n\n    use_dashboard = False\n    if dashboard is not None:\n        use_dashboard = dashboard\n    else:\n        # Auto-detect: enable if TTY and not machine mode\n        use_dashboard = sys.stderr.isatty() and (current_log_mode != \"machine\")\n\n    # Create dashboard if enabled\n    dashboard_obj: Optional[PipelineDashboard] = None\n    if use_dashboard:\n        dashboard_obj = PipelineDashboard(\n            steps=[\"Linearize\", \"Extract\", \"Compile\"],\n            log_lines=dashboard_log_lines,\n        )\n        dashboard_obj.install_log_handler()\n\n        # Suppress normal console handler to prevent log spam\n        # (dashboard will show logs in its panel, file handlers still work)\n        import logging\n\n        root_logger = logging.getLogger()\n        # Remove stderr StreamHandlers (but keep file handlers and dashboard handler)\n        handlers_to_remove = [\n            h\n            for h in root_logger.handlers\n            if isinstance(h, logging.StreamHandler)\n            and hasattr(h, \"stream\")\n            and h.stream == sys.stderr\n            and h != dashboard_obj.log_handler\n        ]\n        for handler in handlers_to_remove:\n            root_logger.removeHandler(handler)\n\n    try:\n        if dashboard_obj:\n            # Run with dashboard\n            with dashboard_obj.run_live():\n                # Step 1: Linearize\n                dashboard_obj.set_step_status(\"Linearize\", \"running\")\n                linearize_cb = dashboard_obj.get_progress_callback(\"Linearize\")\n                linearize_export(input, evidence_dir, limit=limit, progress_cb=linearize_cb)\n                dashboard_obj.set_step_status(\"Linearize\", \"complete\")\n\n                # Step 2: Extract\n                dashboard_obj.set_step_status(\"Extract\", \"running\")\n                extract_cb = dashboard_obj.get_progress_callback(\"Extract\")\n                extract_export(\n                    input,\n                    evidence_dir,\n                    atoms_dir,\n                    fast_model=fast_model,\n                    big_model=big_model,\n                    max_concurrency=max_concurrency,\n                    skip_existing=skip_existing,\n                    use_openrouter=use_openrouter,\n                    conversation_id=conversation_id,\n                    limit=limit,\n                    progress_cb=extract_cb,\n                )\n                dashboard_obj.set_step_status(\"Extract\", \"complete\")\n\n                # Step 3: Compile\n                dashboard_obj.set_step_status(\"Compile\", \"running\")\n                compile_cb = dashboard_obj.get_progress_callback(\"Compile\")\n                compile_docs(atoms_dir, docs_dir, progress_cb=compile_cb)\n                dashboard_obj.set_step_status(\"Compile\", \"complete\")\n        else:\n            # Run without dashboard (original behavior)\n            console.print(\"[bold]Running full pipeline[/bold]\")\n\n            # Step 1: Linearize\n            console.print(\"\\n[bold cyan]Step 1: Linearizing conversations[/bold cyan]\")\n            linearize_export(input, evidence_dir, limit=limit)\n\n            # Step 2: Extract\n            console.print(\"\\n[bold cyan]Step 2: Extracting knowledge atoms[/bold cyan]\")\n            extract_export(\n                input,\n                evidence_dir,\n                atoms_dir,\n                fast_model=fast_model,\n                big_model=big_model,\n                max_concurrency=max_concurrency,\n                skip_existing=skip_existing,\n                use_openrouter=use_openrouter,\n                conversation_id=conversation_id,\n                limit=limit,\n                progress_cb=None,\n            )\n\n            # Step 3: Compile\n            console.print(\"\\n[bold cyan]Step 3: Compiling documentation[/bold cyan]\")\n            compile_docs(atoms_dir, docs_dir, progress_cb=None)\n\n            console.print(\"\\n[bold green] Pipeline complete![/bold green]\")\n    finally:\n        if dashboard_obj:\n            dashboard_obj.remove_log_handler()",
    "start_line": 19,
    "end_line": 148,
    "has_docstring": true,
    "docstring": "Run the full pipeline: linearize  extract  compile.",
    "parameters": [
      "input",
      "evidence_dir",
      "atoms_dir",
      "docs_dir",
      "fast_model",
      "big_model",
      "max_concurrency",
      "skip_existing",
      "use_openrouter",
      "conversation_id",
      "limit",
      "dashboard",
      "dashboard_log_lines"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function run_all_command",
    "component_id": "src.ck_exporter.cli.commands.run_all.run_all_command"
  },
  "src.ck_exporter.cli.commands.topics.discover_topics_command": {
    "id": "src.ck_exporter.cli.commands.topics.discover_topics_command",
    "name": "discover_topics_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/topics.py",
    "relative_path": "src/ck_exporter/cli/commands/topics.py",
    "depends_on": [
      "apps.review-api.src.review_api.main.get_topic",
      "src.ck_exporter.pipeline.topics.build_conversation_documents",
      "src.ck_exporter.core.models.topics.Topic",
      "src.ck_exporter.topic_discovery.label_topics_with_llm",
      "src.ck_exporter.pipeline.topics.save_topic_registry"
    ],
    "source_code": "def discover_topics_command(\n    input: Path = typer.Option(..., \"--input\", \"-i\", help=\"Path to conversation JSON file(s)\"),\n    atoms: Path = typer.Option(Path(\"output/project\"), \"--atoms\", \"-a\", help=\"Path to consolidated atoms.jsonl directory\"),\n    out: Path = typer.Option(Path(\"output\"), \"--out\", \"-o\", help=\"Output directory\"),\n    target_topics: int = typer.Option(50, \"--target-topics\", help=\"Target number of topics\"),\n    embedding_model: str = typer.Option(\"openai/text-embedding-3-small\", \"--embedding-model\", help=\"OpenRouter embedding model\"),\n    label_model: str = typer.Option(None, \"--label-model\", help=\"LLM model for topic labeling (default: z-ai/glm-4.7)\"),\n    skip_labeling: bool = typer.Option(False, \"--skip-labeling/--no-skip-labeling\", help=\"Skip LLM labeling of topics\"),\n    use_openrouter: bool = typer.Option(True, \"--openrouter/--no-openrouter\", help=\"Use OpenRouter API (default: True)\"),\n    pooling: bool = typer.Option(True, \"--pooling/--no-pooling\", help=\"Use chunked pooling for embeddings (default: True)\"),\n    chunk_tokens: int = typer.Option(600, \"--chunk-tokens\", help=\"Maximum tokens per chunk when pooling (default: 600)\"),\n    chunk_overlap: int = typer.Option(80, \"--chunk-overlap\", help=\"Token overlap between chunks when pooling (default: 80)\"),\n    embedding_cache_dir: Path = typer.Option(None, \"--embedding-cache-dir\", help=\"Directory for caching embeddings (default: .cache/embeddings)\"),\n    limit: Optional[int] = typer.Option(None, \"--limit\", \"-n\", help=\"Limit number of conversations to process (deterministic: first N by sorted filename)\"),\n) -> None:\n    \"\"\"Discover topics from conversation artifacts using BERTopic.\"\"\"\n    if not input.exists():\n        console.print(f\"[red]Input file not found: {input}[/red]\")\n        raise typer.Exit(1)\n\n    atoms_path = atoms / \"atoms.jsonl\" if atoms.is_dir() else atoms\n    decisions_path = atoms / \"decisions.jsonl\" if atoms.is_dir() else atoms.parent / \"decisions.jsonl\"\n    questions_path = atoms / \"open_questions.jsonl\" if atoms.is_dir() else atoms.parent / \"open_questions.jsonl\"\n\n    if not atoms_path.exists():\n        console.print(f\"[red]Atoms file not found: {atoms_path}[/red]\")\n        raise typer.Exit(1)\n\n    console.print(\"[bold]Building conversation documents...[/bold]\")\n    documents, titles = build_conversation_documents(input, atoms_path, decisions_path, questions_path, limit=limit)\n\n    if not documents:\n        console.print(\"[red]No conversations found[/red]\")\n        raise typer.Exit(1)\n\n    console.print(f\"[green] Built documents for {len(documents)} conversations[/green]\")\n\n    console.print(\"[bold]Discovering topics...[/bold]\")\n    cache_dir = embedding_cache_dir if embedding_cache_dir else Path(\".cache/embeddings\")\n    topic_model, embeddings, doc_ids = discover_topics_func(\n        documents,\n        embedding_model=embedding_model,\n        target_topics=target_topics,\n        use_openrouter=use_openrouter,\n        use_pooling=pooling,\n        chunk_tokens=chunk_tokens,\n        overlap_tokens=chunk_overlap,\n        cache_dir=cache_dir,\n    )\n\n    doc_texts = [documents[conv_id] for conv_id in doc_ids]\n    topics_out = topic_model.topics_\n\n    if skip_labeling:\n        console.print(\"[yellow]Skipping LLM labeling[/yellow]\")\n        # Create basic topics without LLM labels\n        topic_info = topic_model.get_topic_info()\n        topics = []\n        for idx, row in topic_info.iterrows():\n            topic_id = int(row[\"Topic\"])\n            if topic_id == -1:\n                continue\n            topic_words = topic_model.get_topic(topic_id)\n            keywords = [word for word, _ in topic_words[:10]] if topic_words else []\n            topics.append(\n                {\n                    \"topic_id\": topic_id,\n                    \"name\": f\"Topic {topic_id}\",\n                    \"description\": f\"Topic {topic_id} with keywords: {', '.join(keywords[:5])}\",\n                    \"keywords\": keywords,\n                    \"representative_conversations\": [],\n                }\n            )\n        from ck_exporter.core.models import Topic\n        topics = [Topic(**t) for t in topics]\n    else:\n        console.print(\"[bold]Labeling topics with LLM...[/bold]\")\n        topics = label_topics_with_llm(\n            topic_model,\n            documents,\n            doc_ids,\n            doc_texts,\n            embedding_model=embedding_model,\n            use_openrouter=use_openrouter,\n            label_model=label_model,\n        )\n\n    registry_path = out / \"topic_registry.json\"\n    save_topic_registry(\n        topics,\n        topic_model,\n        embeddings,\n        topics_out,\n        doc_ids,\n        embedding_model,\n        registry_path,\n    )\n\n    console.print(f\"[bold green] Topic discovery complete![/bold green]\")\n    console.print(f\"   Registry saved to: {registry_path}\")",
    "start_line": 20,
    "end_line": 119,
    "has_docstring": true,
    "docstring": "Discover topics from conversation artifacts using BERTopic.",
    "parameters": [
      "input",
      "atoms",
      "out",
      "target_topics",
      "embedding_model",
      "label_model",
      "skip_labeling",
      "use_openrouter",
      "pooling",
      "chunk_tokens",
      "chunk_overlap",
      "embedding_cache_dir",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function discover_topics_command",
    "component_id": "src.ck_exporter.cli.commands.topics.discover_topics_command"
  },
  "src.ck_exporter.cli.commands.topics.assign_topics_command": {
    "id": "src.ck_exporter.cli.commands.topics.assign_topics_command",
    "name": "assign_topics_command",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/cli/commands/topics.py",
    "relative_path": "src/ck_exporter/cli/commands/topics.py",
    "depends_on": [
      "src.ck_exporter.pipeline.assignment.load_topic_registry",
      "src.ck_exporter.pipeline.assignment.save_assignments",
      "src.ck_exporter.pipeline.assignment.assign_topics"
    ],
    "source_code": "def assign_topics_command(\n    input: Path = typer.Option(..., \"--input\", \"-i\", help=\"Path to conversation JSON file(s)\"),\n    atoms: Path = typer.Option(Path(\"output/project\"), \"--atoms\", \"-a\", help=\"Path to consolidated atoms.jsonl directory\"),\n    registry: Path = typer.Option(..., \"--registry\", \"-r\", help=\"Path to topic_registry.json\"),\n    out: Path = typer.Option(Path(\"output\"), \"--out\", \"-o\", help=\"Output directory\"),\n    embedding_model: str = typer.Option(\"openai/text-embedding-3-small\", \"--embedding-model\", help=\"OpenRouter embedding model\"),\n    primary_threshold: float = typer.Option(0.60, \"--primary-threshold\", help=\"Minimum score for primary topic\"),\n    secondary_threshold: float = typer.Option(0.55, \"--secondary-threshold\", help=\"Minimum score for secondary topics\"),\n    use_openrouter: bool = typer.Option(True, \"--openrouter/--no-openrouter\", help=\"Use OpenRouter API (default: True)\"),\n    pooling: bool = typer.Option(True, \"--pooling/--no-pooling\", help=\"Use chunked pooling for embeddings (default: True)\"),\n    chunk_tokens: int = typer.Option(600, \"--chunk-tokens\", help=\"Maximum tokens per chunk when pooling (default: 600)\"),\n    chunk_overlap: int = typer.Option(80, \"--chunk-overlap\", help=\"Token overlap between chunks when pooling (default: 80)\"),\n    embedding_cache_dir: Path = typer.Option(None, \"--embedding-cache-dir\", help=\"Directory for caching embeddings (default: .cache/embeddings)\"),\n    limit: Optional[int] = typer.Option(None, \"--limit\", \"-n\", help=\"Limit number of conversations to process (deterministic: first N by sorted filename)\"),\n) -> None:\n    \"\"\"Assign conversations to discovered topics (multi-label).\"\"\"\n    if not input.exists():\n        console.print(f\"[red]Input file not found: {input}[/red]\")\n        raise typer.Exit(1)\n\n    if not registry.exists():\n        console.print(f\"[red]Topic registry not found: {registry}[/red]\")\n        raise typer.Exit(1)\n\n    atoms_path = atoms / \"atoms.jsonl\" if atoms.is_dir() else atoms\n    decisions_path = atoms / \"decisions.jsonl\" if atoms.is_dir() else atoms.parent / \"decisions.jsonl\"\n    questions_path = atoms / \"open_questions.jsonl\" if atoms.is_dir() else atoms.parent / \"open_questions.jsonl\"\n\n    if not atoms_path.exists():\n        console.print(f\"[red]Atoms file not found: {atoms_path}[/red]\")\n        raise typer.Exit(1)\n\n    console.print(\"[bold]Loading topic registry...[/bold]\")\n    topic_registry = load_topic_registry(registry)\n\n    console.print(f\"[green] Loaded {topic_registry.num_topics} topics[/green]\")\n\n    console.print(\"[bold]Assigning topics to conversations...[/bold]\")\n    cache_dir = embedding_cache_dir if embedding_cache_dir else Path(\".cache/embeddings\")\n    assignments = assign_topics(\n        input,\n        atoms_path,\n        decisions_path,\n        questions_path,\n        topic_registry,\n        embedding_model=embedding_model,\n        primary_threshold=primary_threshold,\n        secondary_threshold=secondary_threshold,\n        use_openrouter=use_openrouter,\n        use_pooling=pooling,\n        chunk_tokens=chunk_tokens,\n        overlap_tokens=chunk_overlap,\n        cache_dir=cache_dir,\n        limit=limit,\n    )\n\n    assignments_path = out / \"assignments.jsonl\"\n    save_assignments(assignments, assignments_path)\n\n    console.print(f\"[bold green] Topic assignment complete![/bold green]\")\n    console.print(f\"   Assignments saved to: {assignments_path}\")",
    "start_line": 122,
    "end_line": 182,
    "has_docstring": true,
    "docstring": "Assign conversations to discovered topics (multi-label).",
    "parameters": [
      "input",
      "atoms",
      "registry",
      "out",
      "embedding_model",
      "primary_threshold",
      "secondary_threshold",
      "use_openrouter",
      "pooling",
      "chunk_tokens",
      "chunk_overlap",
      "embedding_cache_dir",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function assign_topics_command",
    "component_id": "src.ck_exporter.cli.commands.topics.assign_topics_command"
  },
  "src.ck_exporter.core.models.atoms.Evidence": {
    "id": "src.ck_exporter.core.models.atoms.Evidence",
    "name": "Evidence",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/atoms.py",
    "relative_path": "src/ck_exporter/core/models/atoms.py",
    "depends_on": [],
    "source_code": "class Evidence(BaseModel):\n    \"\"\"Evidence pointer linking an atom back to source messages.\"\"\"\n\n    message_id: Optional[str] = None\n    time_iso: Optional[str] = None\n    text_snippet: Optional[str] = Field(None, description=\"Relevant snippet from the message\")",
    "start_line": 9,
    "end_line": 14,
    "has_docstring": true,
    "docstring": "Evidence pointer linking an atom back to source messages.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class Evidence",
    "component_id": "src.ck_exporter.core.models.atoms.Evidence"
  },
  "src.ck_exporter.core.models.atoms.Atom": {
    "id": "src.ck_exporter.core.models.atoms.Atom",
    "name": "Atom",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/atoms.py",
    "relative_path": "src/ck_exporter/core/models/atoms.py",
    "depends_on": [],
    "source_code": "class Atom(BaseModel):\n    \"\"\"A knowledge atom extracted from conversation.\"\"\"\n\n    type: Literal[\n        \"decision\",\n        \"requirement\",\n        \"definition\",\n        \"metric\",\n        \"risk\",\n        \"assumption\",\n        \"constraint\",\n        \"idea\",\n        \"fact\",\n    ]\n    topic: str = Field(..., description=\"Topic category (e.g., 'pricing', 'architecture', 'ICP')\")\n    statement: str = Field(..., description=\"The actual knowledge statement\")\n    status: Literal[\"active\", \"deprecated\", \"uncertain\"] = \"active\"\n    evidence: List[Evidence] = Field(default_factory=list)\n    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat())",
    "start_line": 17,
    "end_line": 35,
    "has_docstring": true,
    "docstring": "A knowledge atom extracted from conversation.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class Atom",
    "component_id": "src.ck_exporter.core.models.atoms.Atom"
  },
  "src.ck_exporter.core.models.atoms.DecisionAtom": {
    "id": "src.ck_exporter.core.models.atoms.DecisionAtom",
    "name": "DecisionAtom",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/atoms.py",
    "relative_path": "src/ck_exporter/core/models/atoms.py",
    "depends_on": [
      "src.ck_exporter.core.models.atoms.Atom"
    ],
    "source_code": "class DecisionAtom(Atom):\n    \"\"\"A decision atom with additional context.\"\"\"\n\n    type: Literal[\"decision\"] = \"decision\"\n    alternatives: List[str] = Field(default_factory=list, description=\"Alternatives considered\")\n    rationale: Optional[str] = Field(None, description=\"Why this decision was made\")\n    consequences: Optional[str] = Field(None, description=\"Expected consequences\")",
    "start_line": 38,
    "end_line": 44,
    "has_docstring": true,
    "docstring": "A decision atom with additional context.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Atom"
    ],
    "class_name": null,
    "display_name": "class DecisionAtom",
    "component_id": "src.ck_exporter.core.models.atoms.DecisionAtom"
  },
  "src.ck_exporter.core.models.atoms.OpenQuestion": {
    "id": "src.ck_exporter.core.models.atoms.OpenQuestion",
    "name": "OpenQuestion",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/atoms.py",
    "relative_path": "src/ck_exporter/core/models/atoms.py",
    "depends_on": [],
    "source_code": "class OpenQuestion(BaseModel):\n    \"\"\"An open question or uncertainty.\"\"\"\n\n    question: str\n    topic: str\n    context: Optional[str] = None\n    evidence: List[Evidence] = Field(default_factory=list)\n    extracted_at: str = Field(default_factory=lambda: datetime.now().isoformat())",
    "start_line": 47,
    "end_line": 54,
    "has_docstring": true,
    "docstring": "An open question or uncertainty.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class OpenQuestion",
    "component_id": "src.ck_exporter.core.models.atoms.OpenQuestion"
  },
  "src.ck_exporter.core.models.topics.Topic": {
    "id": "src.ck_exporter.core.models.topics.Topic",
    "name": "Topic",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/topics.py",
    "relative_path": "src/ck_exporter/core/models/topics.py",
    "depends_on": [],
    "source_code": "class Topic(BaseModel):\n    \"\"\"A discovered topic with metadata.\"\"\"\n\n    topic_id: int\n    name: str = Field(..., description=\"Short topic name (3-5 words)\")\n    description: str = Field(..., description=\"1-2 sentence description\")\n    keywords: List[str] = Field(default_factory=list, description=\"Top representative terms\")\n    representative_conversations: List[str] = Field(\n        default_factory=list, description=\"conversation_ids that represent this topic\"\n    )\n    centroid_embedding: Optional[List[float]] = Field(\n        None, description=\"Embedding vector for topic centroid\"\n    )",
    "start_line": 9,
    "end_line": 21,
    "has_docstring": true,
    "docstring": "A discovered topic with metadata.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class Topic",
    "component_id": "src.ck_exporter.core.models.topics.Topic"
  },
  "src.ck_exporter.core.models.topics.TopicRegistry": {
    "id": "src.ck_exporter.core.models.topics.TopicRegistry",
    "name": "TopicRegistry",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/topics.py",
    "relative_path": "src/ck_exporter/core/models/topics.py",
    "depends_on": [],
    "source_code": "class TopicRegistry(BaseModel):\n    \"\"\"Registry of discovered topics.\"\"\"\n\n    generated_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n    embedding_model: str\n    num_topics: int\n    topics: List[Topic]",
    "start_line": 24,
    "end_line": 30,
    "has_docstring": true,
    "docstring": "Registry of discovered topics.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class TopicRegistry",
    "component_id": "src.ck_exporter.core.models.topics.TopicRegistry"
  },
  "src.ck_exporter.core.models.topics.TopicAssignment": {
    "id": "src.ck_exporter.core.models.topics.TopicAssignment",
    "name": "TopicAssignment",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/topics.py",
    "relative_path": "src/ck_exporter/core/models/topics.py",
    "depends_on": [],
    "source_code": "class TopicAssignment(BaseModel):\n    \"\"\"A topic assignment for a conversation.\"\"\"\n\n    topic_id: int\n    name: str\n    score: float = Field(..., ge=0.0, le=1.0, description=\"Cosine similarity score 0-1\")\n    rank: Literal[\"primary\", \"secondary\"]",
    "start_line": 33,
    "end_line": 39,
    "has_docstring": true,
    "docstring": "A topic assignment for a conversation.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class TopicAssignment",
    "component_id": "src.ck_exporter.core.models.topics.TopicAssignment"
  },
  "src.ck_exporter.core.models.topics.ConversationTopics": {
    "id": "src.ck_exporter.core.models.topics.ConversationTopics",
    "name": "ConversationTopics",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/models/topics.py",
    "relative_path": "src/ck_exporter/core/models/topics.py",
    "depends_on": [],
    "source_code": "class ConversationTopics(BaseModel):\n    \"\"\"Topic assignments for a conversation.\"\"\"\n\n    conversation_id: str\n    title: str\n    project_id: Optional[str] = None\n    project_name: Optional[str] = None\n    topics: List[TopicAssignment]\n    atom_count: int\n    review_flag: bool = Field(\n        False, description=\"True if assignment needs human review\"\n    )",
    "start_line": 42,
    "end_line": 53,
    "has_docstring": true,
    "docstring": "Topic assignments for a conversation.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class ConversationTopics",
    "component_id": "src.ck_exporter.core.models.topics.ConversationTopics"
  },
  "src.ck_exporter.core.ports.atom_extractor.AtomExtractor": {
    "id": "src.ck_exporter.core.ports.atom_extractor.AtomExtractor",
    "name": "AtomExtractor",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/ports/atom_extractor.py",
    "relative_path": "src/ck_exporter/core/ports/atom_extractor.py",
    "depends_on": [],
    "source_code": "class AtomExtractor(Protocol):\n    \"\"\"Interface for extracting knowledge atoms from conversation chunks.\"\"\"\n\n    def extract_from_chunk(self, chunk_text: str) -> dict[str, Any]:\n        \"\"\"\n        Extract candidate atoms from a conversation chunk (Pass 1).\n\n        Args:\n            chunk_text: Formatted conversation chunk text\n\n        Returns:\n            Dict with keys: \"facts\", \"decisions\", \"open_questions\"\n            Each value is a list of atom dicts\n        \"\"\"\n        ...\n\n    def refine_atoms(\n        self,\n        all_candidates: dict[str, list[dict[str, Any]]],\n        conversation_id: str,\n        conversation_title: str | None,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Refine and consolidate candidate atoms (Pass 2).\n\n        Args:\n            all_candidates: Dict with \"facts\", \"decisions\", \"open_questions\" lists\n            conversation_id: Conversation identifier\n            conversation_title: Optional conversation title\n\n        Returns:\n            Dict with keys: \"facts\", \"decisions\", \"open_questions\"\n            Each value is a refined/deduplicated list of atom dicts\n        \"\"\"\n        ...",
    "start_line": 6,
    "end_line": 40,
    "has_docstring": true,
    "docstring": "Interface for extracting knowledge atoms from conversation chunks.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Protocol"
    ],
    "class_name": null,
    "display_name": "class AtomExtractor",
    "component_id": "src.ck_exporter.core.ports.atom_extractor.AtomExtractor"
  },
  "src.ck_exporter.core.ports.embedder.Embedder": {
    "id": "src.ck_exporter.core.ports.embedder.Embedder",
    "name": "Embedder",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/ports/embedder.py",
    "relative_path": "src/ck_exporter/core/ports/embedder.py",
    "depends_on": [],
    "source_code": "class Embedder(Protocol):\n    \"\"\"Interface for text embedding services.\"\"\"\n\n    def embed(self, texts: list[str], batch_size: int = 100) -> np.ndarray:\n        \"\"\"\n        Generate embeddings for multiple texts.\n\n        Args:\n            texts: List of text strings to embed\n            batch_size: Maximum texts per API call (default 100)\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        ...\n\n    def embed_pooled(\n        self,\n        texts: list[str],\n        chunk_tokens: int = 600,\n        overlap_tokens: int = 80,\n        pooling: str = \"mean\",\n        cache_dir: Path | None = None,\n        batch_size: int = 100,\n    ) -> np.ndarray:\n        \"\"\"\n        Generate pooled embeddings for texts by chunking, embedding chunks, and pooling.\n\n        Args:\n            texts: List of text strings to embed\n            chunk_tokens: Maximum tokens per chunk (default 600)\n            overlap_tokens: Token overlap between chunks (default 80)\n            pooling: Pooling method (\"mean\" for normalized mean pooling)\n            cache_dir: Optional directory for caching embeddings\n            batch_size: Maximum chunks per API call (default 100)\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        ...",
    "start_line": 9,
    "end_line": 48,
    "has_docstring": true,
    "docstring": "Interface for text embedding services.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Protocol"
    ],
    "class_name": null,
    "display_name": "class Embedder",
    "component_id": "src.ck_exporter.core.ports.embedder.Embedder"
  },
  "src.ck_exporter.core.ports.llm.LLMClient": {
    "id": "src.ck_exporter.core.ports.llm.LLMClient",
    "name": "LLMClient",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/ports/llm.py",
    "relative_path": "src/ck_exporter/core/ports/llm.py",
    "depends_on": [],
    "source_code": "class LLMClient(Protocol):\n    \"\"\"Interface for LLM chat/completion services.\"\"\"\n\n    def chat(\n        self,\n        model: str,\n        system: str,\n        user: str,\n        *,\n        temperature: float = 0.3,\n        json_object: bool = False,\n    ) -> str:\n        \"\"\"\n        Generate a chat completion.\n\n        Args:\n            model: Model identifier (e.g., \"z-ai/glm-4.7\")\n            system: System message content\n            user: User message content\n            temperature: Sampling temperature (default 0.3)\n            json_object: If True, request JSON object response format\n\n        Returns:\n            Generated text content (may contain JSON if json_object=True)\n        \"\"\"\n        ...",
    "start_line": 6,
    "end_line": 31,
    "has_docstring": true,
    "docstring": "Interface for LLM chat/completion services.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Protocol"
    ],
    "class_name": null,
    "display_name": "class LLMClient",
    "component_id": "src.ck_exporter.core.ports.llm.LLMClient"
  },
  "src.ck_exporter.core.ports.topic_labeler.TopicLabeler": {
    "id": "src.ck_exporter.core.ports.topic_labeler.TopicLabeler",
    "name": "TopicLabeler",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/core/ports/topic_labeler.py",
    "relative_path": "src/ck_exporter/core/ports/topic_labeler.py",
    "depends_on": [],
    "source_code": "class TopicLabeler(Protocol):\n    \"\"\"Interface for labeling discovered topic clusters.\"\"\"\n\n    def label_topic(\n        self,\n        topic_id: int,\n        representative_docs: list[tuple[str, str]],\n        keywords: list[str],\n    ) -> dict[str, str]:\n        \"\"\"\n        Generate a label (name + description) for a topic cluster.\n\n        Args:\n            topic_id: Numeric topic identifier\n            representative_docs: List of (conversation_id, doc_text) tuples\n            keywords: Top keywords from topic model\n\n        Returns:\n            Dict with \"name\" and \"description\" keys\n        \"\"\"\n        ...",
    "start_line": 6,
    "end_line": 26,
    "has_docstring": true,
    "docstring": "Interface for labeling discovered topic clusters.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Protocol"
    ],
    "class_name": null,
    "display_name": "class TopicLabeler",
    "component_id": "src.ck_exporter.core.ports.topic_labeler.TopicLabeler"
  },
  "src.ck_exporter.embeddings.EmbeddingClient": {
    "id": "src.ck_exporter.embeddings.EmbeddingClient",
    "name": "EmbeddingClient",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/embeddings.py",
    "relative_path": "src/ck_exporter/embeddings.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_embedder.OpenRouterEmbedder"
    ],
    "source_code": "class EmbeddingClient:\n    \"\"\"\n    Backward-compatible wrapper around OpenRouterEmbedder.\n\n    Provides old method names (get_embeddings, get_embeddings_pooled) for compatibility.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"openai/text-embedding-3-small\",\n        use_openrouter: bool = True,\n    ):\n        \"\"\"\n        Initialize embedding client.\n\n        Args:\n            model: Model identifier (OpenRouter format)\n            use_openrouter: If True, use OpenRouter API; otherwise use standard OpenAI\n        \"\"\"\n        self._embedder = OpenRouterEmbedder(model=model, use_openrouter=use_openrouter)\n        self.model = model\n\n    def get_embeddings(self, texts: list[str], batch_size: int = 100) -> np.ndarray:\n        \"\"\"\n        Get embeddings for multiple texts, batching as needed.\n\n        Args:\n            texts: List of text strings to embed\n            batch_size: Maximum texts per API call (default 100)\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        return self._embedder.embed(texts, batch_size=batch_size)\n\n    def get_embeddings_pooled(\n        self,\n        texts: list[str],\n        chunk_tokens: int = 600,\n        overlap_tokens: int = 80,\n        cache_dir: Optional[Path] = None,\n    ) -> np.ndarray:\n        \"\"\"\n        Get pooled embeddings for multiple texts by chunking, embedding chunks, and pooling.\n\n        Args:\n            texts: List of text strings to embed\n            chunk_tokens: Maximum tokens per chunk (default 600)\n            overlap_tokens: Token overlap between chunks (default 80)\n            cache_dir: Optional directory for caching embeddings\n\n        Returns:\n            numpy array of shape (n_texts, embedding_dim)\n        \"\"\"\n        return self._embedder.embed_pooled(\n            texts,\n            chunk_tokens=chunk_tokens,\n            overlap_tokens=overlap_tokens,\n            pooling=\"mean\",\n            cache_dir=cache_dir,\n        )",
    "start_line": 18,
    "end_line": 78,
    "has_docstring": true,
    "docstring": "Backward-compatible wrapper around OpenRouterEmbedder.\n\nProvides old method names (get_embeddings, get_embeddings_pooled) for compatibility.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EmbeddingClient",
    "component_id": "src.ck_exporter.embeddings.EmbeddingClient"
  },
  "src.ck_exporter.extract_openai.extract_export": {
    "id": "src.ck_exporter.extract_openai.extract_export",
    "name": "extract_export",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/extract_openai.py",
    "relative_path": "src/ck_exporter/extract_openai.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.bootstrap.build_atom_extractor"
    ],
    "source_code": "def extract_export(\n    input_path: Path,\n    evidence_dir: Path,\n    atoms_dir: Path,\n    fast_model: Optional[str] = None,\n    big_model: Optional[str] = None,\n    max_concurrency: Optional[int] = None,\n    skip_existing: bool = True,\n    use_openrouter: bool = True,\n    conversation_id: Optional[str] = None,\n    limit: Optional[int] = None,\n    progress_cb: Optional[Callable[[int, int, Optional[dict]], None]] = None,\n) -> None:\n    \"\"\"\n    Process export and extract atoms for all conversations using two-pass OpenRouter pipeline.\n\n    This is a backward-compatible wrapper that creates adapters and calls the new pipeline.\n\n    Accepts either:\n    - A top-level list of conversations (standard export format)\n    - A single conversation object with mapping/current_node\n    \"\"\"\n    # Set defaults from environment or hardcoded defaults\n    max_concurrency = max_concurrency or int(os.getenv(\"CKX_MAX_CONCURRENCY\", \"8\"))\n\n    # Create shared client for efficiency\n    shared_client = make_openrouter_client(use_openrouter)\n\n    # Build atom extractor using bootstrap (handles env var selection)\n    extractor = build_atom_extractor(\n        fast_model=fast_model,\n        big_model=big_model,\n        use_openrouter=use_openrouter,\n        shared_client=shared_client,\n    )\n\n    # Delegate to pipeline\n    _extract_export(\n        input_path=input_path,\n        evidence_dir=evidence_dir,\n        atoms_dir=atoms_dir,\n        extractor=extractor,\n        max_concurrency=max_concurrency,\n        skip_existing=skip_existing,\n        conversation_id=conversation_id,\n        limit=limit,\n        progress_cb=progress_cb,\n    )",
    "start_line": 18,
    "end_line": 65,
    "has_docstring": true,
    "docstring": "Process export and extract atoms for all conversations using two-pass OpenRouter pipeline.\n\nThis is a backward-compatible wrapper that creates adapters and calls the new pipeline.\n\nAccepts either:\n- A top-level list of conversations (standard export format)\n- A single conversation object with mapping/current_node",
    "parameters": [
      "input_path",
      "evidence_dir",
      "atoms_dir",
      "fast_model",
      "big_model",
      "max_concurrency",
      "skip_existing",
      "use_openrouter",
      "conversation_id",
      "limit",
      "progress_cb"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_export",
    "component_id": "src.ck_exporter.extract_openai.extract_export"
  },
  "src.ck_exporter.logging.JsonFormatter": {
    "id": "src.ck_exporter.logging.JsonFormatter",
    "name": "JsonFormatter",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [],
    "source_code": "class JsonFormatter(logging.Formatter):\n    \"\"\"JSON formatter for structured logging.\"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format log record as JSON.\"\"\"\n        log_data: Dict[str, Any] = {\n            \"ts\": datetime.utcnow().isoformat() + \"Z\",\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n        }\n\n        # Add event type if present\n        if hasattr(record, \"event\"):\n            log_data[\"event\"] = record.event\n\n        # Add all extra fields\n        for key, value in record.__dict__.items():\n            if key not in {\n                \"name\",\n                \"msg\",\n                \"args\",\n                \"created\",\n                \"filename\",\n                \"funcName\",\n                \"levelname\",\n                \"levelno\",\n                \"lineno\",\n                \"module\",\n                \"msecs\",\n                \"message\",\n                \"pathname\",\n                \"process\",\n                \"processName\",\n                \"relativeCreated\",\n                \"thread\",\n                \"threadName\",\n                \"exc_info\",\n                \"exc_text\",\n                \"stack_info\",\n                \"event\",\n            }:\n                log_data[key] = value\n\n        # Add exception info if present\n        if record.exc_info:\n            exc_type, exc_value, exc_traceback = record.exc_info\n            log_data[\"exception\"] = {\n                \"type\": exc_type.__name__ if exc_type else None,\n                \"message\": str(exc_value) if exc_value else None,\n                \"traceback\": self.formatException(record.exc_info) if exc_traceback else None,\n            }\n\n        return json.dumps(log_data, ensure_ascii=False)",
    "start_line": 23,
    "end_line": 76,
    "has_docstring": true,
    "docstring": "JSON formatter for structured logging.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "logging.Formatter"
    ],
    "class_name": null,
    "display_name": "class JsonFormatter",
    "component_id": "src.ck_exporter.logging.JsonFormatter"
  },
  "src.ck_exporter.logging.PlainFormatter": {
    "id": "src.ck_exporter.logging.PlainFormatter",
    "name": "PlainFormatter",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [],
    "source_code": "class PlainFormatter(logging.Formatter):\n    \"\"\"Plain text formatter for simple log output.\"\"\"\n\n    def __init__(self):\n        super().__init__(fmt=\"%(levelname)s %(name)s: %(message)s\")\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format log record as plain text.\"\"\"\n        msg = super().format(record)\n        if record.exc_info:\n            msg += \"\\n\" + self.formatException(record.exc_info)\n        return msg",
    "start_line": 79,
    "end_line": 90,
    "has_docstring": true,
    "docstring": "Plain text formatter for simple log output.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "logging.Formatter"
    ],
    "class_name": null,
    "display_name": "class PlainFormatter",
    "component_id": "src.ck_exporter.logging.PlainFormatter"
  },
  "src.ck_exporter.logging.get_current_log_mode": {
    "id": "src.ck_exporter.logging.get_current_log_mode",
    "name": "get_current_log_mode",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [],
    "source_code": "def get_current_log_mode() -> Optional[str]:\n    \"\"\"Get the current logging mode.\"\"\"\n    return _current_mode",
    "start_line": 93,
    "end_line": 95,
    "has_docstring": true,
    "docstring": "Get the current logging mode.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_current_log_mode",
    "component_id": "src.ck_exporter.logging.get_current_log_mode"
  },
  "src.ck_exporter.logging.should_show_progress": {
    "id": "src.ck_exporter.logging.should_show_progress",
    "name": "should_show_progress",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [],
    "source_code": "def should_show_progress() -> bool:\n    \"\"\"\n    Determine if Rich progress bars/spinners should be shown.\n\n    Returns:\n        True if progress should be shown (interactive mode), False otherwise\n    \"\"\"\n    global _current_mode\n    if _current_mode is None:\n        # Default: show progress if stderr is a TTY\n        return sys.stderr.isatty()\n    \n    # human and hybrid modes show progress if interactive\n    if _current_mode in (\"human\", \"hybrid\"):\n        return sys.stderr.isatty()\n    \n    # machine mode never shows progress\n    return False",
    "start_line": 98,
    "end_line": 115,
    "has_docstring": true,
    "docstring": "Determine if Rich progress bars/spinners should be shown.\n\nReturns:\n    True if progress should be shown (interactive mode), False otherwise",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function should_show_progress",
    "component_id": "src.ck_exporter.logging.should_show_progress"
  },
  "src.ck_exporter.logging.configure_logging": {
    "id": "src.ck_exporter.logging.configure_logging",
    "name": "configure_logging",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [
      "src.ck_exporter.logging.JsonFormatter",
      "src.ck_exporter.logging.PlainFormatter"
    ],
    "source_code": "def configure_logging(\n    level: str = \"INFO\",\n    format: str = \"json\",  # noqa: A002\n    log_file: Optional[Path] = None,\n    third_party_level: str = \"WARNING\",\n    hybrid_ui: bool = True,\n    mode: Literal[\"human\", \"hybrid\", \"machine\", \"auto\"] = \"auto\",\n) -> None:\n    \"\"\"\n    Configure logging for ck_exporter.\n\n    Args:\n        level: Log level for ck_exporter modules (DEBUG, INFO, WARNING, ERROR)\n        format: Log format (json, rich, plain) - used when mode is not specified\n        log_file: Optional file path to write logs to\n        third_party_level: Log level for third-party libraries (default WARNING)\n        hybrid_ui: If True, RichHandler will use stderr (for hybrid UI mode)\n        mode: Logging mode (human/hybrid/machine/auto). If 'auto', determines based on TTY and log_file.\n            - human: Rich console logs to stderr (default for interactive)\n            - hybrid: Rich console logs to stderr + JSON logs to file\n            - machine: JSON logs to file only (no console output)\n            - auto: human if TTY, hybrid if log_file set, otherwise plain\n    \"\"\"\n    global _current_mode\n    \n    # Resolve auto mode\n    if mode == \"auto\":\n        if sys.stderr.isatty():\n            mode = \"human\"\n        elif log_file:\n            mode = \"hybrid\"\n        else:\n            mode = \"human\"  # Default to human even if not TTY\n    \n    _current_mode = mode\n    \n    # Convert level strings to logging constants\n    log_level = getattr(logging, level.upper(), logging.INFO)\n    third_party_log_level = getattr(logging, third_party_level.upper(), logging.WARNING)\n\n    # Get root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(log_level)\n\n    # Remove existing handlers\n    root_logger.handlers.clear()\n\n    # Determine console format based on mode\n    if mode == \"machine\":\n        # Machine mode: no console output, only file (if specified)\n        console_format = None\n    elif mode == \"hybrid\":\n        # Hybrid mode: Rich console + JSON file\n        console_format = \"rich\"\n    elif mode == \"human\":\n        # Human mode: Rich console (or plain if rich not available)\n        console_format = \"rich\"\n    else:\n        # Fallback to format parameter for backward compatibility\n        console_format = format\n\n    # Create console handler based on mode/format\n    if console_format == \"json\":\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(JsonFormatter())\n    elif console_format == \"rich\":\n        if RichHandler is None:\n            # Fallback to plain if rich not available\n            handler = logging.StreamHandler(sys.stderr)\n            handler.setFormatter(PlainFormatter())\n        else:\n            handler = RichHandler(\n                console=None,  # Use default console\n                show_path=False,\n                rich_tracebacks=True,\n                show_time=True,\n            )\n            # RichHandler writes to stderr by default, which is what we want\n    elif console_format == \"plain\":\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(PlainFormatter())\n    else:\n        # No console handler for machine mode (unless log_file not set, then use plain)\n        handler = None\n\n    if handler:\n        handler.setLevel(log_level)\n        root_logger.addHandler(handler)\n\n    # Add file handler if specified (always JSON in file)\n    if log_file:\n        file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\n        file_handler.setFormatter(JsonFormatter())\n        file_handler.setLevel(log_level)\n        root_logger.addHandler(file_handler)\n    elif mode == \"machine\":\n        # Machine mode requires a log file\n        # If not provided, fall back to stderr with JSON\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(JsonFormatter())\n        handler.setLevel(log_level)\n        root_logger.addHandler(handler)\n\n    # Configure third-party loggers to be quieter\n    third_party_loggers = [\n        \"bertopic\",\n        \"hdbscan\",\n        \"umap\",\n        \"numba\",\n        \"openai\",\n        \"httpx\",\n        \"urllib3\",\n        \"httpcore\",\n    ]\n    for logger_name in third_party_loggers:\n        third_party_logger = logging.getLogger(logger_name)\n        third_party_logger.setLevel(third_party_log_level)",
    "start_line": 118,
    "end_line": 234,
    "has_docstring": true,
    "docstring": "Configure logging for ck_exporter.\n\nArgs:\n    level: Log level for ck_exporter modules (DEBUG, INFO, WARNING, ERROR)\n    format: Log format (json, rich, plain) - used when mode is not specified\n    log_file: Optional file path to write logs to\n    third_party_level: Log level for third-party libraries (default WARNING)\n    hybrid_ui: If True, RichHandler will use stderr (for hybrid UI mode)\n    mode: Logging mode (human/hybrid/machine/auto). If 'auto', determines based on TTY and log_file.\n        - human: Rich console logs to stderr (default for interactive)\n        - hybrid: Rich console logs to stderr + JSON logs to file\n        - machine: JSON logs to file only (no console output)\n        - auto: human if TTY, hybrid if log_file set, otherwise plain",
    "parameters": [
      "level",
      "format",
      "log_file",
      "third_party_level",
      "hybrid_ui",
      "mode"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function configure_logging",
    "component_id": "src.ck_exporter.logging.configure_logging"
  },
  "src.ck_exporter.logging.get_logger": {
    "id": "src.ck_exporter.logging.get_logger",
    "name": "get_logger",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [],
    "source_code": "def get_logger(name: str) -> logging.Logger:\n    \"\"\"\n    Get a logger instance for a module.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Logger instance\n    \"\"\"\n    return logging.getLogger(name)",
    "start_line": 239,
    "end_line": 249,
    "has_docstring": true,
    "docstring": "Get a logger instance for a module.\n\nArgs:\n    name: Logger name (typically __name__)\n\nReturns:\n    Logger instance",
    "parameters": [
      "name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_logger",
    "component_id": "src.ck_exporter.logging.get_logger"
  },
  "src.ck_exporter.logging.with_context": {
    "id": "src.ck_exporter.logging.with_context",
    "name": "with_context",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/logging.py",
    "relative_path": "src/ck_exporter/logging.py",
    "depends_on": [],
    "source_code": "def with_context(logger: logging.Logger, **fields: Any) -> logging.LoggerAdapter:\n    \"\"\"\n    Create a logger adapter with context fields.\n\n    Args:\n        logger: Base logger\n        **fields: Context fields to include in all log records\n\n    Returns:\n        LoggerAdapter with context fields\n\n    Example:\n        logger = get_logger(__name__)\n        conv_logger = with_context(logger, conversation_id=\"abc123\", stage=\"extract\")\n        conv_logger.info(\"Processing chunk\", extra={\"chunk_num\": 1})\n    \"\"\"\n    return logging.LoggerAdapter(logger, fields)",
    "start_line": 263,
    "end_line": 279,
    "has_docstring": true,
    "docstring": "Create a logger adapter with context fields.\n\nArgs:\n    logger: Base logger\n    **fields: Context fields to include in all log records\n\nReturns:\n    LoggerAdapter with context fields\n\nExample:\n    logger = get_logger(__name__)\n    conv_logger = with_context(logger, conversation_id=\"abc123\", stage=\"extract\")\n    conv_logger.info(\"Processing chunk\", extra={\"chunk_num\": 1})",
    "parameters": [
      "logger"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function with_context",
    "component_id": "src.ck_exporter.logging.with_context"
  },
  "src.ck_exporter.pipeline.assignment.load_topic_registry": {
    "id": "src.ck_exporter.pipeline.assignment.load_topic_registry",
    "name": "load_topic_registry",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/assignment.py",
    "relative_path": "src/ck_exporter/pipeline/assignment.py",
    "depends_on": [
      "src.ck_exporter.core.models.topics.TopicRegistry"
    ],
    "source_code": "def load_topic_registry(registry_path: Path) -> TopicRegistry:\n    \"\"\"\n    Load topic registry from JSON file.\n\n    Args:\n        registry_path: Path to topic_registry.json\n\n    Returns:\n        TopicRegistry object\n    \"\"\"\n    if not registry_path.exists():\n        raise FileNotFoundError(f\"Topic registry not found: {registry_path}\")\n\n    with registry_path.open(\"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    return TopicRegistry(**data)",
    "start_line": 22,
    "end_line": 38,
    "has_docstring": true,
    "docstring": "Load topic registry from JSON file.\n\nArgs:\n    registry_path: Path to topic_registry.json\n\nReturns:\n    TopicRegistry object",
    "parameters": [
      "registry_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function load_topic_registry",
    "component_id": "src.ck_exporter.pipeline.assignment.load_topic_registry"
  },
  "src.ck_exporter.pipeline.assignment.assign_topics": {
    "id": "src.ck_exporter.pipeline.assignment.assign_topics",
    "name": "assign_topics",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/assignment.py",
    "relative_path": "src/ck_exporter/pipeline/assignment.py",
    "depends_on": [
      "src.ck_exporter.core.models.topics.TopicAssignment",
      "src.ck_exporter.pipeline.topics.build_conversation_documents",
      "src.ck_exporter.embeddings.EmbeddingClient",
      "src.ck_exporter.core.models.topics.ConversationTopics",
      "src.ck_exporter.pipeline.io.load.load_conversations",
      "src.ck_exporter.adapters.openrouter_embedder.cosine_similarity",
      "src.ck_exporter.pipeline.io.load.is_claude_conversation",
      "src.ck_exporter.pipeline.io.load.convert_claude_to_chatgpt",
      "src.ck_exporter.pipeline.topics.read_jsonl"
    ],
    "source_code": "def assign_topics(\n    input_path: Path,\n    atoms_path: Path,\n    decisions_path: Path,\n    questions_path: Path,\n    registry: TopicRegistry,\n    embedding_model: str = \"openai/text-embedding-3-small\",\n    primary_threshold: float = 0.60,\n    secondary_threshold: float = 0.55,\n    use_openrouter: bool = True,\n    use_pooling: bool = True,\n    chunk_tokens: int = 600,\n    overlap_tokens: int = 80,\n    cache_dir: Optional[Path] = None,\n    limit: Optional[int] = None,\n) -> List[ConversationTopics]:\n    \"\"\"\n    Assign topics to conversations using cosine similarity.\n\n    Args:\n        input_path: Path to conversation JSON file(s)\n        atoms_path: Path to consolidated atoms.jsonl\n        decisions_path: Path to consolidated decisions.jsonl\n        questions_path: Path to consolidated open_questions.jsonl\n        registry: TopicRegistry with discovered topics\n        embedding_model: OpenRouter model identifier\n        primary_threshold: Minimum score for primary topic\n        secondary_threshold: Minimum score for secondary topics\n        use_openrouter: Whether to use OpenRouter\n        use_pooling: Whether to use chunked pooling (default True)\n        chunk_tokens: Maximum tokens per chunk when pooling (default 600)\n        overlap_tokens: Token overlap between chunks when pooling (default 80)\n        cache_dir: Optional directory for caching embeddings\n        limit: Optional limit on number of conversations to process\n\n    Returns:\n        List of ConversationTopics assignments\n    \"\"\"\n    # Validate that embedding model matches registry\n    if registry.embedding_model != embedding_model:\n        logger.warning(\n            \"Embedding model mismatch\",\n            extra={\n                \"event\": \"assignment.model_mismatch\",\n                \"registry_model\": registry.embedding_model,\n                \"assignment_model\": embedding_model,\n            },\n        )\n    # Build conversation documents\n    documents, titles = build_conversation_documents(\n        input_path, atoms_path, decisions_path, questions_path\n    )\n\n    if not documents:\n        logger.warning(\n            \"No conversations found\",\n            extra={\"event\": \"assignment.empty\"},\n        )\n        return []\n\n    # Build conversation metadata map (project id/name when available)\n    project_meta: Dict[str, Dict[str, str]] = {}\n    try:\n        conversations = load_conversations(input_path, limit=limit)\n        for conv in conversations:\n            if is_claude_conversation(conv):\n                conv = convert_claude_to_chatgpt(conv)\n            conv_id = conv.get(\"conversation_id\") or conv.get(\"id\") or conv.get(\"uuid\", \"\")\n            if not conv_id:\n                continue\n\n            project_id = conv.get(\"project_id\") or conv.get(\"project_uuid\")\n            project_name = conv.get(\"project_name\")\n            if not project_id and isinstance(conv.get(\"project\"), dict):\n                project_id = conv[\"project\"].get(\"uuid\")\n            if not project_name and isinstance(conv.get(\"project\"), dict):\n                project_name = conv[\"project\"].get(\"name\")\n\n            meta: Dict[str, str] = {}\n            if project_id:\n                meta[\"project_id\"] = str(project_id)\n            if project_name:\n                meta[\"project_name\"] = str(project_name)\n            if meta:\n                project_meta[conv_id] = meta\n    except Exception:\n        # Metadata is optional; assignment can proceed without it.\n        project_meta = {}\n\n    # Count atoms per conversation for metadata\n    atoms = read_jsonl(atoms_path)\n    atom_counts = {}\n    for atom in atoms:\n        conv_id = atom.get(\"source_conversation_id\", \"\")\n        if conv_id:\n            atom_counts[conv_id] = atom_counts.get(conv_id, 0) + 1\n\n    # Generate embeddings for all conversations\n    logger.info(\n        \"Generating embeddings\",\n        extra={\n            \"event\": \"assignment.embeddings.start\",\n            \"num_conversations\": len(documents),\n            \"use_pooling\": use_pooling,\n        },\n    )\n    embedding_client = EmbeddingClient(model=embedding_model, use_openrouter=use_openrouter)\n\n    conv_ids = list(documents.keys())\n    doc_texts = [documents[conv_id] for conv_id in conv_ids]\n    if use_pooling:\n        conv_embeddings = embedding_client.get_embeddings_pooled(\n            doc_texts,\n            chunk_tokens=chunk_tokens,\n            overlap_tokens=overlap_tokens,\n            cache_dir=cache_dir,\n        )\n    else:\n        conv_embeddings = embedding_client.get_embeddings(doc_texts)\n\n    logger.info(\n        \"Generated embeddings\",\n        extra={\n            \"event\": \"assignment.embeddings.complete\",\n            \"shape\": list(conv_embeddings.shape),\n        },\n    )\n\n    # Get topic centroids\n    topic_centroids = {}\n    for topic in registry.topics:\n        if topic.centroid_embedding:\n            topic_centroids[topic.topic_id] = np.array(topic.centroid_embedding)\n\n    if not topic_centroids:\n        logger.error(\n            \"No topic centroids found in registry\",\n            extra={\"event\": \"assignment.error\", \"reason\": \"no_centroids\"},\n        )\n        return []\n\n    # Assign topics to each conversation\n    assignments = []\n    logger.info(\n        \"Assigning topics to conversations\",\n        extra={\"event\": \"assignment.start\"},\n    )\n\n    for i, conv_id in enumerate(conv_ids):\n        conv_embedding = conv_embeddings[i]\n        title = titles.get(conv_id, \"Untitled Conversation\")\n        meta = project_meta.get(conv_id, {})\n\n        # Compute similarity to all topic centroids\n        similarities = []\n        for topic_id, centroid in topic_centroids.items():\n            try:\n                score = cosine_similarity(conv_embedding, centroid)\n                topic_name = next((t.name for t in registry.topics if t.topic_id == topic_id), f\"Topic {topic_id}\")\n                similarities.append((topic_id, topic_name, score))\n            except Exception as e:\n                logger.warning(\n                    \"Error computing similarity for topic\",\n                    extra={\n                        \"event\": \"assignment.similarity.error\",\n                        \"topic_id\": topic_id,\n                    },\n                    exc_info=True,\n                )\n                continue\n\n        # Sort by score descending\n        similarities.sort(key=lambda x: x[2], reverse=True)\n\n        if not similarities:\n            # No valid similarities, create empty assignment\n            assignments.append(\n                ConversationTopics(\n                    conversation_id=conv_id,\n                    title=title,\n                    project_id=meta.get(\"project_id\"),\n                    project_name=meta.get(\"project_name\"),\n                    topics=[],\n                    atom_count=atom_counts.get(conv_id, 0),\n                    review_flag=True,\n                )\n            )\n            continue\n\n        # Primary topic: always assign top-scoring topic\n        primary_id, primary_name, primary_score = similarities[0]\n        topic_assignments = [\n            TopicAssignment(\n                topic_id=primary_id,\n                name=primary_name,\n                score=primary_score,\n                rank=\"primary\",\n            )\n        ]\n\n        # Secondary topics: any topic with score >= secondary_threshold\n        # and within 0.25 of primary score\n        for topic_id, topic_name, score in similarities[1:]:\n            if score >= secondary_threshold and (primary_score - score) <= 0.25:\n                topic_assignments.append(\n                    TopicAssignment(\n                        topic_id=topic_id,\n                        name=topic_name,\n                        score=score,\n                        rank=\"secondary\",\n                    )\n                )\n\n        # Review flag: set if primary score is low or if secondary is very close to primary\n        review_flag = False\n        if primary_score < primary_threshold:\n            review_flag = True\n        elif len(similarities) > 1:\n            secondary_score = similarities[1][2]\n            if secondary_score >= secondary_threshold and (primary_score - secondary_score) < 0.08:\n                review_flag = True\n\n        assignments.append(\n            ConversationTopics(\n                conversation_id=conv_id,\n                title=title,\n                project_id=meta.get(\"project_id\"),\n                project_name=meta.get(\"project_name\"),\n                topics=topic_assignments,\n                atom_count=atom_counts.get(conv_id, 0),\n                review_flag=review_flag,\n            )\n            )\n\n    logger.info(\n        \"Assigned topics to conversations\",\n        extra={\n            \"event\": \"assignment.complete\",\n            \"num_assignments\": len(assignments),\n        },\n    )\n    return assignments",
    "start_line": 41,
    "end_line": 282,
    "has_docstring": true,
    "docstring": "Assign topics to conversations using cosine similarity.\n\nArgs:\n    input_path: Path to conversation JSON file(s)\n    atoms_path: Path to consolidated atoms.jsonl\n    decisions_path: Path to consolidated decisions.jsonl\n    questions_path: Path to consolidated open_questions.jsonl\n    registry: TopicRegistry with discovered topics\n    embedding_model: OpenRouter model identifier\n    primary_threshold: Minimum score for primary topic\n    secondary_threshold: Minimum score for secondary topics\n    use_openrouter: Whether to use OpenRouter\n    use_pooling: Whether to use chunked pooling (default True)\n    chunk_tokens: Maximum tokens per chunk when pooling (default 600)\n    overlap_tokens: Token overlap between chunks when pooling (default 80)\n    cache_dir: Optional directory for caching embeddings\n    limit: Optional limit on number of conversations to process\n\nReturns:\n    List of ConversationTopics assignments",
    "parameters": [
      "input_path",
      "atoms_path",
      "decisions_path",
      "questions_path",
      "registry",
      "embedding_model",
      "primary_threshold",
      "secondary_threshold",
      "use_openrouter",
      "use_pooling",
      "chunk_tokens",
      "overlap_tokens",
      "cache_dir",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function assign_topics",
    "component_id": "src.ck_exporter.pipeline.assignment.assign_topics"
  },
  "src.ck_exporter.pipeline.assignment.save_assignments": {
    "id": "src.ck_exporter.pipeline.assignment.save_assignments",
    "name": "save_assignments",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/assignment.py",
    "relative_path": "src/ck_exporter/pipeline/assignment.py",
    "depends_on": [],
    "source_code": "def save_assignments(assignments: List[ConversationTopics], output_path: Path) -> None:\n    \"\"\"\n    Save topic assignments to JSONL file.\n\n    Args:\n        assignments: List of ConversationTopics assignments\n        output_path: Path to save assignments.jsonl\n    \"\"\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        for assignment in assignments:\n            # Convert to dict for JSON serialization\n            assignment_dict = assignment.model_dump(exclude_none=True)\n            f.write(json.dumps(assignment_dict, ensure_ascii=False) + \"\\n\")\n\n    logger.info(\n        \"Saved assignments\",\n        extra={\n            \"event\": \"assignment.saved\",\n            \"output_path\": str(output_path),\n            \"num_assignments\": len(assignments),\n        },\n    )\n\n    # Also create review queue for flagged items\n    review_items = [a for a in assignments if a.review_flag]\n    if review_items:\n        review_path = output_path.parent / \"review_queue.jsonl\"\n        with review_path.open(\"w\", encoding=\"utf-8\") as f:\n            for assignment in review_items:\n                primary = next((t for t in assignment.topics if t.rank == \"primary\"), None)\n                review_dict = {\n                    \"conversation_id\": assignment.conversation_id,\n                    \"title\": assignment.title,\n                    \"project_id\": assignment.project_id,\n                    \"project_name\": assignment.project_name,\n                    \"primary_topic\": primary.name if primary else \"None\",\n                    \"primary_score\": primary.score if primary else 0.0,\n                    \"reason\": \"low_confidence\" if (primary and primary.score < 0.60) else \"ambiguous\",\n                }\n                # Keep output tidy for non-Claude exports\n                f.write(json.dumps({k: v for k, v in review_dict.items() if v is not None}, ensure_ascii=False) + \"\\n\")\n\n        logger.info(\n            \"Created review queue\",\n            extra={\n                \"event\": \"assignment.review_queue\",\n                \"review_path\": str(review_path),\n                \"num_review_items\": len(review_items),\n            },\n        )",
    "start_line": 285,
    "end_line": 336,
    "has_docstring": true,
    "docstring": "Save topic assignments to JSONL file.\n\nArgs:\n    assignments: List of ConversationTopics assignments\n    output_path: Path to save assignments.jsonl",
    "parameters": [
      "assignments",
      "output_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function save_assignments",
    "component_id": "src.ck_exporter.pipeline.assignment.save_assignments"
  },
  "src.ck_exporter.pipeline.compile.sanitize_filename": {
    "id": "src.ck_exporter.pipeline.compile.sanitize_filename",
    "name": "sanitize_filename",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/compile.py",
    "relative_path": "src/ck_exporter/pipeline/compile.py",
    "depends_on": [],
    "source_code": "def sanitize_filename(name: str) -> str:\n    \"\"\"\n    Sanitize a string to be safe for use in filenames.\n    \n    Removes or replaces characters that are invalid on common filesystems:\n    - Windows: < > : \" / \\\\ | ? *\n    - Unix/Linux: / and null bytes\n    - Control characters (0x00-0x1f)\n    \n    Also handles:\n    - Leading/trailing whitespace and dots\n    - Empty strings after sanitization\n    - Overly long names (limits to 200 chars)\n    \n    Args:\n        name: The string to sanitize\n        \n    Returns:\n        A filesystem-safe string\n    \"\"\"\n    if not name or not isinstance(name, str):\n        return \"unnamed\"\n    \n    # Replace invalid characters with hyphens\n    sanitized = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', '-', name)\n    \n    # Replace multiple consecutive spaces/hyphens with single hyphen\n    sanitized = re.sub(r'[\\s\\-]+', '-', sanitized)\n    \n    # Remove leading/trailing dots, spaces, and hyphens\n    sanitized = sanitized.strip('. -')\n    \n    # Limit length to prevent filesystem issues\n    sanitized = sanitized[:200]\n    \n    # If empty after sanitization, use default\n    return sanitized if sanitized else \"unnamed\"",
    "start_line": 20,
    "end_line": 56,
    "has_docstring": true,
    "docstring": "Sanitize a string to be safe for use in filenames.\n\nRemoves or replaces characters that are invalid on common filesystems:\n- Windows: < > : \" / \\ | ? *\n- Unix/Linux: / and null bytes\n- Control characters (0x00-0x1f)\n\nAlso handles:\n- Leading/trailing whitespace and dots\n- Empty strings after sanitization\n- Overly long names (limits to 200 chars)\n\nArgs:\n    name: The string to sanitize\n    \nReturns:\n    A filesystem-safe string",
    "parameters": [
      "name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function sanitize_filename",
    "component_id": "src.ck_exporter.pipeline.compile.sanitize_filename"
  },
  "src.ck_exporter.pipeline.compile.load_atoms_jsonl": {
    "id": "src.ck_exporter.pipeline.compile.load_atoms_jsonl",
    "name": "load_atoms_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/compile.py",
    "relative_path": "src/ck_exporter/pipeline/compile.py",
    "depends_on": [],
    "source_code": "def load_atoms_jsonl(file_path: Path) -> List[Dict[str, Any]]:\n    \"\"\"Load atoms from a JSONL file.\"\"\"\n    if not file_path.exists():\n        return []\n\n    atoms = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                try:\n                    atoms.append(json.loads(line))\n                except json.JSONDecodeError:\n                    continue\n\n    return atoms",
    "start_line": 59,
    "end_line": 74,
    "has_docstring": true,
    "docstring": "Load atoms from a JSONL file.",
    "parameters": [
      "file_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function load_atoms_jsonl",
    "component_id": "src.ck_exporter.pipeline.compile.load_atoms_jsonl"
  },
  "src.ck_exporter.pipeline.compile.group_atoms_by_topic": {
    "id": "src.ck_exporter.pipeline.compile.group_atoms_by_topic",
    "name": "group_atoms_by_topic",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/compile.py",
    "relative_path": "src/ck_exporter/pipeline/compile.py",
    "depends_on": [],
    "source_code": "def group_atoms_by_topic(atoms: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Group atoms by topic.\"\"\"\n    grouped = {}\n    for atom in atoms:\n        topic = atom.get(\"topic\", \"uncategorized\")\n        if topic not in grouped:\n            grouped[topic] = []\n        grouped[topic].append(atom)\n    return grouped",
    "start_line": 77,
    "end_line": 85,
    "has_docstring": true,
    "docstring": "Group atoms by topic.",
    "parameters": [
      "atoms"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function group_atoms_by_topic",
    "component_id": "src.ck_exporter.pipeline.compile.group_atoms_by_topic"
  },
  "src.ck_exporter.pipeline.compile.compile_conversation_docs": {
    "id": "src.ck_exporter.pipeline.compile.compile_conversation_docs",
    "name": "compile_conversation_docs",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/compile.py",
    "relative_path": "src/ck_exporter/pipeline/compile.py",
    "depends_on": [
      "src.ck_exporter.logging.with_context",
      "src.ck_exporter.pipeline.compile.load_atoms_jsonl",
      "src.ck_exporter.pipeline.compile.sanitize_filename"
    ],
    "source_code": "def compile_conversation_docs(\n    conversation_id: str,\n    atoms_dir: Path,\n    output_dir: Path,\n    title: Optional[str] = None,\n) -> None:\n    \"\"\"Compile docs for a single conversation.\"\"\"\n    conv_atoms_dir = atoms_dir / conversation_id\n\n    # Load atoms\n    facts = load_atoms_jsonl(conv_atoms_dir / \"facts.jsonl\")\n    decisions = load_atoms_jsonl(conv_atoms_dir / \"decisions.jsonl\")\n    questions = load_atoms_jsonl(conv_atoms_dir / \"open_questions.jsonl\")\n\n    conv_logger = with_context(logger, conversation_id=conversation_id)\n\n    if not facts and not decisions and not questions:\n        conv_logger.warning(\n            \"No atoms found\",\n            extra={\"event\": \"compile.conversation.skipped\", \"reason\": \"no_atoms\"},\n        )\n        return\n\n    # Setup Jinja2 environment\n    if TEMPLATE_DIR.exists():\n        env = Environment(\n            loader=FileSystemLoader(str(TEMPLATE_DIR)),\n            autoescape=select_autoescape([\"html\", \"xml\"]),\n        )\n    else:\n        # Fallback: create basic templates inline\n        env = Environment(loader=FileSystemLoader(\".\"))\n\n    # Create output directory\n    conv_output_dir = output_dir / conversation_id\n    conv_output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Compile overview\n    overview_template = env.get_template(\"overview.md.j2\") if TEMPLATE_DIR.exists() else None\n    if overview_template:\n        overview_content = overview_template.render(\n            conversation_id=conversation_id,\n            title=title or f\"Conversation {conversation_id}\",\n            facts=facts,\n            decisions=decisions,\n            questions=questions,\n        )\n        (conv_output_dir / \"overview.md\").write_text(overview_content, encoding=\"utf-8\")\n\n    # Compile architecture doc\n    arch_facts = [f for f in facts if f.get(\"topic\", \"\").lower() in [\"architecture\", \"pipeline\", \"storage\", \"integrations\"]]\n    if arch_facts:\n        arch_template = env.get_template(\"architecture.md.j2\") if TEMPLATE_DIR.exists() else None\n        if arch_template:\n            arch_content = arch_template.render(\n                conversation_id=conversation_id,\n                facts=arch_facts,\n                decisions=[d for d in decisions if d.get(\"topic\", \"\").lower() in [\"architecture\", \"pipeline\", \"storage\", \"integrations\"]],\n            )\n            (conv_output_dir / \"architecture.md\").write_text(arch_content, encoding=\"utf-8\")\n\n    # Compile ADRs\n    adr_output_dir = output_dir / \"decisions\" / conversation_id\n    adr_output_dir.mkdir(parents=True, exist_ok=True)\n\n    adr_template = env.get_template(\"adr.md.j2\") if TEMPLATE_DIR.exists() else None\n    for idx, decision in enumerate(decisions, start=1):\n        # Sanitize topic for use in filename\n        topic_safe = sanitize_filename(decision.get('topic', 'decision'))\n        \n        if adr_template:\n            adr_content = adr_template.render(\n                adr_number=idx,\n                decision=decision,\n                conversation_id=conversation_id,\n            )\n            adr_path = adr_output_dir / f\"ADR-{idx:04d}-{topic_safe}.md\"\n            adr_path.write_text(adr_content, encoding=\"utf-8\")\n        else:\n            # Fallback: simple markdown\n            adr_path = adr_output_dir / f\"ADR-{idx:04d}-{topic_safe}.md\"\n            adr_content = f\"\"\"# ADR {idx:04d}: {decision.get('statement', 'Decision')}\n\n**Status**: {decision.get('status', 'active')}\n**Topic**: {decision.get('topic', 'uncategorized')}\n\n## Decision\n\n{decision.get('statement', '')}\n\n## Rationale\n\n{decision.get('rationale', 'Not provided')}\n\n## Alternatives Considered\n\n{chr(10).join(f\"- {alt}\" for alt in decision.get('alternatives', [])) or 'None listed'}\n\n## Consequences\n\n{decision.get('consequences', 'Not specified')}\n\n## Evidence\n\n{chr(10).join(f\"- Message ID: {e.get('message_id')} at {e.get('time_iso')}\" for e in decision.get('evidence', []))}\n\"\"\"\n            adr_path.write_text(adr_content, encoding=\"utf-8\")\n\n    conv_logger.info(\n        \"Compiled docs\",\n        extra={\n            \"event\": \"compile.conversation.complete\",\n            \"num_facts\": len(facts),\n            \"num_decisions\": len(decisions),\n            \"num_questions\": len(questions),\n        },\n    )",
    "start_line": 88,
    "end_line": 204,
    "has_docstring": true,
    "docstring": "Compile docs for a single conversation.",
    "parameters": [
      "conversation_id",
      "atoms_dir",
      "output_dir",
      "title"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function compile_conversation_docs",
    "component_id": "src.ck_exporter.pipeline.compile.compile_conversation_docs"
  },
  "src.ck_exporter.pipeline.compile.compile_docs": {
    "id": "src.ck_exporter.pipeline.compile.compile_docs",
    "name": "compile_docs",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/compile.py",
    "relative_path": "src/ck_exporter/pipeline/compile.py",
    "depends_on": [
      "src.ck_exporter.pipeline.compile.compile_conversation_docs",
      "src.ck_exporter.logging.should_show_progress"
    ],
    "source_code": "def compile_docs(\n    atoms_dir: Path,\n    output_dir: Path,\n    progress_cb: Optional[Callable[[int, int, Optional[dict]], None]] = None,\n) -> None:\n    \"\"\"\n    Compile documentation for all conversations.\n\n    Args:\n        atoms_dir: Directory containing conversation atom directories\n        output_dir: Output directory for compiled docs\n        progress_cb: Optional callback(completed, total, context) for progress updates\n    \"\"\"\n    if not atoms_dir.exists():\n        raise ValueError(f\"Atoms directory not found: {atoms_dir}\")\n\n    conversation_dirs = [d for d in atoms_dir.iterdir() if d.is_dir()]\n\n    if not conversation_dirs:\n        logger.warning(\n            \"No conversation directories found\",\n            extra={\"event\": \"compile.export.empty\"},\n        )\n        return\n\n    logger.info(\n        \"Compiling docs\",\n        extra={\n            \"event\": \"compile.export.start\",\n            \"num_conversations\": len(conversation_dirs),\n        },\n    )\n\n    # Notify progress callback of total\n    if progress_cb:\n        progress_cb(0, len(conversation_dirs), {})\n\n    if should_show_progress() and not progress_cb:\n        console = Console(stderr=True)\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            console=console,\n        ) as progress:\n            task = progress.add_task(\"Compiling docs...\", total=len(conversation_dirs))\n\n            for conv_dir in conversation_dirs:\n                conv_id = conv_dir.name\n                try:\n                    compile_conversation_docs(conv_id, atoms_dir, output_dir)\n                except Exception as e:\n                    logger.exception(\n                        \"Error compiling conversation\",\n                        extra={\n                            \"event\": \"compile.conversation.error\",\n                            \"conversation_id\": conv_id,\n                        },\n                    )\n\n                progress.advance(task)\n    else:\n        # Non-interactive mode or dashboard mode: process without progress bar\n        completed = 0\n        for conv_dir in conversation_dirs:\n            conv_id = conv_dir.name\n            try:\n                compile_conversation_docs(conv_id, atoms_dir, output_dir)\n            except Exception as e:\n                logger.exception(\n                    \"Error compiling conversation\",\n                    extra={\n                        \"event\": \"compile.conversation.error\",\n                        \"conversation_id\": conv_id,\n                    },\n                )\n            completed += 1\n            if progress_cb:\n                progress_cb(completed, len(conversation_dirs), {\"conversation_id\": conv_id})\n\n    logger.info(\n        \"Compilation complete\",\n        extra={\"event\": \"compile.export.complete\"},\n    )",
    "start_line": 207,
    "end_line": 289,
    "has_docstring": true,
    "docstring": "Compile documentation for all conversations.\n\nArgs:\n    atoms_dir: Directory containing conversation atom directories\n    output_dir: Output directory for compiled docs\n    progress_cb: Optional callback(completed, total, context) for progress updates",
    "parameters": [
      "atoms_dir",
      "output_dir",
      "progress_cb"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function compile_docs",
    "component_id": "src.ck_exporter.pipeline.compile.compile_docs"
  },
  "src.ck_exporter.pipeline.consolidate.ConsolidateStats": {
    "id": "src.ck_exporter.pipeline.consolidate.ConsolidateStats",
    "name": "ConsolidateStats",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "class ConsolidateStats:\n    \"\"\"Statistics from consolidation process.\"\"\"\n\n    atoms_in: int = 0\n    atoms_out: int = 0\n    decisions_in: int = 0\n    decisions_out: int = 0\n    questions_in: int = 0\n    questions_out: int = 0",
    "start_line": 19,
    "end_line": 27,
    "has_docstring": true,
    "docstring": "Statistics from consolidation process.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ConsolidateStats",
    "component_id": "src.ck_exporter.pipeline.consolidate.ConsolidateStats"
  },
  "src.ck_exporter.pipeline.consolidate._iter_conversation_dirs": {
    "id": "src.ck_exporter.pipeline.consolidate._iter_conversation_dirs",
    "name": "_iter_conversation_dirs",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "def _iter_conversation_dirs(atoms_dir: Path) -> List[Path]:\n    \"\"\"Get all conversation directories from atoms directory.\"\"\"\n    if not atoms_dir.exists():\n        return []\n    return sorted([p for p in atoms_dir.iterdir() if p.is_dir()], key=lambda p: p.name)",
    "start_line": 30,
    "end_line": 34,
    "has_docstring": true,
    "docstring": "Get all conversation directories from atoms directory.",
    "parameters": [
      "atoms_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _iter_conversation_dirs",
    "component_id": "src.ck_exporter.pipeline.consolidate._iter_conversation_dirs"
  },
  "src.ck_exporter.pipeline.consolidate._read_jsonl": {
    "id": "src.ck_exporter.pipeline.consolidate._read_jsonl",
    "name": "_read_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "def _read_jsonl(path: Path) -> Iterable[Dict[str, Any]]:\n    \"\"\"Read JSONL file and yield each JSON object.\"\"\"\n    if not path.exists():\n        return\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                obj = json.loads(line)\n            except json.JSONDecodeError:\n                continue\n            if isinstance(obj, dict):\n                yield obj",
    "start_line": 37,
    "end_line": 51,
    "has_docstring": true,
    "docstring": "Read JSONL file and yield each JSON object.",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _read_jsonl",
    "component_id": "src.ck_exporter.pipeline.consolidate._read_jsonl"
  },
  "src.ck_exporter.pipeline.consolidate._normalize_evidence_key": {
    "id": "src.ck_exporter.pipeline.consolidate._normalize_evidence_key",
    "name": "_normalize_evidence_key",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "def _normalize_evidence_key(ev: Any) -> Optional[Tuple[str, str, str]]:\n    \"\"\"Normalize evidence dict to a stable key for deduplication.\"\"\"\n    if not isinstance(ev, dict):\n        return None\n    return (\n        str(ev.get(\"conversation_id\") or \"\"),\n        str(ev.get(\"message_id\") or \"\"),\n        str(ev.get(\"time_iso\") or \"\"),\n    )",
    "start_line": 54,
    "end_line": 62,
    "has_docstring": true,
    "docstring": "Normalize evidence dict to a stable key for deduplication.",
    "parameters": [
      "ev"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _normalize_evidence_key",
    "component_id": "src.ck_exporter.pipeline.consolidate._normalize_evidence_key"
  },
  "src.ck_exporter.pipeline.consolidate._merge_evidence": {
    "id": "src.ck_exporter.pipeline.consolidate._merge_evidence",
    "name": "_merge_evidence",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "def _merge_evidence(\n    existing: List[Dict[str, Any]],\n    incoming: List[Dict[str, Any]],\n    conversation_id: str,\n) -> List[Dict[str, Any]]:\n    \"\"\"Merge evidence arrays, deduplicating by (conversation_id, message_id, time_iso).\"\"\"\n    merged: List[Dict[str, Any]] = []\n    seen = set()\n\n    def add_evidence(ev: Any, fallback_conv_id: str) -> None:\n        \"\"\"Add evidence if not already seen.\"\"\"\n        if not isinstance(ev, dict):\n            return\n        # Ensure conversation_id is present\n        if \"conversation_id\" not in ev or not ev.get(\"conversation_id\"):\n            ev = {**ev, \"conversation_id\": fallback_conv_id}\n        key = _normalize_evidence_key(ev)\n        if key is None or key in seen:\n            return\n        seen.add(key)\n        merged.append(ev)\n\n    # Add existing evidence\n    for ev in existing:\n        add_evidence(ev, conversation_id)\n\n    # Add incoming evidence\n    for ev in incoming:\n        add_evidence(ev, conversation_id)\n\n    return merged",
    "start_line": 65,
    "end_line": 95,
    "has_docstring": true,
    "docstring": "Merge evidence arrays, deduplicating by (conversation_id, message_id, time_iso).",
    "parameters": [
      "existing",
      "incoming",
      "conversation_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _merge_evidence",
    "component_id": "src.ck_exporter.pipeline.consolidate._merge_evidence"
  },
  "src.ck_exporter.pipeline.consolidate.add_evidence": {
    "id": "src.ck_exporter.pipeline.consolidate.add_evidence",
    "name": "add_evidence",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [
      "src.ck_exporter.pipeline.consolidate._normalize_evidence_key"
    ],
    "source_code": "    def add_evidence(ev: Any, fallback_conv_id: str) -> None:\n        \"\"\"Add evidence if not already seen.\"\"\"\n        if not isinstance(ev, dict):\n            return\n        # Ensure conversation_id is present\n        if \"conversation_id\" not in ev or not ev.get(\"conversation_id\"):\n            ev = {**ev, \"conversation_id\": fallback_conv_id}\n        key = _normalize_evidence_key(ev)\n        if key is None or key in seen:\n            return\n        seen.add(key)\n        merged.append(ev)",
    "start_line": 74,
    "end_line": 85,
    "has_docstring": true,
    "docstring": "Add evidence if not already seen.",
    "parameters": [
      "ev",
      "fallback_conv_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function add_evidence",
    "component_id": "src.ck_exporter.pipeline.consolidate.add_evidence"
  },
  "src.ck_exporter.pipeline.consolidate._write_jsonl": {
    "id": "src.ck_exporter.pipeline.consolidate._write_jsonl",
    "name": "_write_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "def _write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n    \"\"\"Write list of dicts to JSONL file.\"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        for row in rows:\n            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")",
    "start_line": 98,
    "end_line": 103,
    "has_docstring": true,
    "docstring": "Write list of dicts to JSONL file.",
    "parameters": [
      "path",
      "rows"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _write_jsonl",
    "component_id": "src.ck_exporter.pipeline.consolidate._write_jsonl"
  },
  "src.ck_exporter.pipeline.consolidate._concat_markdown_files": {
    "id": "src.ck_exporter.pipeline.consolidate._concat_markdown_files",
    "name": "_concat_markdown_files",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [],
    "source_code": "def _concat_markdown_files(md_files: List[Path], out_path: Path) -> None:\n    \"\"\"Concatenate multiple markdown files into one.\"\"\"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    parts: List[str] = []\n    for p in sorted(md_files):\n        try:\n            content = p.read_text(encoding=\"utf-8\")\n        except Exception:\n            continue\n        parts.append(f\"\\n\\n---\\n\\n<!-- SOURCE_FILE: {p.as_posix()} -->\\n\\n{content}\\n\")\n    out_path.write_text(\"\".join(parts).lstrip(), encoding=\"utf-8\")",
    "start_line": 106,
    "end_line": 116,
    "has_docstring": true,
    "docstring": "Concatenate multiple markdown files into one.",
    "parameters": [
      "md_files",
      "out_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _concat_markdown_files",
    "component_id": "src.ck_exporter.pipeline.consolidate._concat_markdown_files"
  },
  "src.ck_exporter.pipeline.consolidate.consolidate_project": {
    "id": "src.ck_exporter.pipeline.consolidate.consolidate_project",
    "name": "consolidate_project",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/consolidate.py",
    "relative_path": "src/ck_exporter/pipeline/consolidate.py",
    "depends_on": [
      "src.ck_exporter.pipeline.consolidate._concat_markdown_files",
      "src.ck_exporter.pipeline.consolidate._read_jsonl",
      "src.ck_exporter.pipeline.consolidate._write_jsonl",
      "src.ck_exporter.pipeline.consolidate._merge_evidence",
      "src.ck_exporter.pipeline.consolidate._iter_conversation_dirs",
      "src.ck_exporter.pipeline.consolidate.ConsolidateStats"
    ],
    "source_code": "def consolidate_project(\n    atoms_dir: Path,\n    docs_dir: Path,\n    out_dir: Path,\n    include_docs: bool = True,\n) -> ConsolidateStats:\n    \"\"\"\n    Consolidate per-conversation outputs into project-wide knowledge packet.\n\n    Args:\n        atoms_dir: Directory containing per-conversation atom JSONL files\n        docs_dir: Directory containing per-conversation markdown docs\n        out_dir: Output directory for consolidated files\n        include_docs: Whether to concatenate markdown docs\n\n    Returns:\n        Statistics about the consolidation process\n    \"\"\"\n    stats = ConsolidateStats()\n\n    # Deduplication maps: key -> consolidated object\n    atoms_by_key: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n    decisions_by_key: Dict[Tuple[str, str, str], Dict[str, Any]] = {}\n    questions_by_key: Dict[Tuple[str, str], Dict[str, Any]] = {}\n\n    conversation_dirs = _iter_conversation_dirs(atoms_dir)\n    logger.info(\n        \"Processing conversations\",\n        extra={\n            \"event\": \"consolidate.start\",\n            \"num_conversations\": len(conversation_dirs),\n        },\n    )\n\n    for conv_dir in conversation_dirs:\n        conv_id = conv_dir.name\n\n        # Process atoms (facts.jsonl)\n        for fn in ATOM_FILES:\n            for atom in _read_jsonl(conv_dir / fn):\n                stats.atoms_in += 1\n                # Dedupe key: (type, topic, statement)\n                key = (\n                    str(atom.get(\"type\") or \"\"),\n                    str(atom.get(\"topic\") or \"\"),\n                    str(atom.get(\"statement\") or \"\"),\n                )\n                if key in atoms_by_key:\n                    # Merge evidence\n                    existing_ev = atoms_by_key[key].get(\"evidence\", [])\n                    incoming_ev = atom.get(\"evidence\", [])\n                    atoms_by_key[key][\"evidence\"] = _merge_evidence(\n                        existing_ev, incoming_ev, conv_id\n                    )\n                else:\n                    # New atom, add source conversation ID\n                    atom = dict(atom)\n                    atom[\"source_conversation_id\"] = conv_id\n                    # Ensure evidence has conversation_id\n                    ev_list = atom.get(\"evidence\", [])\n                    for ev in ev_list:\n                        if isinstance(ev, dict) and not ev.get(\"conversation_id\"):\n                            ev[\"conversation_id\"] = conv_id\n                    atoms_by_key[key] = atom\n\n        # Process decisions (decisions.jsonl)\n        for fn in DECISION_FILES:\n            for d in _read_jsonl(conv_dir / fn):\n                stats.decisions_in += 1\n                # Dedupe key: (type, topic, statement)\n                key = (\n                    str(d.get(\"type\") or \"decision\"),\n                    str(d.get(\"topic\") or \"\"),\n                    str(d.get(\"statement\") or \"\"),\n                )\n                if key in decisions_by_key:\n                    # Merge evidence\n                    existing_ev = decisions_by_key[key].get(\"evidence\", [])\n                    incoming_ev = d.get(\"evidence\", [])\n                    decisions_by_key[key][\"evidence\"] = _merge_evidence(\n                        existing_ev, incoming_ev, conv_id\n                    )\n                else:\n                    # New decision, add source conversation ID\n                    d = dict(d)\n                    d[\"source_conversation_id\"] = conv_id\n                    # Ensure evidence has conversation_id\n                    ev_list = d.get(\"evidence\", [])\n                    for ev in ev_list:\n                        if isinstance(ev, dict) and not ev.get(\"conversation_id\"):\n                            ev[\"conversation_id\"] = conv_id\n                    decisions_by_key[key] = d\n\n        # Process open questions (open_questions.jsonl)\n        for fn in QUESTION_FILES:\n            for q in _read_jsonl(conv_dir / fn):\n                stats.questions_in += 1\n                # Dedupe key: (topic, question)\n                key = (\n                    str(q.get(\"topic\") or \"\"),\n                    str(q.get(\"question\") or \"\"),\n                )\n                if key in questions_by_key:\n                    # Merge evidence\n                    existing_ev = questions_by_key[key].get(\"evidence\", [])\n                    incoming_ev = q.get(\"evidence\", [])\n                    questions_by_key[key][\"evidence\"] = _merge_evidence(\n                        existing_ev, incoming_ev, conv_id\n                    )\n                else:\n                    # New question, add source conversation ID\n                    q = dict(q)\n                    q[\"source_conversation_id\"] = conv_id\n                    # Ensure evidence has conversation_id\n                    ev_list = q.get(\"evidence\", [])\n                    for ev in ev_list:\n                        if isinstance(ev, dict) and not ev.get(\"conversation_id\"):\n                            ev[\"conversation_id\"] = conv_id\n                    questions_by_key[key] = q\n\n    # Convert to lists\n    atoms_out = list(atoms_by_key.values())\n    decisions_out = list(decisions_by_key.values())\n    questions_out = list(questions_by_key.values())\n\n    stats.atoms_out = len(atoms_out)\n    stats.decisions_out = len(decisions_out)\n    stats.questions_out = len(questions_out)\n\n    # Write consolidated JSONL files\n    project_dir = out_dir / \"project\"\n    logger.info(\n        \"Writing consolidated files\",\n        extra={\n            \"event\": \"consolidate.write\",\n            \"project_dir\": str(project_dir),\n            \"atoms_out\": stats.atoms_out,\n            \"decisions_out\": stats.decisions_out,\n            \"questions_out\": stats.questions_out,\n        },\n    )\n\n    _write_jsonl(project_dir / \"atoms.jsonl\", atoms_out)\n    _write_jsonl(project_dir / \"decisions.jsonl\", decisions_out)\n    _write_jsonl(project_dir / \"open_questions.jsonl\", questions_out)\n\n    # Write manifest\n    manifest_lines = [\n        \"# Project Knowledge Manifest\",\n        \"\",\n        \"## Statistics\",\n        \"\",\n        f\"- **Atoms**: {stats.atoms_in} input  {stats.atoms_out} output (deduped)\",\n        f\"- **Decisions**: {stats.decisions_in} input  {stats.decisions_out} output (deduped)\",\n        f\"- **Open Questions**: {stats.questions_in} input  {stats.questions_out} output (deduped)\",\n        \"\",\n        \"## Files\",\n        \"\",\n        \"- `atoms.jsonl` - Consolidated knowledge atoms\",\n        \"- `decisions.jsonl` - Consolidated decisions/ADRs\",\n        \"- `open_questions.jsonl` - Consolidated open questions\",\n    ]\n\n    if include_docs:\n        manifest_lines.extend([\n            \"- `docs_concat.md` - Concatenated non-ADR documentation\",\n            \"- `adrs_concat.md` - Concatenated ADR files\",\n        ])\n\n    manifest_lines.append(\"\")\n\n    (project_dir / \"manifest.md\").write_text(\"\\n\".join(manifest_lines), encoding=\"utf-8\")\n\n    # Optionally concatenate markdown docs\n    if include_docs and docs_dir.exists():\n        # Find all non-ADR markdown files\n        md_files = sorted([\n            p\n            for p in docs_dir.rglob(\"*.md\")\n            if \"decisions\" not in p.as_posix().replace(\"\\\\\", \"/\")\n        ])\n\n        # Find all ADR files\n        adr_files = sorted([\n            p\n            for p in (docs_dir / \"decisions\").rglob(\"*.md\")\n        ]) if (docs_dir / \"decisions\").exists() else []\n\n        if md_files:\n            logger.debug(\n                \"Concatenating markdown docs\",\n                extra={\n                    \"event\": \"consolidate.concat.docs\",\n                    \"num_files\": len(md_files),\n                },\n            )\n            _concat_markdown_files(md_files, project_dir / \"docs_concat.md\")\n\n        if adr_files:\n            logger.debug(\n                \"Concatenating ADR files\",\n                extra={\n                    \"event\": \"consolidate.concat.adrs\",\n                    \"num_files\": len(adr_files),\n                },\n            )\n            _concat_markdown_files(adr_files, project_dir / \"adrs_concat.md\")\n\n    return stats",
    "start_line": 119,
    "end_line": 327,
    "has_docstring": true,
    "docstring": "Consolidate per-conversation outputs into project-wide knowledge packet.\n\nArgs:\n    atoms_dir: Directory containing per-conversation atom JSONL files\n    docs_dir: Directory containing per-conversation markdown docs\n    out_dir: Output directory for consolidated files\n    include_docs: Whether to concatenate markdown docs\n\nReturns:\n    Statistics about the consolidation process",
    "parameters": [
      "atoms_dir",
      "docs_dir",
      "out_dir",
      "include_docs"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function consolidate_project",
    "component_id": "src.ck_exporter.pipeline.consolidate.consolidate_project"
  },
  "src.ck_exporter.pipeline.extract.format_chunk_for_extraction": {
    "id": "src.ck_exporter.pipeline.extract.format_chunk_for_extraction",
    "name": "format_chunk_for_extraction",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/extract.py",
    "relative_path": "src/ck_exporter/pipeline/extract.py",
    "depends_on": [],
    "source_code": "def format_chunk_for_extraction(messages: list[dict[str, Any]]) -> str:\n    \"\"\"Format a message chunk as text for extraction.\"\"\"\n    lines = []\n    for msg in messages:\n        role = msg.get(\"role\", \"unknown\")\n        text = msg.get(\"text\", \"\")\n        time_iso = msg.get(\"time_iso\", \"\")\n        msg_id = msg.get(\"id\", \"\")\n\n        lines.append(f\"[{role.upper()}] {time_iso} (ID: {msg_id})\")\n        lines.append(text)\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)",
    "start_line": 23,
    "end_line": 36,
    "has_docstring": true,
    "docstring": "Format a message chunk as text for extraction.",
    "parameters": [
      "messages"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function format_chunk_for_extraction",
    "component_id": "src.ck_exporter.pipeline.extract.format_chunk_for_extraction"
  },
  "src.ck_exporter.pipeline.extract._conversation_outputs_exist": {
    "id": "src.ck_exporter.pipeline.extract._conversation_outputs_exist",
    "name": "_conversation_outputs_exist",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/extract.py",
    "relative_path": "src/ck_exporter/pipeline/extract.py",
    "depends_on": [],
    "source_code": "def _conversation_outputs_exist(conv_id: str, atoms_dir: Path) -> bool:\n    \"\"\"Check if final outputs already exist for a conversation.\"\"\"\n    facts_path = atoms_dir / conv_id / \"facts.jsonl\"\n    # Consider it complete if at least facts.jsonl exists and is non-empty\n    return facts_path.exists() and facts_path.stat().st_size > 0",
    "start_line": 39,
    "end_line": 43,
    "has_docstring": true,
    "docstring": "Check if final outputs already exist for a conversation.",
    "parameters": [
      "conv_id",
      "atoms_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _conversation_outputs_exist",
    "component_id": "src.ck_exporter.pipeline.extract._conversation_outputs_exist"
  },
  "src.ck_exporter.pipeline.extract.extract_conversation": {
    "id": "src.ck_exporter.pipeline.extract.extract_conversation",
    "name": "extract_conversation",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/extract.py",
    "relative_path": "src/ck_exporter/pipeline/extract.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.schema_helpers.get_conversation_id",
      "src.ck_exporter.pipeline.extract.format_chunk_for_extraction",
      "src.ck_exporter.utils.chunking.chunk_messages",
      "src.ck_exporter.pipeline.linearize.linearize_conversation",
      "src.ck_exporter.logging.with_context",
      "src.ck_exporter.pipeline.extract._conversation_outputs_exist"
    ],
    "source_code": "def extract_conversation(\n    conversation: dict[str, Any],\n    evidence_dir: Path,\n    atoms_dir: Path,\n    extractor: AtomExtractor,\n    max_chunk_tokens: int = 8000,\n    skip_existing: bool = True,\n    chunk_max_concurrency: int | None = None,\n) -> None:\n    \"\"\"\n    Extract atoms from a single conversation using two-pass pipeline.\n\n    Args:\n        conversation: Conversation dict\n        evidence_dir: Directory with linearized markdown evidence\n        atoms_dir: Output directory for atoms JSONL files\n        extractor: Atom extractor implementation\n        max_chunk_tokens: Maximum tokens per chunk\n        skip_existing: Skip if outputs already exist\n    \"\"\"\n    conv_id = get_conversation_id(conversation)\n    if not conv_id:\n        logger.warning(\n            \"Skipping conversation without ID\",\n            extra={\"event\": \"extract.conversation.skipped\", \"reason\": \"no_id\"},\n        )\n        return\n\n    conv_logger = with_context(logger, conversation_id=conv_id)\n\n    # Check if outputs already exist\n    if skip_existing and _conversation_outputs_exist(conv_id, atoms_dir):\n        conv_logger.debug(\n            \"Skipping conversation (outputs already exist)\",\n            extra={\"event\": \"extract.conversation.skipped\", \"reason\": \"exists\"},\n        )\n        return\n\n    # Load linearized messages\n    evidence_path = evidence_dir / conv_id / \"conversation.md\"\n    if not evidence_path.exists():\n        conv_logger.info(\n            \"No evidence found, linearizing\",\n            extra={\"event\": \"extract.linearize.triggered\"},\n        )\n        messages = linearize_conversation(conversation)\n    else:\n        # Re-linearize to get message structure\n        messages = linearize_conversation(conversation)\n\n    if not messages:\n        conv_logger.warning(\n            \"No messages found\",\n            extra={\"event\": \"extract.conversation.skipped\", \"reason\": \"no_messages\"},\n        )\n        return\n\n    # Chunk messages\n    chunks = chunk_messages(messages, max_tokens=max_chunk_tokens, model=\"gpt-4\")\n\n    conv_logger.info(\n        \"Processing conversation\",\n        extra={\n            \"event\": \"extract.conversation.start\",\n            \"num_chunks\": len(chunks),\n            \"num_messages\": len(messages),\n        },\n    )\n\n    # Pass 1: Extract candidates from each chunk (parallelized)\n    all_candidates = {\"facts\": [], \"decisions\": [], \"open_questions\": []}\n\n    # Determine chunk concurrency (default: use env var or conservative default)\n    if chunk_max_concurrency is None:\n        chunk_max_concurrency = int(os.getenv(\"CKX_CHUNK_MAX_CONCURRENCY\", \"4\"))\n\n    # For single chunk or very few chunks, use sequential processing\n    if len(chunks) <= 1 or chunk_max_concurrency <= 1:\n        for i, chunk in enumerate(chunks, 1):\n            chunk_text = format_chunk_for_extraction(chunk)\n            result = extractor.extract_from_chunk(chunk_text)\n\n            # Collect candidates\n            num_facts = len(result.get(\"facts\", []))\n            num_decisions = len(result.get(\"decisions\", []))\n            num_questions = len(result.get(\"open_questions\", []))\n            all_candidates[\"facts\"].extend(result.get(\"facts\", []))\n            all_candidates[\"decisions\"].extend(result.get(\"decisions\", []))\n            all_candidates[\"open_questions\"].extend(result.get(\"open_questions\", []))\n\n            conv_logger.debug(\n                \"Pass 1 chunk extracted\",\n                extra={\n                    \"event\": \"extract.pass1.chunk\",\n                    \"chunk_num\": i,\n                    \"total_chunks\": len(chunks),\n                    \"facts\": num_facts,\n                    \"decisions\": num_decisions,\n                    \"questions\": num_questions,\n                },\n            )\n    else:\n        # Parallel chunk extraction with deterministic ordering\n        chunk_results: list[tuple[int, dict[str, Any]]] = []\n\n        def extract_chunk(idx_and_chunk: tuple[int, list[dict[str, Any]]]) -> tuple[int, dict[str, Any]]:\n            \"\"\"Extract from a single chunk, returning (index, result) for ordering.\"\"\"\n            idx, chunk = idx_and_chunk\n            chunk_text = format_chunk_for_extraction(chunk)\n            result = extractor.extract_from_chunk(chunk_text)\n            return (idx, result)\n\n        with ThreadPoolExecutor(max_workers=chunk_max_concurrency) as executor:\n            # Submit all chunks with their indices\n            futures = {\n                executor.submit(extract_chunk, (i, chunk)): i\n                for i, chunk in enumerate(chunks)\n            }\n\n            # Collect results as they complete\n            for future in as_completed(futures):\n                try:\n                    idx, result = future.result()\n                    chunk_results.append((idx, result))\n\n                    num_facts = len(result.get(\"facts\", []))\n                    num_decisions = len(result.get(\"decisions\", []))\n                    num_questions = len(result.get(\"open_questions\", []))\n\n                    conv_logger.debug(\n                        \"Pass 1 chunk extracted\",\n                        extra={\n                            \"event\": \"extract.pass1.chunk\",\n                            \"chunk_num\": idx + 1,\n                            \"total_chunks\": len(chunks),\n                            \"facts\": num_facts,\n                            \"decisions\": num_decisions,\n                            \"questions\": num_questions,\n                        },\n                    )\n                except Exception as e:\n                    chunk_idx = futures[future]\n                    conv_logger.exception(\n                        \"Error extracting chunk\",\n                        extra={\n                            \"event\": \"extract.pass1.chunk.error\",\n                            \"chunk_num\": chunk_idx + 1,\n                            \"total_chunks\": len(chunks),\n                        },\n                    )\n\n        # Sort by index to maintain deterministic order, then aggregate\n        chunk_results.sort(key=lambda x: x[0])\n        for idx, result in chunk_results:\n            all_candidates[\"facts\"].extend(result.get(\"facts\", []))\n            all_candidates[\"decisions\"].extend(result.get(\"decisions\", []))\n            all_candidates[\"open_questions\"].extend(result.get(\"open_questions\", []))\n\n    # Pass 2: Refine and consolidate all candidates\n    conv_logger.info(\n        \"Pass 2: Refining candidates\",\n        extra={\n            \"event\": \"extract.pass2.start\",\n            \"candidate_facts\": len(all_candidates[\"facts\"]),\n            \"candidate_decisions\": len(all_candidates[\"decisions\"]),\n            \"candidate_questions\": len(all_candidates[\"open_questions\"]),\n        },\n    )\n    conversation_title = get_title(conversation)\n    refined = extractor.refine_atoms(all_candidates, conv_id, conversation_title)\n\n    final_facts = refined.get(\"facts\", [])\n    final_decisions = refined.get(\"decisions\", [])\n    final_questions = refined.get(\"open_questions\", [])\n\n    conv_logger.info(\n        \"Pass 2: Refinement complete\",\n        extra={\n            \"event\": \"extract.pass2.complete\",\n            \"final_facts\": len(final_facts),\n            \"final_decisions\": len(final_decisions),\n            \"final_questions\": len(final_questions),\n        },\n    )\n\n    # Write final JSONL files\n    facts_path = atoms_dir / conv_id / \"facts.jsonl\"\n    decisions_path = atoms_dir / conv_id / \"decisions.jsonl\"\n    questions_path = atoms_dir / conv_id / \"open_questions.jsonl\"\n\n    if final_facts:\n        write_jsonl(facts_path, final_facts)\n    if final_decisions:\n        write_jsonl(decisions_path, final_decisions)\n    if final_questions:\n        write_jsonl(questions_path, final_questions)\n\n    conv_logger.info(\n        \"Extraction complete\",\n        extra={\n            \"event\": \"extract.conversation.complete\",\n            \"facts\": len(final_facts),\n            \"decisions\": len(final_decisions),\n            \"questions\": len(final_questions),\n        },\n    )",
    "start_line": 46,
    "end_line": 251,
    "has_docstring": true,
    "docstring": "Extract atoms from a single conversation using two-pass pipeline.\n\nArgs:\n    conversation: Conversation dict\n    evidence_dir: Directory with linearized markdown evidence\n    atoms_dir: Output directory for atoms JSONL files\n    extractor: Atom extractor implementation\n    max_chunk_tokens: Maximum tokens per chunk\n    skip_existing: Skip if outputs already exist",
    "parameters": [
      "conversation",
      "evidence_dir",
      "atoms_dir",
      "extractor",
      "max_chunk_tokens",
      "skip_existing",
      "chunk_max_concurrency"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_conversation",
    "component_id": "src.ck_exporter.pipeline.extract.extract_conversation"
  },
  "src.ck_exporter.pipeline.extract.extract_chunk": {
    "id": "src.ck_exporter.pipeline.extract.extract_chunk",
    "name": "extract_chunk",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/extract.py",
    "relative_path": "src/ck_exporter/pipeline/extract.py",
    "depends_on": [
      "src.ck_exporter.pipeline.extract.format_chunk_for_extraction"
    ],
    "source_code": "        def extract_chunk(idx_and_chunk: tuple[int, list[dict[str, Any]]]) -> tuple[int, dict[str, Any]]:\n            \"\"\"Extract from a single chunk, returning (index, result) for ordering.\"\"\"\n            idx, chunk = idx_and_chunk\n            chunk_text = format_chunk_for_extraction(chunk)\n            result = extractor.extract_from_chunk(chunk_text)\n            return (idx, result)",
    "start_line": 151,
    "end_line": 156,
    "has_docstring": true,
    "docstring": "Extract from a single chunk, returning (index, result) for ordering.",
    "parameters": [
      "idx_and_chunk"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_chunk",
    "component_id": "src.ck_exporter.pipeline.extract.extract_chunk"
  },
  "src.ck_exporter.pipeline.extract.extract_export": {
    "id": "src.ck_exporter.pipeline.extract.extract_export",
    "name": "extract_export",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/extract.py",
    "relative_path": "src/ck_exporter/pipeline/extract.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.schema_helpers.get_conversation_id",
      "src.ck_exporter.pipeline.io.load.load_conversations",
      "src.ck_exporter.logging.should_show_progress",
      "src.ck_exporter.pipeline.extract._conversation_outputs_exist"
    ],
    "source_code": "def extract_export(\n    input_path: Path,\n    evidence_dir: Path,\n    atoms_dir: Path,\n    extractor: AtomExtractor,\n    max_concurrency: int = 8,\n    skip_existing: bool = True,\n    conversation_id: Optional[str] = None,\n    limit: Optional[int] = None,\n    progress_cb: Optional[Callable[[int, int, Optional[dict]], None]] = None,\n) -> None:\n    \"\"\"\n    Process export(s) and extract atoms for all conversations.\n\n    Args:\n        input_path: Path to conversation export JSON or directory of per-conversation JSON files\n        evidence_dir: Directory with linearized markdown evidence\n        atoms_dir: Output directory for atoms JSONL files\n        extractor: Atom extractor implementation\n        max_concurrency: Maximum concurrent conversations\n        skip_existing: Skip conversations with existing outputs\n        conversation_id: Optional filter to single conversation ID\n        limit: Optional limit on number of conversations to process\n        progress_cb: Optional callback(completed, total, context) for progress updates\n    \"\"\"\n    logger.info(\n        \"Loading export\",\n        extra={\n            \"event\": \"extract.export.load\",\n            \"input_path\": str(input_path),\n            \"limit\": limit,\n        },\n    )\n\n    conversations = load_conversations(input_path, limit=limit)\n\n    # Filter to single conversation if requested\n    if conversation_id:\n        conversations = [\n            conv for conv in conversations if get_conversation_id(conv) == conversation_id\n        ]\n        if not conversations:\n            logger.error(\n                \"Conversation not found in export\",\n                extra={\n                    \"event\": \"extract.export.error\",\n                    \"conversation_id\": conversation_id,\n                },\n            )\n            raise typer.Exit(1)\n        logger.info(\n            \"Filtering to single conversation\",\n            extra={\n                \"event\": \"extract.export.filter\",\n                \"conversation_id\": conversation_id,\n            },\n        )\n\n    # Filter out conversations that already have outputs (if skip_existing)\n    if skip_existing:\n        original_count = len(conversations)\n        conversations = [\n            conv\n            for conv in conversations\n            if not _conversation_outputs_exist(get_conversation_id(conv) or \"\", atoms_dir)\n        ]\n        skipped_count = original_count - len(conversations)\n        if skipped_count > 0:\n            logger.debug(\n                \"Skipped conversations with existing outputs\",\n                extra={\n                    \"event\": \"extract.export.skipped\",\n                    \"skipped_count\": skipped_count,\n                },\n            )\n\n    if not conversations:\n        logger.warning(\n            \"No conversations to process\",\n            extra={\"event\": \"extract.export.empty\"},\n        )\n        return\n\n    logger.info(\n        \"Processing conversations\",\n        extra={\n            \"event\": \"extract.export.start\",\n            \"num_conversations\": len(conversations),\n            \"max_concurrency\": max_concurrency,\n        },\n    )\n\n    # Notify progress callback of total\n    if progress_cb:\n        progress_cb(0, len(conversations), {})\n\n    # Process conversations with bounded concurrency\n    with ThreadPoolExecutor(max_workers=max_concurrency) as executor:\n        futures = {}\n        for conv in conversations:\n            conv_id = get_conversation_id(conv) or \"unknown\"\n            future = executor.submit(\n                extract_conversation,\n                conv,\n                evidence_dir,\n                atoms_dir,\n                extractor,\n                skip_existing=skip_existing,\n            )\n            futures[future] = conv_id\n\n        # Track completion count for progress callback\n        completed_count = 0\n\n        if should_show_progress():\n            console = Console(stderr=True)\n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                console=console,\n            ) as progress:\n                task = progress.add_task(\n                    f\"Extracting atoms from {len(conversations)} conversation(s)...\",\n                    total=len(conversations),\n                )\n\n                for future in as_completed(futures):\n                    conv_id = futures[future]\n                    try:\n                        future.result()\n                    except Exception as e:\n                        logger.exception(\n                            \"Error processing conversation\",\n                            extra={\n                                \"event\": \"extract.conversation.error\",\n                                \"conversation_id\": conv_id,\n                            },\n                        )\n                    progress.advance(task)\n                    completed_count += 1\n                    if progress_cb:\n                        progress_cb(completed_count, len(conversations), {\"conversation_id\": conv_id})\n        else:\n            # Non-interactive mode or dashboard mode: process without progress bar\n            for future in as_completed(futures):\n                conv_id = futures[future]\n                try:\n                    future.result()\n                except Exception as e:\n                    logger.exception(\n                        \"Error processing conversation\",\n                        extra={\n                            \"event\": \"extract.conversation.error\",\n                            \"conversation_id\": conv_id,\n                        },\n                    )\n                completed_count += 1\n                if progress_cb:\n                    progress_cb(completed_count, len(conversations), {\"conversation_id\": conv_id})\n\n    logger.info(\n        \"Extraction complete\",\n        extra={\"event\": \"extract.export.complete\"},\n    )",
    "start_line": 254,
    "end_line": 417,
    "has_docstring": true,
    "docstring": "Process export(s) and extract atoms for all conversations.\n\nArgs:\n    input_path: Path to conversation export JSON or directory of per-conversation JSON files\n    evidence_dir: Directory with linearized markdown evidence\n    atoms_dir: Output directory for atoms JSONL files\n    extractor: Atom extractor implementation\n    max_concurrency: Maximum concurrent conversations\n    skip_existing: Skip conversations with existing outputs\n    conversation_id: Optional filter to single conversation ID\n    limit: Optional limit on number of conversations to process\n    progress_cb: Optional callback(completed, total, context) for progress updates",
    "parameters": [
      "input_path",
      "evidence_dir",
      "atoms_dir",
      "extractor",
      "max_concurrency",
      "skip_existing",
      "conversation_id",
      "limit",
      "progress_cb"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_export",
    "component_id": "src.ck_exporter.pipeline.extract.extract_export"
  },
  "src.ck_exporter.pipeline.io.load.is_chatgpt_single_conversation": {
    "id": "src.ck_exporter.pipeline.io.load.is_chatgpt_single_conversation",
    "name": "is_chatgpt_single_conversation",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [],
    "source_code": "def is_chatgpt_single_conversation(obj: Any) -> bool:\n    \"\"\"Check if object is a ChatGPT-style single conversation.\"\"\"\n    if not isinstance(obj, dict):\n        return False\n    return \"mapping\" in obj and \"current_node\" in obj",
    "start_line": 16,
    "end_line": 20,
    "has_docstring": true,
    "docstring": "Check if object is a ChatGPT-style single conversation.",
    "parameters": [
      "obj"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_chatgpt_single_conversation",
    "component_id": "src.ck_exporter.pipeline.io.load.is_chatgpt_single_conversation"
  },
  "src.ck_exporter.pipeline.io.load.is_claude_conversation": {
    "id": "src.ck_exporter.pipeline.io.load.is_claude_conversation",
    "name": "is_claude_conversation",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [],
    "source_code": "def is_claude_conversation(obj: Any) -> bool:\n    \"\"\"Check if object is a Claude AI conversation export.\"\"\"\n    if not isinstance(obj, dict):\n        return False\n    return obj.get(\"platform\") == \"CLAUDE_AI\" and isinstance(obj.get(\"chat_messages\"), list)",
    "start_line": 23,
    "end_line": 27,
    "has_docstring": true,
    "docstring": "Check if object is a Claude AI conversation export.",
    "parameters": [
      "obj"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_claude_conversation",
    "component_id": "src.ck_exporter.pipeline.io.load.is_claude_conversation"
  },
  "src.ck_exporter.pipeline.io.load.parse_iso_timestamp": {
    "id": "src.ck_exporter.pipeline.io.load.parse_iso_timestamp",
    "name": "parse_iso_timestamp",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [],
    "source_code": "def parse_iso_timestamp(iso_str: str) -> float | None:\n    \"\"\"Parse ISO timestamp string to epoch seconds float.\"\"\"\n    if not iso_str:\n        return None\n    try:\n        # Handle ISO format with timezone (e.g., \"2025-12-18T18:06:43.449478+00:00\")\n        dt = datetime.fromisoformat(iso_str.replace(\"Z\", \"+00:00\"))\n        return dt.timestamp()\n    except (ValueError, AttributeError):\n        return None",
    "start_line": 30,
    "end_line": 39,
    "has_docstring": true,
    "docstring": "Parse ISO timestamp string to epoch seconds float.",
    "parameters": [
      "iso_str"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_iso_timestamp",
    "component_id": "src.ck_exporter.pipeline.io.load.parse_iso_timestamp"
  },
  "src.ck_exporter.pipeline.io.load.convert_claude_to_chatgpt": {
    "id": "src.ck_exporter.pipeline.io.load.convert_claude_to_chatgpt",
    "name": "convert_claude_to_chatgpt",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.load.parse_iso_timestamp"
    ],
    "source_code": "def convert_claude_to_chatgpt(claude_conv: dict) -> dict:\n    \"\"\"\n    Convert Claude conversation export to ChatGPT-style conversation dict.\n\n    Creates a synthetic mapping/current_node structure that works with existing\n    linearization logic.\n    \"\"\"\n    # Preserve Claude \"project\" metadata when available (used by topic pipeline).\n    # In observed exports this is present as:\n    # - top-level \"project_uuid\"\n    # - and/or nested \"project\": {\"uuid\": \"...\", \"name\": \"...\"}\n    project_id = claude_conv.get(\"project_uuid\")\n    project = claude_conv.get(\"project\")\n    if not project_id and isinstance(project, dict):\n        project_id = project.get(\"uuid\")\n    project_name = project.get(\"name\") if isinstance(project, dict) else None\n\n    chat_messages = claude_conv.get(\"chat_messages\", [])\n    if not chat_messages:\n        # Empty conversation - return minimal structure\n        return {\n            \"conversation_id\": claude_conv.get(\"uuid\", \"unknown\"),\n            \"title\": claude_conv.get(\"name\") or \"Untitled Conversation\",\n            \"project_id\": project_id,\n            \"project_name\": project_name,\n            \"mapping\": {},\n            \"current_node\": None,\n        }\n\n    # Build mapping: one node per message\n    mapping = {}\n    previous_uuid = None\n\n    for msg in chat_messages:\n        msg_uuid = msg.get(\"uuid\")\n        if not msg_uuid:\n            continue  # Skip messages without UUID\n\n        # Map sender to role\n        sender = msg.get(\"sender\", \"\").lower()\n        if sender == \"human\":\n            role = \"user\"\n        elif sender == \"assistant\":\n            role = \"assistant\"\n        else:\n            role = \"system\"  # Unknown sender types -> system\n\n        # Parse timestamp\n        created_at = msg.get(\"created_at\")\n        create_time = parse_iso_timestamp(created_at) if created_at else None\n\n        # Extract text from either 'text' field or 'content' array\n        text = msg.get(\"text\", \"\")\n        if not text and \"content\" in msg:\n            # Newer Claude export format: content is an array of objects with 'text' field\n            content_items = msg.get(\"content\", [])\n            if isinstance(content_items, list):\n                text_parts = [item.get(\"text\", \"\") for item in content_items if isinstance(item, dict)]\n                text = \"\\n\".join(text_parts).strip()\n\n        # Build message node\n        mapping[msg_uuid] = {\n            \"id\": msg_uuid,\n            \"parent\": previous_uuid,\n            \"message\": {\n                \"id\": msg_uuid,\n                \"author\": {\n                    \"role\": role,\n                    \"name\": None,\n                    \"metadata\": {},\n                },\n                \"create_time\": create_time,\n                \"update_time\": None,\n                \"content\": {\n                    \"content_type\": \"text\",\n                    \"parts\": [text],\n                },\n                \"status\": \"finished_successfully\",\n                \"end_turn\": True,\n                \"weight\": 1,\n                \"metadata\": {},\n                \"recipient\": \"all\",\n                \"channel\": None,\n            },\n        }\n\n        previous_uuid = msg_uuid\n\n    # Current node is the last message UUID\n    current_node = previous_uuid if previous_uuid else None\n\n    return {\n        \"conversation_id\": claude_conv.get(\"uuid\", \"unknown\"),\n        \"title\": claude_conv.get(\"name\") or \"Untitled Conversation\",\n        \"project_id\": project_id,\n        \"project_name\": project_name,\n        \"mapping\": mapping,\n        \"current_node\": current_node,\n    }",
    "start_line": 42,
    "end_line": 140,
    "has_docstring": true,
    "docstring": "Convert Claude conversation export to ChatGPT-style conversation dict.\n\nCreates a synthetic mapping/current_node structure that works with existing\nlinearization logic.",
    "parameters": [
      "claude_conv"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function convert_claude_to_chatgpt",
    "component_id": "src.ck_exporter.pipeline.io.load.convert_claude_to_chatgpt"
  },
  "src.ck_exporter.pipeline.io.load._list_json_files": {
    "id": "src.ck_exporter.pipeline.io.load._list_json_files",
    "name": "_list_json_files",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [],
    "source_code": "def _list_json_files(input_dir: Path) -> list[Path]:\n    \"\"\"\n    List JSON files in a directory (recursively), sorted for determinism.\n\n    This is used to support inputs like `chatgpt-conversations/` which contain one\n    JSON file per conversation.\n    \"\"\"\n    # Prefer direct children first (common case), then fall back to recursive search.\n    direct = sorted(p for p in input_dir.glob(\"*.json\") if p.is_file())\n    if direct:\n        return direct\n    return sorted(p for p in input_dir.rglob(\"*.json\") if p.is_file())",
    "start_line": 143,
    "end_line": 154,
    "has_docstring": true,
    "docstring": "List JSON files in a directory (recursively), sorted for determinism.\n\nThis is used to support inputs like `chatgpt-conversations/` which contain one\nJSON file per conversation.",
    "parameters": [
      "input_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _list_json_files",
    "component_id": "src.ck_exporter.pipeline.io.load._list_json_files"
  },
  "src.ck_exporter.pipeline.io.load._load_conversations_file": {
    "id": "src.ck_exporter.pipeline.io.load._load_conversations_file",
    "name": "_load_conversations_file",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.load.is_claude_conversation",
      "src.ck_exporter.pipeline.io.load.convert_claude_to_chatgpt",
      "src.ck_exporter.pipeline.io.load.is_chatgpt_single_conversation"
    ],
    "source_code": "def _load_conversations_file(input_path: Path) -> List[dict]:\n    \"\"\"\n    Load and normalize conversations from a single JSON file.\n\n    Load and normalize conversations from JSON file.\n\n    Supports:\n    - List of conversations (standard ChatGPT export format)\n    - Single ChatGPT conversation dict (with mapping/current_node)\n    - Claude conversation export (with platform=\"CLAUDE_AI\" and chat_messages[])\n\n    For single ChatGPT conversations missing id/conversation_id, injects conversation_id\n    based on filename stem.\n\n    Raises ValueError if input format is not recognized.\n    \"\"\"\n    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    # Case 1: Already a list (standard export format OR list of Claude exports)\n    if isinstance(data, list):\n        normalized: List[dict] = []\n        for i, item in enumerate(data):\n            if not isinstance(item, dict):\n                continue\n\n            # Convert Claude exports even when embedded in a list\n            if is_claude_conversation(item):\n                normalized.append(convert_claude_to_chatgpt(item))\n                continue\n\n            # If it's a single ChatGPT conversation object, ensure it has an ID\n            if is_chatgpt_single_conversation(item):\n                if not item.get(\"id\") and not item.get(\"conversation_id\"):\n                    # Prefer Claude-style uuid if present; otherwise fall back to index-based ID\n                    item[\"conversation_id\"] = item.get(\"uuid\") or f\"{input_path.stem}_{i}\"\n                normalized.append(item)\n                continue\n\n            # Otherwise, include as-is (downstream may skip if no ID)\n            normalized.append(item)\n\n        return normalized\n\n    # Case 2: Claude conversation export\n    if is_claude_conversation(data):\n        converted = convert_claude_to_chatgpt(data)\n        return [converted]\n\n    # Case 3: Single ChatGPT conversation dict\n    if is_chatgpt_single_conversation(data):\n        # Ensure conversation_id exists\n        if not data.get(\"id\") and not data.get(\"conversation_id\"):\n            # Use filename stem as conversation_id\n            conversation_id = input_path.stem\n            data[\"conversation_id\"] = conversation_id\n        return [data]\n\n    # Case 4: Unrecognized format\n    raise ValueError(\n        \"Unsupported input format. Expected one of:\\n\"\n        \"  - A list of conversations (standard ChatGPT export), or\\n\"\n        \"  - A single ChatGPT conversation object with 'mapping' and 'current_node' fields, or\\n\"\n        \"  - A Claude conversation export with 'platform'='CLAUDE_AI' and 'chat_messages' array.\\n\"\n        f\"Got: {type(data).__name__} with keys: {list(data.keys())[:10] if isinstance(data, dict) else 'N/A'}\"\n    )",
    "start_line": 157,
    "end_line": 222,
    "has_docstring": true,
    "docstring": "Load and normalize conversations from a single JSON file.\n\nLoad and normalize conversations from JSON file.\n\nSupports:\n- List of conversations (standard ChatGPT export format)\n- Single ChatGPT conversation dict (with mapping/current_node)\n- Claude conversation export (with platform=\"CLAUDE_AI\" and chat_messages[])\n\nFor single ChatGPT conversations missing id/conversation_id, injects conversation_id\nbased on filename stem.\n\nRaises ValueError if input format is not recognized.",
    "parameters": [
      "input_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _load_conversations_file",
    "component_id": "src.ck_exporter.pipeline.io.load._load_conversations_file"
  },
  "src.ck_exporter.pipeline.io.load.load_conversations": {
    "id": "src.ck_exporter.pipeline.io.load.load_conversations",
    "name": "load_conversations",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/load.py",
    "relative_path": "src/ck_exporter/pipeline/io/load.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.load._load_conversations_file",
      "src.ck_exporter.pipeline.io.load._list_json_files"
    ],
    "source_code": "def load_conversations(input_path: Path, limit: Optional[int] = None) -> List[dict]:\n    \"\"\"\n    Load and normalize conversations from a JSON file OR directory.\n\n    Supports:\n    - List of conversations (standard ChatGPT export format)\n    - Single ChatGPT conversation dict (with mapping/current_node)\n    - Claude conversation export (with platform=\"CLAUDE_AI\" and chat_messages[])\n    - A directory containing many `.json` files, each containing any of the above\n      (common for per-conversation exports like `chatgpt-conversations/`)\n\n    For single ChatGPT conversations missing id/conversation_id, injects conversation_id\n    based on filename stem.\n\n    Args:\n        input_path: Path to JSON file or directory containing JSON files\n        limit: Optional limit on number of conversations to return. For directories,\n               selects first N files by sorted filename (deterministic). For list exports,\n               returns first N conversations after normalization.\n\n    Raises ValueError if input format is not recognized.\n    \"\"\"\n    if input_path.is_dir():\n        json_files = _list_json_files(input_path)\n        if not json_files:\n            raise ValueError(f\"No .json files found in directory: {input_path}\")\n\n        conversations: List[dict] = []\n        for p in json_files:\n            # Stop if we've reached the limit\n            if limit is not None and len(conversations) >= limit:\n                break\n\n            # Be liberal in what we accept: skip files that aren't a supported conversation format.\n            try:\n                file_conversations = _load_conversations_file(p)\n                conversations.extend(file_conversations)\n\n                # If limit is set and we've exceeded it, trim to limit\n                if limit is not None and len(conversations) > limit:\n                    conversations = conversations[:limit]\n                    break\n            except ValueError:\n                continue\n        return conversations\n\n    # For single file inputs, load and optionally limit\n    conversations = _load_conversations_file(input_path)\n    if limit is not None:\n        return conversations[:limit]\n    return conversations",
    "start_line": 225,
    "end_line": 275,
    "has_docstring": true,
    "docstring": "Load and normalize conversations from a JSON file OR directory.\n\nSupports:\n- List of conversations (standard ChatGPT export format)\n- Single ChatGPT conversation dict (with mapping/current_node)\n- Claude conversation export (with platform=\"CLAUDE_AI\" and chat_messages[])\n- A directory containing many `.json` files, each containing any of the above\n  (common for per-conversation exports like `chatgpt-conversations/`)\n\nFor single ChatGPT conversations missing id/conversation_id, injects conversation_id\nbased on filename stem.\n\nArgs:\n    input_path: Path to JSON file or directory containing JSON files\n    limit: Optional limit on number of conversations to return. For directories,\n           selects first N files by sorted filename (deterministic). For list exports,\n           returns first N conversations after normalization.\n\nRaises ValueError if input format is not recognized.",
    "parameters": [
      "input_path",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function load_conversations",
    "component_id": "src.ck_exporter.pipeline.io.load.load_conversations"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_conversation_id": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_conversation_id",
    "name": "get_conversation_id",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_conversation_id(conversation: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Extract conversation ID from a conversation dict.\"\"\"\n    return conversation.get(\"id\") or conversation.get(\"conversation_id\")",
    "start_line": 6,
    "end_line": 8,
    "has_docstring": true,
    "docstring": "Extract conversation ID from a conversation dict.",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_conversation_id",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_conversation_id"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_current_node": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_current_node",
    "name": "get_current_node",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_current_node(conversation: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Get the current node ID from a conversation.\"\"\"\n    return conversation.get(\"current_node\")",
    "start_line": 11,
    "end_line": 13,
    "has_docstring": true,
    "docstring": "Get the current node ID from a conversation.",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_current_node",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_current_node"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_mapping": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_mapping",
    "name": "get_mapping",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_mapping(conversation: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Get the mapping dict from a conversation.\"\"\"\n    return conversation.get(\"mapping\", {})",
    "start_line": 16,
    "end_line": 18,
    "has_docstring": true,
    "docstring": "Get the mapping dict from a conversation.",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_mapping",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_mapping"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_title": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_title",
    "name": "get_title",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_title(conversation: Dict[str, Any]) -> str:\n    \"\"\"Get conversation title, with fallback.\"\"\"\n    return conversation.get(\"title\") or \"Untitled Conversation\"",
    "start_line": 21,
    "end_line": 23,
    "has_docstring": true,
    "docstring": "Get conversation title, with fallback.",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_title",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_title"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_project_id": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_project_id",
    "name": "get_project_id",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_project_id(conversation: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Extract project ID when available (e.g., Claude exports).\"\"\"\n    project_id = conversation.get(\"project_id\") or conversation.get(\"project_uuid\")\n    if project_id:\n        return str(project_id)\n    project = conversation.get(\"project\")\n    if isinstance(project, dict) and project.get(\"uuid\"):\n        return str(project.get(\"uuid\"))\n    return None",
    "start_line": 26,
    "end_line": 34,
    "has_docstring": true,
    "docstring": "Extract project ID when available (e.g., Claude exports).",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_project_id",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_project_id"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_project_name": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_project_name",
    "name": "get_project_name",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_project_name(conversation: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Extract project name when available (e.g., Claude exports).\"\"\"\n    name = conversation.get(\"project_name\")\n    if name:\n        return str(name)\n    project = conversation.get(\"project\")\n    if isinstance(project, dict) and project.get(\"name\"):\n        return str(project.get(\"name\"))\n    return None",
    "start_line": 37,
    "end_line": 45,
    "has_docstring": true,
    "docstring": "Extract project name when available (e.g., Claude exports).",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_project_name",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_project_name"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_message_role": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_role",
    "name": "get_message_role",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_message_role(message: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Extract role from a message dict.\"\"\"\n    author = message.get(\"author\", {})\n    if isinstance(author, dict):\n        return author.get(\"role\")\n    return None",
    "start_line": 48,
    "end_line": 53,
    "has_docstring": true,
    "docstring": "Extract role from a message dict.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_message_role",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_role"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_message_parts": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_parts",
    "name": "get_message_parts",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_message_parts(message: Dict[str, Any]) -> List[str]:\n    \"\"\"Extract text parts from a message content.\"\"\"\n    content = message.get(\"content\", {})\n    if isinstance(content, dict):\n        parts = content.get(\"parts\", [])\n        if isinstance(parts, list):\n            return [p for p in parts if isinstance(p, str)]\n    return []",
    "start_line": 56,
    "end_line": 63,
    "has_docstring": true,
    "docstring": "Extract text parts from a message content.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_message_parts",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_parts"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_message_id": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_id",
    "name": "get_message_id",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_message_id(message: Dict[str, Any]) -> Optional[str]:\n    \"\"\"Extract message ID.\"\"\"\n    return message.get(\"id\")",
    "start_line": 66,
    "end_line": 68,
    "has_docstring": true,
    "docstring": "Extract message ID.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_message_id",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_id"
  },
  "src.ck_exporter.pipeline.io.schema_helpers.get_message_create_time": {
    "id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_create_time",
    "name": "get_message_create_time",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/io/schema_helpers.py",
    "relative_path": "src/ck_exporter/pipeline/io/schema_helpers.py",
    "depends_on": [],
    "source_code": "def get_message_create_time(message: Dict[str, Any]) -> Optional[float]:\n    \"\"\"Extract create_time timestamp.\"\"\"\n    return message.get(\"create_time\")",
    "start_line": 71,
    "end_line": 73,
    "has_docstring": true,
    "docstring": "Extract create_time timestamp.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_message_create_time",
    "component_id": "src.ck_exporter.pipeline.io.schema_helpers.get_message_create_time"
  },
  "src.ck_exporter.pipeline.linearize.linearize_conversation": {
    "id": "src.ck_exporter.pipeline.linearize.linearize_conversation",
    "name": "linearize_conversation",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/linearize.py",
    "relative_path": "src/ck_exporter/pipeline/linearize.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.schema_helpers.get_mapping",
      "src.ck_exporter.pipeline.io.schema_helpers.get_message_id",
      "src.ck_exporter.pipeline.io.schema_helpers.get_current_node",
      "src.ck_exporter.pipeline.io.schema_helpers.get_message_role",
      "src.ck_exporter.pipeline.io.schema_helpers.get_message_parts",
      "src.ck_exporter.pipeline.io.schema_helpers.get_message_create_time"
    ],
    "source_code": "def linearize_conversation(conversation: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Linearize a single conversation by walking the current_node path.\n\n    Returns a list of messages with role, text, timestamps, and IDs.\n    \"\"\"\n    mapping = get_mapping(conversation)\n    current = get_current_node(conversation)\n\n    if not current:\n        return []\n\n    # Walk parent chain to root\n    path = []\n    node_id = current\n    visited = set()\n\n    while node_id and node_id not in visited:\n        visited.add(node_id)\n        path.append(node_id)\n        node = mapping.get(node_id, {})\n        node_id = node.get(\"parent\")\n\n    # Reverse to get chronological order\n    path.reverse()\n\n    # Extract messages\n    messages = []\n    for node_id in path:\n        node = mapping.get(node_id, {})\n        message = node.get(\"message\")\n        if not message:\n            continue\n\n        role = get_message_role(message)\n        parts = get_message_parts(message)\n        text = \"\\n\".join(parts).strip()\n\n        if not text or not role:\n            continue\n\n        create_time = get_message_create_time(message)\n        message_id = get_message_id(message)\n\n        messages.append({\n            \"id\": message_id,\n            \"role\": role,\n            \"create_time\": create_time,\n            \"time_iso\": datetime.fromtimestamp(create_time).isoformat() if create_time else None,\n            \"text\": text,\n        })\n\n    return messages",
    "start_line": 27,
    "end_line": 79,
    "has_docstring": true,
    "docstring": "Linearize a single conversation by walking the current_node path.\n\nReturns a list of messages with role, text, timestamps, and IDs.",
    "parameters": [
      "conversation"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function linearize_conversation",
    "component_id": "src.ck_exporter.pipeline.linearize.linearize_conversation"
  },
  "src.ck_exporter.pipeline.linearize.write_conversation_markdown": {
    "id": "src.ck_exporter.pipeline.linearize.write_conversation_markdown",
    "name": "write_conversation_markdown",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/linearize.py",
    "relative_path": "src/ck_exporter/pipeline/linearize.py",
    "depends_on": [],
    "source_code": "def write_conversation_markdown(\n    messages: List[Dict[str, Any]],\n    conversation_id: str,\n    title: str,\n    output_dir: Path,\n    project_id: Optional[str] = None,\n    project_name: Optional[str] = None,\n) -> Path:\n    \"\"\"Write linearized messages to markdown file.\"\"\"\n    output_path = output_dir / conversation_id / \"conversation.md\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"# {title}\\n\\n\")\n        f.write(f\"Conversation ID: `{conversation_id}`\\n\\n\")\n        if project_name and project_id:\n            f.write(f\"Project: **{project_name}** (`{project_id}`)\\n\\n\")\n        elif project_name:\n            f.write(f\"Project: **{project_name}**\\n\\n\")\n        elif project_id:\n            f.write(f\"Project ID: `{project_id}`\\n\\n\")\n        f.write(\"---\\n\\n\")\n\n        for msg in messages:\n            role = msg[\"role\"]\n            text = msg[\"text\"]\n            time_iso = msg.get(\"time_iso\", \"\")\n            msg_id = msg.get(\"id\", \"\")\n\n            f.write(f\"## {role.title()}\\n\\n\")\n            if time_iso:\n                f.write(f\"**Time**: {time_iso}\\n\\n\")\n            if msg_id:\n                f.write(f\"**Message ID**: `{msg_id}`\\n\\n\")\n            f.write(f\"{text}\\n\\n\")\n            f.write(\"---\\n\\n\")\n\n    return output_path",
    "start_line": 82,
    "end_line": 119,
    "has_docstring": true,
    "docstring": "Write linearized messages to markdown file.",
    "parameters": [
      "messages",
      "conversation_id",
      "title",
      "output_dir",
      "project_id",
      "project_name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function write_conversation_markdown",
    "component_id": "src.ck_exporter.pipeline.linearize.write_conversation_markdown"
  },
  "src.ck_exporter.pipeline.linearize.linearize_export": {
    "id": "src.ck_exporter.pipeline.linearize.linearize_export",
    "name": "linearize_export",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/linearize.py",
    "relative_path": "src/ck_exporter/pipeline/linearize.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.schema_helpers.get_title",
      "src.ck_exporter.pipeline.io.load.load_conversations",
      "src.ck_exporter.pipeline.io.schema_helpers.get_project_id",
      "src.ck_exporter.pipeline.io.schema_helpers.get_project_name",
      "src.ck_exporter.logging.should_show_progress",
      "src.ck_exporter.pipeline.linearize.linearize_conversation",
      "src.ck_exporter.logging.with_context",
      "src.ck_exporter.pipeline.linearize.write_conversation_markdown"
    ],
    "source_code": "def linearize_export(\n    input_path: Path,\n    output_dir: Path,\n    limit: Optional[int] = None,\n    progress_cb: Optional[Callable[[int, int, Optional[dict]], None]] = None,\n) -> None:\n    \"\"\"\n    Process ChatGPT/Claude exports (or per-conversation directories) and write linearized conversations.\n\n    Accepts either:\n    - A top-level list of conversations (standard export format)\n    - A single conversation object with mapping/current_node\n    - A directory containing many per-conversation `.json` files\n\n    Args:\n        input_path: Path to JSON file or directory containing JSON files\n        output_dir: Output directory for linearized markdown files\n        limit: Optional limit on number of conversations to process\n        progress_cb: Optional callback(completed, total, context) for progress updates\n    \"\"\"\n    logger.info(\n        \"Loading export\",\n        extra={\n            \"event\": \"linearize.export.load\",\n            \"input_path\": str(input_path),\n            \"limit\": limit,\n        },\n    )\n\n    conversations = load_conversations(input_path, limit=limit)\n\n    logger.info(\n        \"Found conversations\",\n        extra={\"event\": \"linearize.export.found\", \"num_conversations\": len(conversations)},\n    )\n\n    # Notify progress callback of total\n    if progress_cb:\n        progress_cb(0, len(conversations), {})\n\n    if should_show_progress() and not progress_cb:\n        console = Console(stderr=True)\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            console=console,\n        ) as progress:\n            task = progress.add_task(\"Linearizing conversations...\", total=len(conversations))\n\n            for conv in conversations:\n                conv_id = conv.get(\"id\") or conv.get(\"conversation_id\")\n                if not conv_id:\n                    logger.warning(\n                        \"Skipping conversation without ID\",\n                        extra={\"event\": \"linearize.conversation.skipped\", \"reason\": \"no_id\"},\n                    )\n                    progress.advance(task)\n                    continue\n\n                conv_logger = with_context(logger, conversation_id=conv_id)\n                title = get_title(conv)\n                project_id = get_project_id(conv)\n                project_name = get_project_name(conv)\n                messages = linearize_conversation(conv)\n\n                if messages:\n                    output_path = write_conversation_markdown(\n                        messages,\n                        conv_id,\n                        title,\n                        output_dir,\n                        project_id=project_id,\n                        project_name=project_name,\n                    )\n                    conv_logger.debug(\n                        \"Linearized conversation\",\n                        extra={\n                            \"event\": \"linearize.conversation.complete\",\n                            \"num_messages\": len(messages),\n                            \"output_path\": str(output_path),\n                        },\n                    )\n                else:\n                    conv_logger.warning(\n                        \"No messages found\",\n                        extra={\"event\": \"linearize.conversation.skipped\", \"reason\": \"no_messages\"},\n                    )\n\n                progress.advance(task)\n                # Update progress callback if provided\n                if progress_cb:\n                    progress_cb(progress.tasks[task].completed, len(conversations), {\"conversation_id\": conv_id})\n    else:\n        # Non-interactive mode or dashboard mode: process without progress bar\n        completed = 0\n        for conv in conversations:\n            conv_id = conv.get(\"id\") or conv.get(\"conversation_id\")\n            if not conv_id:\n                logger.warning(\n                    \"Skipping conversation without ID\",\n                    extra={\"event\": \"linearize.conversation.skipped\", \"reason\": \"no_id\"},\n                )\n                continue\n\n            conv_logger = with_context(logger, conversation_id=conv_id)\n            title = get_title(conv)\n            project_id = get_project_id(conv)\n            project_name = get_project_name(conv)\n            messages = linearize_conversation(conv)\n\n            if messages:\n                output_path = write_conversation_markdown(\n                    messages,\n                    conv_id,\n                    title,\n                    output_dir,\n                    project_id=project_id,\n                    project_name=project_name,\n                )\n                conv_logger.debug(\n                    \"Linearized conversation\",\n                    extra={\n                        \"event\": \"linearize.conversation.complete\",\n                        \"num_messages\": len(messages),\n                        \"output_path\": str(output_path),\n                    },\n                )\n            else:\n                conv_logger.warning(\n                    \"No messages found\",\n                    extra={\"event\": \"linearize.conversation.skipped\", \"reason\": \"no_messages\"},\n                )\n\n            # Update progress callback\n            completed += 1\n            if progress_cb:\n                progress_cb(completed, len(conversations), {\"conversation_id\": conv_id})\n\n    logger.info(\n        \"Linearization complete\",\n        extra={\"event\": \"linearize.export.complete\"},\n    )",
    "start_line": 122,
    "end_line": 263,
    "has_docstring": true,
    "docstring": "Process ChatGPT/Claude exports (or per-conversation directories) and write linearized conversations.\n\nAccepts either:\n- A top-level list of conversations (standard export format)\n- A single conversation object with mapping/current_node\n- A directory containing many per-conversation `.json` files\n\nArgs:\n    input_path: Path to JSON file or directory containing JSON files\n    output_dir: Output directory for linearized markdown files\n    limit: Optional limit on number of conversations to process\n    progress_cb: Optional callback(completed, total, context) for progress updates",
    "parameters": [
      "input_path",
      "output_dir",
      "limit",
      "progress_cb"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function linearize_export",
    "component_id": "src.ck_exporter.pipeline.linearize.linearize_export"
  },
  "src.ck_exporter.pipeline.topics.read_jsonl": {
    "id": "src.ck_exporter.pipeline.topics.read_jsonl",
    "name": "read_jsonl",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/topics.py",
    "relative_path": "src/ck_exporter/pipeline/topics.py",
    "depends_on": [
      "src.ck_exporter.pipeline.consolidate._read_jsonl"
    ],
    "source_code": "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n    \"\"\"Read JSONL file and return list of objects.\"\"\"\n    return list(_read_jsonl(path))",
    "start_line": 29,
    "end_line": 31,
    "has_docstring": true,
    "docstring": "Read JSONL file and return list of objects.",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function read_jsonl",
    "component_id": "src.ck_exporter.pipeline.topics.read_jsonl"
  },
  "src.ck_exporter.pipeline.topics.build_conversation_documents": {
    "id": "src.ck_exporter.pipeline.topics.build_conversation_documents",
    "name": "build_conversation_documents",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/topics.py",
    "relative_path": "src/ck_exporter/pipeline/topics.py",
    "depends_on": [
      "src.ck_exporter.pipeline.io.load.is_claude_conversation",
      "src.ck_exporter.pipeline.io.load.convert_claude_to_chatgpt",
      "src.ck_exporter.pipeline.topics.read_jsonl",
      "src.ck_exporter.pipeline.io.load.load_conversations"
    ],
    "source_code": "def build_conversation_documents(\n    input_path: Path,\n    atoms_path: Path,\n    decisions_path: Path,\n    questions_path: Path,\n    limit: Optional[int] = None,\n) -> Tuple[Dict[str, str], Dict[str, str]]:\n    \"\"\"\n    Build documents for each conversation by concatenating artifacts.\n\n    Args:\n        input_path: Path to conversation JSON file(s)\n        atoms_path: Path to consolidated atoms.jsonl\n        decisions_path: Path to consolidated decisions.jsonl\n        questions_path: Path to consolidated open_questions.jsonl\n        limit: Optional limit on number of conversations to process\n\n    Returns:\n        Tuple of (conversation_documents, conversation_titles)\n        - conversation_documents: dict mapping conversation_id to document text\n        - conversation_titles: dict mapping conversation_id to title\n    \"\"\"\n    # Load conversations to get titles\n    conversations = load_conversations(input_path, limit=limit)\n    conversation_titles = {}\n    conversation_projects: Dict[str, str] = {}\n    for conv in conversations:\n        # Handle Claude conversations that might not be converted yet\n        if is_claude_conversation(conv):\n            conv = convert_claude_to_chatgpt(conv)\n        conv_id = conv.get(\"conversation_id\") or conv.get(\"id\") or conv.get(\"uuid\", \"\")\n        title = conv.get(\"title\") or conv.get(\"name\") or \"Untitled Conversation\"\n        conversation_titles[conv_id] = title\n        project_id = conv.get(\"project_id\") or conv.get(\"project_uuid\")\n        project_name = conv.get(\"project_name\")\n        if not project_id and isinstance(conv.get(\"project\"), dict):\n            project_id = conv[\"project\"].get(\"uuid\")\n        if not project_name and isinstance(conv.get(\"project\"), dict):\n            project_name = conv[\"project\"].get(\"name\")\n\n        if project_name and project_id:\n            conversation_projects[conv_id] = f\"{project_name} ({project_id})\"\n        elif project_name:\n            conversation_projects[conv_id] = str(project_name)\n        elif project_id:\n            conversation_projects[conv_id] = str(project_id)\n\n    # Load consolidated artifacts\n    atoms = read_jsonl(atoms_path)\n    decisions = read_jsonl(decisions_path)\n    questions = read_jsonl(questions_path)\n\n    # Group artifacts by conversation_id\n    atoms_by_conv = defaultdict(list)\n    decisions_by_conv = defaultdict(list)\n    questions_by_conv = defaultdict(list)\n\n    for atom in atoms:\n        conv_id = atom.get(\"source_conversation_id\", \"\")\n        if conv_id:\n            atoms_by_conv[conv_id].append(atom.get(\"statement\", \"\"))\n\n    for decision in decisions:\n        conv_id = decision.get(\"source_conversation_id\", \"\")\n        if conv_id:\n            decisions_by_conv[conv_id].append(decision.get(\"statement\", \"\"))\n\n    for question in questions:\n        conv_id = question.get(\"source_conversation_id\", \"\")\n        if conv_id:\n            questions_by_conv[conv_id].append(question.get(\"question\", \"\"))\n\n    # Build documents\n    conversation_documents = {}\n    all_conv_ids = (\n        set(conversation_titles.keys())\n        | set(atoms_by_conv.keys())\n        | set(decisions_by_conv.keys())\n        | set(questions_by_conv.keys())\n    )\n\n    for conv_id in all_conv_ids:\n        title = conversation_titles.get(conv_id, \"Untitled Conversation\")\n        parts = [f\"Title: {title}\"]\n\n        project_label = conversation_projects.get(conv_id)\n        if project_label:\n            parts.append(f\"Project: {project_label}\")\n\n        atom_statements = atoms_by_conv.get(conv_id, [])\n        if atom_statements:\n            parts.append(\"\\nFacts and Knowledge:\")\n            parts.extend(f\"- {stmt}\" for stmt in atom_statements)\n\n        decision_statements = decisions_by_conv.get(conv_id, [])\n        if decision_statements:\n            parts.append(\"\\nDecisions:\")\n            parts.extend(f\"- {stmt}\" for stmt in decision_statements)\n\n        question_texts = questions_by_conv.get(conv_id, [])\n        if question_texts:\n            parts.append(\"\\nOpen Questions:\")\n            parts.extend(f\"- {q}\" for q in question_texts)\n\n        conversation_documents[conv_id] = \"\\n\".join(parts)\n\n    return conversation_documents, conversation_titles",
    "start_line": 34,
    "end_line": 140,
    "has_docstring": true,
    "docstring": "Build documents for each conversation by concatenating artifacts.\n\nArgs:\n    input_path: Path to conversation JSON file(s)\n    atoms_path: Path to consolidated atoms.jsonl\n    decisions_path: Path to consolidated decisions.jsonl\n    questions_path: Path to consolidated open_questions.jsonl\n    limit: Optional limit on number of conversations to process\n\nReturns:\n    Tuple of (conversation_documents, conversation_titles)\n    - conversation_documents: dict mapping conversation_id to document text\n    - conversation_titles: dict mapping conversation_id to title",
    "parameters": [
      "input_path",
      "atoms_path",
      "decisions_path",
      "questions_path",
      "limit"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function build_conversation_documents",
    "component_id": "src.ck_exporter.pipeline.topics.build_conversation_documents"
  },
  "src.ck_exporter.pipeline.topics.discover_topics": {
    "id": "src.ck_exporter.pipeline.topics.discover_topics",
    "name": "discover_topics",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/topics.py",
    "relative_path": "src/ck_exporter/pipeline/topics.py",
    "depends_on": [],
    "source_code": "def discover_topics(\n    documents: Dict[str, str],\n    embedder: Embedder,\n    target_topics: int = 50,\n    use_pooling: bool = True,\n    chunk_tokens: int = 600,\n    overlap_tokens: int = 80,\n    cache_dir: Optional[Path] = None,\n) -> Tuple[BERTopic, np.ndarray, List[str]]:\n    \"\"\"\n    Discover topics using BERTopic with pre-computed embeddings.\n\n    Args:\n        documents: Dict mapping conversation_id to document text\n        embedder: Embedder implementation\n        target_topics: Target number of topics\n        use_pooling: Whether to use chunked pooling (default True)\n        chunk_tokens: Maximum tokens per chunk when pooling (default 600)\n        overlap_tokens: Token overlap between chunks when pooling (default 80)\n        cache_dir: Optional directory for caching embeddings\n\n    Returns:\n        Tuple of (bertopic_model, embeddings, doc_ids)\n    \"\"\"\n    if not documents:\n        raise ValueError(\"No documents provided\")\n\n    doc_ids = list(documents.keys())\n    doc_texts = [documents[conv_id] for conv_id in doc_ids]\n\n    logger.info(\n        \"Generating embeddings for conversations\",\n        extra={\n            \"event\": \"topics.embeddings.start\",\n            \"num_conversations\": len(doc_texts),\n            \"use_pooling\": use_pooling,\n            \"chunk_tokens\": chunk_tokens if use_pooling else None,\n        },\n    )\n\n    # Generate embeddings via embedder\n    if use_pooling:\n        embeddings = embedder.embed_pooled(\n            doc_texts,\n            chunk_tokens=chunk_tokens,\n            overlap_tokens=overlap_tokens,\n            cache_dir=cache_dir,\n        )\n    else:\n        embeddings = embedder.embed(doc_texts)\n\n    logger.info(\n        \"Generated embeddings\",\n        extra={\n            \"event\": \"topics.embeddings.complete\",\n            \"shape\": list(embeddings.shape),\n            \"embedding_dim\": embeddings.shape[1] if len(embeddings.shape) > 1 else None,\n        },\n    )\n\n    # Configure BERTopic with pre-computed embeddings\n    # Gate verbose based on log level - only verbose if DEBUG/INFO logging is enabled\n    bertopic_logger = logging.getLogger(\"bertopic\")\n    verbose = bertopic_logger.level <= logging.INFO\n\n    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42)\n    hdbscan_model = HDBSCAN(min_cluster_size=2, metric=\"euclidean\", cluster_selection_method=\"eom\")\n\n    logger.info(\n        \"Configuring BERTopic\",\n        extra={\n            \"event\": \"topics.bertopic.config\",\n            \"target_topics\": target_topics,\n            \"verbose\": verbose,\n        },\n    )\n\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        nr_topics=target_topics,\n        min_topic_size=2,\n        calculate_probabilities=False,\n        verbose=verbose,\n    )\n\n    logger.info(\n        \"Running BERTopic clustering\",\n        extra={\"event\": \"topics.bertopic.clustering.start\"},\n    )\n    topics, probs = topic_model.fit_transform(doc_texts, embeddings=embeddings)\n\n    num_topics = len(set(topics)) - (1 if -1 in topics else 0)  # Exclude outlier topic -1\n    logger.info(\n        \"Discovered topics\",\n        extra={\n            \"event\": \"topics.bertopic.clustering.complete\",\n            \"num_topics\": num_topics,\n            \"num_outliers\": list(topics).count(-1) if -1 in topics else 0,\n        },\n    )\n\n    return topic_model, embeddings, doc_ids",
    "start_line": 143,
    "end_line": 245,
    "has_docstring": true,
    "docstring": "Discover topics using BERTopic with pre-computed embeddings.\n\nArgs:\n    documents: Dict mapping conversation_id to document text\n    embedder: Embedder implementation\n    target_topics: Target number of topics\n    use_pooling: Whether to use chunked pooling (default True)\n    chunk_tokens: Maximum tokens per chunk when pooling (default 600)\n    overlap_tokens: Token overlap between chunks when pooling (default 80)\n    cache_dir: Optional directory for caching embeddings\n\nReturns:\n    Tuple of (bertopic_model, embeddings, doc_ids)",
    "parameters": [
      "documents",
      "embedder",
      "target_topics",
      "use_pooling",
      "chunk_tokens",
      "overlap_tokens",
      "cache_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function discover_topics",
    "component_id": "src.ck_exporter.pipeline.topics.discover_topics"
  },
  "src.ck_exporter.pipeline.topics.label_topics_with_llm": {
    "id": "src.ck_exporter.pipeline.topics.label_topics_with_llm",
    "name": "label_topics_with_llm",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/topics.py",
    "relative_path": "src/ck_exporter/pipeline/topics.py",
    "depends_on": [
      "apps.review-api.src.review_api.main.get_topic",
      "src.ck_exporter.core.models.topics.Topic",
      "src.ck_exporter.logging.should_show_progress"
    ],
    "source_code": "def label_topics_with_llm(\n    topic_model: BERTopic,\n    documents: Dict[str, str],\n    doc_ids: List[str],\n    doc_texts: List[str],\n    labeler: TopicLabeler,\n) -> List[Topic]:\n    \"\"\"\n    Label discovered topics using LLM.\n\n    Args:\n        topic_model: Fitted BERTopic model\n        documents: Dict mapping conversation_id to document text\n        doc_ids: List of conversation IDs in same order as doc_texts\n        doc_texts: List of document texts\n        labeler: Topic labeler implementation\n\n    Returns:\n        List of Topic objects with names and descriptions\n    \"\"\"\n    # Get topic info from BERTopic\n    topic_info = topic_model.get_topic_info()\n    topics_out = topic_model.topics_\n\n    discovered_topics = []\n\n    logger.info(\n        \"Labeling topics with LLM\",\n        extra={\n            \"event\": \"topics.labeling.start\",\n            \"num_topics\": len(topic_info),\n        },\n    )\n\n    # Progress bar will be rendered conditionally based on logging mode\n    from rich.console import Console\n\n    if should_show_progress():\n        console = Console(stderr=True)\n        with Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            TaskProgressColumn(),\n            console=console,\n        ) as progress:\n            task = progress.add_task(\"Labeling topics\", total=len(topic_info))\n\n            for idx, row in topic_info.iterrows():\n                topic_id = int(row[\"Topic\"])\n                if topic_id == -1:  # Skip outlier topic\n                    progress.update(task, advance=1)\n                    continue\n\n                # Get representative documents for this topic\n                topic_docs = []\n                for i, doc_topic in enumerate(topics_out):\n                    if doc_topic == topic_id:\n                        topic_docs.append((doc_ids[i], doc_texts[i]))\n\n                # Take top 3 representative documents\n                representative_docs = topic_docs[:3]\n                if not representative_docs:\n                    progress.update(task, advance=1)\n                    continue\n\n                # Get keywords from BERTopic\n                topic_words = topic_model.get_topic(topic_id)\n                keywords = [word for word, _ in topic_words[:10]] if topic_words else []\n\n                # Label using labeler\n                label_data = labeler.label_topic(topic_id, representative_docs, keywords)\n                name = label_data.get(\"name\", f\"Topic {topic_id}\")\n                description = label_data.get(\"description\", \"No description available\")\n\n                # Get representative conversation IDs\n                rep_conv_ids = [conv_id for conv_id, _ in representative_docs]\n\n                topic = Topic(\n                    topic_id=topic_id,\n                    name=name,\n                    description=description,\n                    keywords=keywords,\n                    representative_conversations=rep_conv_ids,\n                )\n\n                discovered_topics.append(topic)\n                progress.update(task, advance=1)\n    else:\n        # Non-interactive mode: process without progress bar\n        for idx, row in topic_info.iterrows():\n            topic_id = int(row[\"Topic\"])\n            if topic_id == -1:  # Skip outlier topic\n                continue\n\n            # Get representative documents for this topic\n            topic_docs = []\n            for i, doc_topic in enumerate(topics_out):\n                if doc_topic == topic_id:\n                    topic_docs.append((doc_ids[i], doc_texts[i]))\n\n            # Take top 3 representative documents\n            representative_docs = topic_docs[:3]\n            if not representative_docs:\n                continue\n\n            # Get keywords from BERTopic\n            topic_words = topic_model.get_topic(topic_id)\n            keywords = [word for word, _ in topic_words[:10]] if topic_words else []\n\n            # Label using labeler\n            label_data = labeler.label_topic(topic_id, representative_docs, keywords)\n            name = label_data.get(\"name\", f\"Topic {topic_id}\")\n            description = label_data.get(\"description\", \"No description available\")\n\n            # Get representative conversation IDs\n            rep_conv_ids = [conv_id for conv_id, _ in representative_docs]\n\n            topic = Topic(\n                topic_id=topic_id,\n                name=name,\n                description=description,\n                keywords=keywords,\n                representative_conversations=rep_conv_ids,\n            )\n\n            discovered_topics.append(topic)\n\n    logger.info(\n        \"Labeled topics with LLM\",\n        extra={\n            \"event\": \"topics.labeling.complete\",\n            \"num_labeled\": len(discovered_topics),\n        },\n    )\n\n    return discovered_topics",
    "start_line": 248,
    "end_line": 384,
    "has_docstring": true,
    "docstring": "Label discovered topics using LLM.\n\nArgs:\n    topic_model: Fitted BERTopic model\n    documents: Dict mapping conversation_id to document text\n    doc_ids: List of conversation IDs in same order as doc_texts\n    doc_texts: List of document texts\n    labeler: Topic labeler implementation\n\nReturns:\n    List of Topic objects with names and descriptions",
    "parameters": [
      "topic_model",
      "documents",
      "doc_ids",
      "doc_texts",
      "labeler"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function label_topics_with_llm",
    "component_id": "src.ck_exporter.pipeline.topics.label_topics_with_llm"
  },
  "src.ck_exporter.pipeline.topics.save_topic_registry": {
    "id": "src.ck_exporter.pipeline.topics.save_topic_registry",
    "name": "save_topic_registry",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/pipeline/topics.py",
    "relative_path": "src/ck_exporter/pipeline/topics.py",
    "depends_on": [
      "src.ck_exporter.core.models.topics.TopicRegistry"
    ],
    "source_code": "def save_topic_registry(\n    topics: List[Topic],\n    topic_model: BERTopic,\n    embeddings: np.ndarray,\n    topics_out: np.ndarray,\n    doc_ids: List[str],\n    embedding_model: str,\n    output_path: Path,\n) -> None:\n    \"\"\"\n    Save topic registry with centroid embeddings.\n\n    Args:\n        topics: List of Topic objects\n        topic_model: Fitted BERTopic model\n        embeddings: Document embeddings array\n        topics_out: Topic assignments for each document\n        doc_ids: List of conversation IDs\n        embedding_model: Model identifier\n        output_path: Path to save registry JSON\n    \"\"\"\n    # Calculate centroid embeddings for each topic\n    topic_centroids = {}\n    for topic in topics:\n        topic_id = topic.topic_id\n        # Get embeddings for documents in this topic\n        topic_emb_indices = [i for i, t in enumerate(topics_out) if t == topic_id]\n        if topic_emb_indices:\n            topic_embeddings = embeddings[topic_emb_indices]\n            centroid = np.mean(topic_embeddings, axis=0)\n            topic_centroids[topic_id] = centroid.tolist()\n\n    # Add centroid embeddings to topics\n    for topic in topics:\n        if topic.topic_id in topic_centroids:\n            topic.centroid_embedding = topic_centroids[topic.topic_id]\n\n    from datetime import datetime\n\n    registry = TopicRegistry(\n        generated_at=datetime.now().isoformat(),\n        embedding_model=embedding_model,\n        num_topics=len(topics),\n        topics=topics,\n    )\n\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(registry.model_dump(), f, indent=2, ensure_ascii=False)\n\n    logger.info(\n        \"Saved topic registry\",\n        extra={\n            \"event\": \"topics.registry.saved\",\n            \"output_path\": str(output_path),\n            \"num_topics\": len(topics),\n            \"embedding_model\": embedding_model,\n        },\n    )",
    "start_line": 387,
    "end_line": 445,
    "has_docstring": true,
    "docstring": "Save topic registry with centroid embeddings.\n\nArgs:\n    topics: List of Topic objects\n    topic_model: Fitted BERTopic model\n    embeddings: Document embeddings array\n    topics_out: Topic assignments for each document\n    doc_ids: List of conversation IDs\n    embedding_model: Model identifier\n    output_path: Path to save registry JSON",
    "parameters": [
      "topics",
      "topic_model",
      "embeddings",
      "topics_out",
      "doc_ids",
      "embedding_model",
      "output_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function save_topic_registry",
    "component_id": "src.ck_exporter.pipeline.topics.save_topic_registry"
  },
  "src.ck_exporter.programs.dspy.label_topic.LabelTopic": {
    "id": "src.ck_exporter.programs.dspy.label_topic.LabelTopic",
    "name": "LabelTopic",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/dspy/label_topic.py",
    "relative_path": "src/ck_exporter/programs/dspy/label_topic.py",
    "depends_on": [],
    "source_code": "    class LabelTopic:  # type: ignore\n        pass",
    "start_line": 27,
    "end_line": 28,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class LabelTopic",
    "component_id": "src.ck_exporter.programs.dspy.label_topic.LabelTopic"
  },
  "src.ck_exporter.programs.dspy.label_topic.create_label_topic_program": {
    "id": "src.ck_exporter.programs.dspy.label_topic.create_label_topic_program",
    "name": "create_label_topic_program",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/dspy/label_topic.py",
    "relative_path": "src/ck_exporter/programs/dspy/label_topic.py",
    "depends_on": [],
    "source_code": "def create_label_topic_program(lm: \"dspy.LM\") -> \"dspy.Module\":\n    \"\"\"\n    Create a DSPy program for topic labeling.\n\n    Args:\n        lm: Configured DSPy language model\n\n    Returns:\n        DSPy module for topic labeling\n    \"\"\"\n    if dspy is None:\n        raise ImportError(\"dspy-ai is not installed. Install it with: uv sync --extra dspy\")\n\n    # Set the LM\n    dspy.configure(lm=lm)\n\n    # Create a simple chain-of-thought program\n    class LabelTopicProgram(dspy.Module):\n        def __init__(self):\n            super().__init__()\n            self.labeler = dspy.ChainOfThought(LabelTopic)\n\n        def forward(self, representative_docs: str, keywords: str) -> dict[str, str]:\n            result = self.labeler(representative_docs=representative_docs, keywords=keywords)\n            return {\n                \"name\": result.name,\n                \"description\": result.description,\n            }\n\n    return LabelTopicProgram()",
    "start_line": 31,
    "end_line": 60,
    "has_docstring": true,
    "docstring": "Create a DSPy program for topic labeling.\n\nArgs:\n    lm: Configured DSPy language model\n\nReturns:\n    DSPy module for topic labeling",
    "parameters": [
      "lm"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_label_topic_program",
    "component_id": "src.ck_exporter.programs.dspy.label_topic.create_label_topic_program"
  },
  "src.ck_exporter.programs.dspy.label_topic.LabelTopicProgram": {
    "id": "src.ck_exporter.programs.dspy.label_topic.LabelTopicProgram",
    "name": "LabelTopicProgram",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/dspy/label_topic.py",
    "relative_path": "src/ck_exporter/programs/dspy/label_topic.py",
    "depends_on": [],
    "source_code": "    class LabelTopicProgram(dspy.Module):\n        def __init__(self):\n            super().__init__()\n            self.labeler = dspy.ChainOfThought(LabelTopic)\n\n        def forward(self, representative_docs: str, keywords: str) -> dict[str, str]:\n            result = self.labeler(representative_docs=representative_docs, keywords=keywords)\n            return {\n                \"name\": result.name,\n                \"description\": result.description,\n            }",
    "start_line": 48,
    "end_line": 58,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "dspy.Module"
    ],
    "class_name": null,
    "display_name": "class LabelTopicProgram",
    "component_id": "src.ck_exporter.programs.dspy.label_topic.LabelTopicProgram"
  },
  "src.ck_exporter.programs.dspy.refine_atoms.RefineAtoms": {
    "id": "src.ck_exporter.programs.dspy.refine_atoms.RefineAtoms",
    "name": "RefineAtoms",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/dspy/refine_atoms.py",
    "relative_path": "src/ck_exporter/programs/dspy/refine_atoms.py",
    "depends_on": [],
    "source_code": "    class RefineAtoms:  # type: ignore\n        pass",
    "start_line": 31,
    "end_line": 32,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class RefineAtoms",
    "component_id": "src.ck_exporter.programs.dspy.refine_atoms.RefineAtoms"
  },
  "src.ck_exporter.programs.dspy.refine_atoms.create_refine_atoms_program": {
    "id": "src.ck_exporter.programs.dspy.refine_atoms.create_refine_atoms_program",
    "name": "create_refine_atoms_program",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/dspy/refine_atoms.py",
    "relative_path": "src/ck_exporter/programs/dspy/refine_atoms.py",
    "depends_on": [],
    "source_code": "def create_refine_atoms_program(lm: \"dspy.LM\") -> \"dspy.Module\":\n    \"\"\"\n    Create a DSPy program for atom refinement.\n\n    Args:\n        lm: Configured DSPy language model\n\n    Returns:\n        DSPy module for atom refinement\n    \"\"\"\n    if dspy is None:\n        raise ImportError(\"dspy-ai is not installed. Install it with: uv sync --extra dspy\")\n\n    # Set the LM\n    dspy.configure(lm=lm)\n\n    # Create a chain-of-thought program\n    class RefineAtomsProgram(dspy.Module):\n        def __init__(self):\n            super().__init__()\n            self.refiner = dspy.ChainOfThought(RefineAtoms)\n\n        def forward(\n            self, conversation_id: str, conversation_title: str, candidates_json: str\n        ) -> dict[str, str]:\n            result = self.refiner(\n                conversation_id=conversation_id,\n                conversation_title=conversation_title,\n                candidates_json=candidates_json,\n            )\n            return {\n                \"facts_json\": result.facts_json,\n                \"decisions_json\": result.decisions_json,\n                \"open_questions_json\": result.open_questions_json,\n            }\n\n    return RefineAtomsProgram()",
    "start_line": 35,
    "end_line": 71,
    "has_docstring": true,
    "docstring": "Create a DSPy program for atom refinement.\n\nArgs:\n    lm: Configured DSPy language model\n\nReturns:\n    DSPy module for atom refinement",
    "parameters": [
      "lm"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_refine_atoms_program",
    "component_id": "src.ck_exporter.programs.dspy.refine_atoms.create_refine_atoms_program"
  },
  "src.ck_exporter.programs.dspy.refine_atoms.RefineAtomsProgram": {
    "id": "src.ck_exporter.programs.dspy.refine_atoms.RefineAtomsProgram",
    "name": "RefineAtomsProgram",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/dspy/refine_atoms.py",
    "relative_path": "src/ck_exporter/programs/dspy/refine_atoms.py",
    "depends_on": [],
    "source_code": "    class RefineAtomsProgram(dspy.Module):\n        def __init__(self):\n            super().__init__()\n            self.refiner = dspy.ChainOfThought(RefineAtoms)\n\n        def forward(\n            self, conversation_id: str, conversation_title: str, candidates_json: str\n        ) -> dict[str, str]:\n            result = self.refiner(\n                conversation_id=conversation_id,\n                conversation_title=conversation_title,\n                candidates_json=candidates_json,\n            )\n            return {\n                \"facts_json\": result.facts_json,\n                \"decisions_json\": result.decisions_json,\n                \"open_questions_json\": result.open_questions_json,\n            }",
    "start_line": 52,
    "end_line": 69,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "dspy.Module"
    ],
    "class_name": null,
    "display_name": "class RefineAtomsProgram",
    "component_id": "src.ck_exporter.programs.dspy.refine_atoms.RefineAtomsProgram"
  },
  "src.ck_exporter.programs.json_extract.extract_json_from_text": {
    "id": "src.ck_exporter.programs.json_extract.extract_json_from_text",
    "name": "extract_json_from_text",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/programs/json_extract.py",
    "relative_path": "src/ck_exporter/programs/json_extract.py",
    "depends_on": [],
    "source_code": "def extract_json_from_text(text: str) -> Optional[dict[str, Any]]:\n    \"\"\"\n    Try to extract JSON from text that might have markdown code blocks.\n\n    Args:\n        text: Text that may contain JSON (possibly wrapped in markdown)\n\n    Returns:\n        Parsed JSON dict, or None if parsing fails\n    \"\"\"\n    text = text.strip()\n    # Try to find JSON in markdown code blocks\n    if \"```json\" in text:\n        start = text.find(\"```json\") + 7\n        end = text.find(\"```\", start)\n        if end > start:\n            text = text[start:end].strip()\n    elif \"```\" in text:\n        start = text.find(\"```\") + 3\n        end = text.find(\"```\", start)\n        if end > start:\n            text = text[start:end].strip()\n    # Try parsing\n    try:\n        return json.loads(text)\n    except json.JSONDecodeError:\n        return None",
    "start_line": 7,
    "end_line": 33,
    "has_docstring": true,
    "docstring": "Try to extract JSON from text that might have markdown code blocks.\n\nArgs:\n    text: Text that may contain JSON (possibly wrapped in markdown)\n\nReturns:\n    Parsed JSON dict, or None if parsing fails",
    "parameters": [
      "text"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_json_from_text",
    "component_id": "src.ck_exporter.programs.json_extract.extract_json_from_text"
  },
  "src.ck_exporter.topic_discovery.discover_topics": {
    "id": "src.ck_exporter.topic_discovery.discover_topics",
    "name": "discover_topics",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/topic_discovery.py",
    "relative_path": "src/ck_exporter/topic_discovery.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.bootstrap.build_embedder"
    ],
    "source_code": "def discover_topics(\n    documents: Dict[str, str],\n    embedding_model: str = \"openai/text-embedding-3-small\",\n    target_topics: int = 50,\n    use_openrouter: bool = True,\n    use_pooling: bool = True,\n    chunk_tokens: int = 600,\n    overlap_tokens: int = 80,\n    cache_dir: Optional[Path] = None,\n) -> Tuple[BERTopic, np.ndarray, List[str]]:\n    \"\"\"\n    Discover topics using BERTopic with pre-computed embeddings.\n\n    This is a backward-compatible wrapper that creates adapters and calls the new pipeline.\n\n    Args:\n        documents: Dict mapping conversation_id to document text\n        embedding_model: OpenRouter model identifier\n        target_topics: Target number of topics\n        use_openrouter: Whether to use OpenRouter\n        use_pooling: Whether to use chunked pooling (default True)\n        chunk_tokens: Maximum tokens per chunk when pooling (default 600)\n        overlap_tokens: Token overlap between chunks when pooling (default 80)\n        cache_dir: Optional directory for caching embeddings\n\n    Returns:\n        Tuple of (bertopic_model, embeddings, doc_ids)\n    \"\"\"\n    # Build embedder using bootstrap\n    shared_client = make_openrouter_client(use_openrouter)\n    embedder = build_embedder(\n        model=embedding_model,\n        use_openrouter=use_openrouter,\n        client=shared_client,\n    )\n\n    # Delegate to pipeline\n    return _discover_topics(\n        documents=documents,\n        embedder=embedder,\n        target_topics=target_topics,\n        use_pooling=use_pooling,\n        chunk_tokens=chunk_tokens,\n        overlap_tokens=overlap_tokens,\n        cache_dir=cache_dir,\n    )",
    "start_line": 34,
    "end_line": 79,
    "has_docstring": true,
    "docstring": "Discover topics using BERTopic with pre-computed embeddings.\n\nThis is a backward-compatible wrapper that creates adapters and calls the new pipeline.\n\nArgs:\n    documents: Dict mapping conversation_id to document text\n    embedding_model: OpenRouter model identifier\n    target_topics: Target number of topics\n    use_openrouter: Whether to use OpenRouter\n    use_pooling: Whether to use chunked pooling (default True)\n    chunk_tokens: Maximum tokens per chunk when pooling (default 600)\n    overlap_tokens: Token overlap between chunks when pooling (default 80)\n    cache_dir: Optional directory for caching embeddings\n\nReturns:\n    Tuple of (bertopic_model, embeddings, doc_ids)",
    "parameters": [
      "documents",
      "embedding_model",
      "target_topics",
      "use_openrouter",
      "use_pooling",
      "chunk_tokens",
      "overlap_tokens",
      "cache_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function discover_topics",
    "component_id": "src.ck_exporter.topic_discovery.discover_topics"
  },
  "src.ck_exporter.topic_discovery.label_topics_with_llm": {
    "id": "src.ck_exporter.topic_discovery.label_topics_with_llm",
    "name": "label_topics_with_llm",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/topic_discovery.py",
    "relative_path": "src/ck_exporter/topic_discovery.py",
    "depends_on": [
      "src.ck_exporter.adapters.openrouter_client.make_openrouter_client",
      "src.ck_exporter.bootstrap.build_topic_labeler"
    ],
    "source_code": "def label_topics_with_llm(\n    topic_model: BERTopic,\n    documents: Dict[str, str],\n    doc_ids: List[str],\n    doc_texts: List[str],\n    embedding_model: str,\n    use_openrouter: bool = True,\n    label_model: Optional[str] = None,\n) -> List[Topic]:\n    \"\"\"\n    Label discovered topics using LLM.\n\n    This is a backward-compatible wrapper that creates adapters and calls the new pipeline.\n\n    Args:\n        topic_model: Fitted BERTopic model\n        documents: Dict mapping conversation_id to document text\n        doc_ids: List of conversation IDs in same order as doc_texts\n        doc_texts: List of document texts\n        embedding_model: Embedding model used (for registry)\n        use_openrouter: Whether to use OpenRouter\n        label_model: LLM model for labeling (defaults to fast model)\n\n    Returns:\n        List of Topic objects with names and descriptions\n    \"\"\"\n    # Build labeler using bootstrap (handles env var selection)\n    shared_client = make_openrouter_client(use_openrouter)\n    labeler = build_topic_labeler(\n        label_model=label_model,\n        use_openrouter=use_openrouter,\n        shared_client=shared_client,\n    )\n\n    # Delegate to pipeline\n    return _label_topics_with_llm(\n        topic_model=topic_model,\n        documents=documents,\n        doc_ids=doc_ids,\n        doc_texts=doc_texts,\n        labeler=labeler,\n    )",
    "start_line": 82,
    "end_line": 123,
    "has_docstring": true,
    "docstring": "Label discovered topics using LLM.\n\nThis is a backward-compatible wrapper that creates adapters and calls the new pipeline.\n\nArgs:\n    topic_model: Fitted BERTopic model\n    documents: Dict mapping conversation_id to document text\n    doc_ids: List of conversation IDs in same order as doc_texts\n    doc_texts: List of document texts\n    embedding_model: Embedding model used (for registry)\n    use_openrouter: Whether to use OpenRouter\n    label_model: LLM model for labeling (defaults to fast model)\n\nReturns:\n    List of Topic objects with names and descriptions",
    "parameters": [
      "topic_model",
      "documents",
      "doc_ids",
      "doc_texts",
      "embedding_model",
      "use_openrouter",
      "label_model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function label_topics_with_llm",
    "component_id": "src.ck_exporter.topic_discovery.label_topics_with_llm"
  },
  "src.ck_exporter.ui.dashboard.StepStatus": {
    "id": "src.ck_exporter.ui.dashboard.StepStatus",
    "name": "StepStatus",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/ui/dashboard.py",
    "relative_path": "src/ck_exporter/ui/dashboard.py",
    "depends_on": [],
    "source_code": "class StepStatus:\n    \"\"\"Status for a pipeline step.\"\"\"\n\n    name: str\n    status: str = \"pending\"  # pending, running, complete, error\n    completed: int = 0\n    total: int = 0\n    start_time: Optional[float] = None\n    elapsed: float = 0.0\n    rate: float = 0.0\n    eta: Optional[float] = None\n    current_item: Optional[str] = None",
    "start_line": 26,
    "end_line": 37,
    "has_docstring": true,
    "docstring": "Status for a pipeline step.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class StepStatus",
    "component_id": "src.ck_exporter.ui.dashboard.StepStatus"
  },
  "src.ck_exporter.ui.dashboard.DashboardLogHandler": {
    "id": "src.ck_exporter.ui.dashboard.DashboardLogHandler",
    "name": "DashboardLogHandler",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/ui/dashboard.py",
    "relative_path": "src/ck_exporter/ui/dashboard.py",
    "depends_on": [],
    "source_code": "class DashboardLogHandler(logging.Handler):\n    \"\"\"Log handler that captures logs into a ring buffer for dashboard display.\"\"\"\n\n    def __init__(self, ring_buffer_size: int = 100):\n        super().__init__()\n        self.ring_buffer: Deque[str] = deque(maxlen=ring_buffer_size)\n        self.console = Console(stderr=True, width=120)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        \"\"\"Format and store log record in ring buffer.\"\"\"\n        try:\n            # Format as human-readable (not JSON)\n            level_name = record.levelname\n            logger_name = record.name.split(\".\")[-1]  # Just the module name\n            message = record.getMessage()\n\n            # Color coding\n            if level_name == \"ERROR\":\n                formatted = f\"[red]{level_name:8}[/red] {logger_name}: {message}\"\n            elif level_name == \"WARNING\":\n                formatted = f\"[yellow]{level_name:8}[/yellow] {logger_name}: {message}\"\n            elif level_name == \"INFO\":\n                formatted = f\"[cyan]{level_name:8}[/cyan] {logger_name}: {message}\"\n            elif level_name == \"DEBUG\":\n                formatted = f\"[dim]{level_name:8}[/dim] {logger_name}: {message}\"\n            else:\n                formatted = f\"{level_name:8} {logger_name}: {message}\"\n\n            # Add exception traceback if present\n            if record.exc_info:\n                formatted += \"\\n\" + self.format(record)\n\n            self.ring_buffer.append(formatted)\n        except Exception:\n            # Don't let logging errors break the dashboard\n            pass\n\n    def get_recent_logs(self, max_lines: int = 50) -> list[str]:\n        \"\"\"Get the most recent log lines.\"\"\"\n        return list(self.ring_buffer)[-max_lines:]",
    "start_line": 40,
    "end_line": 79,
    "has_docstring": true,
    "docstring": "Log handler that captures logs into a ring buffer for dashboard display.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "logging.Handler"
    ],
    "class_name": null,
    "display_name": "class DashboardLogHandler",
    "component_id": "src.ck_exporter.ui.dashboard.DashboardLogHandler"
  },
  "src.ck_exporter.ui.dashboard.PipelineDashboard": {
    "id": "src.ck_exporter.ui.dashboard.PipelineDashboard",
    "name": "PipelineDashboard",
    "component_type": "class",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/ui/dashboard.py",
    "relative_path": "src/ck_exporter/ui/dashboard.py",
    "depends_on": [
      "src.ck_exporter.ui.dashboard.DashboardLogHandler",
      "src.ck_exporter.ui.dashboard.StepStatus"
    ],
    "source_code": "class PipelineDashboard:\n    \"\"\"Persistent terminal dashboard showing pipeline progress.\"\"\"\n\n    def __init__(self, steps: list[str], log_lines: int = 50):\n        \"\"\"\n        Initialize dashboard.\n\n        Args:\n            steps: List of step names (e.g., [\"Linearize\", \"Extract\", \"Compile\"])\n            log_lines: Number of log lines to show in tail panel\n        \"\"\"\n        self.steps: Dict[str, StepStatus] = {name: StepStatus(name=name) for name in steps}\n        self.log_lines = log_lines\n        self.log_handler: Optional[DashboardLogHandler] = None\n        self.console = Console(stderr=True)\n        self.start_time = time.time()\n        self.overall_elapsed = 0.0\n\n    def set_step_total(self, step_name: str, total: int) -> None:\n        \"\"\"Set total count for a step.\"\"\"\n        if step_name in self.steps:\n            self.steps[step_name].total = total\n            if self.steps[step_name].start_time is None:\n                self.steps[step_name].start_time = time.time()\n\n    def update_step_progress(\n        self, step_name: str, completed: int, current_item: Optional[str] = None, total: Optional[int] = None\n    ) -> None:\n        \"\"\"Update progress for a step.\"\"\"\n        if step_name not in self.steps:\n            return\n\n        step = self.steps[step_name]\n        step.completed = completed\n        step.current_item = current_item\n        \n        # Update total if provided\n        if total is not None and total > 0:\n            step.total = total\n            if step.start_time is None:\n                step.start_time = time.time()\n\n        if step.start_time is not None:\n            step.elapsed = time.time() - step.start_time\n            if step.elapsed > 0 and step.total > 0:\n                step.rate = step.completed / step.elapsed\n                remaining = step.total - step.completed\n                if step.rate > 0:\n                    step.eta = remaining / step.rate\n                else:\n                    step.eta = None\n\n    def set_step_status(self, step_name: str, status: str) -> None:\n        \"\"\"Set status for a step (pending, running, complete, error).\"\"\"\n        if step_name in self.steps:\n            self.steps[step_name].status = status\n            if status == \"running\" and self.steps[step_name].start_time is None:\n                self.steps[step_name].start_time = time.time()\n            elif status in (\"complete\", \"error\"):\n                # Finalize elapsed time\n                if self.steps[step_name].start_time is not None:\n                    self.steps[step_name].elapsed = time.time() - self.steps[step_name].start_time\n\n    def _format_time(self, seconds: Optional[float]) -> str:\n        \"\"\"Format seconds as human-readable time.\"\"\"\n        if seconds is None:\n            return \"\"\n        if seconds < 60:\n            return f\"{seconds:.1f}s\"\n        elif seconds < 3600:\n            return f\"{int(seconds // 60)}m {int(seconds % 60)}s\"\n        else:\n            hours = int(seconds // 3600)\n            minutes = int((seconds % 3600) // 60)\n            return f\"{hours}h {minutes}m\"\n\n    def _create_status_table(self) -> Table:\n        \"\"\"Create status table showing all steps.\"\"\"\n        table = Table(show_header=True, header_style=\"bold\", box=None, padding=(0, 1))\n        table.add_column(\"Step\", style=\"cyan\", width=12)\n        table.add_column(\"Status\", width=10)\n        table.add_column(\"Progress\", width=20)\n        table.add_column(\"Elapsed\", width=10, justify=\"right\")\n        table.add_column(\"Rate\", width=10, justify=\"right\")\n        table.add_column(\"ETA\", width=10, justify=\"right\")\n\n        for step_name, step in self.steps.items():\n            # Status icon\n            if step.status == \"complete\":\n                status_icon = \"[green][/green]\"\n            elif step.status == \"running\":\n                status_icon = \"[yellow][/yellow]\"\n            elif step.status == \"error\":\n                status_icon = \"[red][/red]\"\n            else:\n                status_icon = \"[dim][/dim]\"\n\n            # Progress\n            if step.total > 0:\n                progress_text = f\"{step.completed}/{step.total}\"\n                if step.current_item:\n                    # Truncate long item names\n                    item = step.current_item[:30] + \"...\" if len(step.current_item) > 30 else step.current_item\n                    progress_text += f\" ({item})\"\n            else:\n                progress_text = \"\"\n\n            # Elapsed\n            elapsed_text = self._format_time(step.elapsed if step.status != \"pending\" else None)\n\n            # Rate\n            if step.rate > 0:\n                rate_text = f\"{step.rate:.2f}/s\"\n            else:\n                rate_text = \"\"\n\n            # ETA\n            eta_text = self._format_time(step.eta)\n\n            table.add_row(step_name, status_icon, progress_text, elapsed_text, rate_text, eta_text)\n\n        return table\n\n    def _create_progress_bars(self) -> Progress:\n        \"\"\"Create Rich Progress bars for active steps.\"\"\"\n        progress = Progress(\n            SpinnerColumn(),\n            TextColumn(\"[progress.description]{task.description}\"),\n            BarColumn(),\n            TaskProgressColumn(),\n            TimeElapsedColumn(),\n            console=self.console,\n        )\n\n        for step_name, step in self.steps.items():\n            if step.status == \"running\" and step.total > 0:\n                task_id = progress.add_task(\n                    f\"{step_name}...\",\n                    total=step.total,\n                    completed=step.completed,\n                )\n                # Store task_id for updates (we'll use step_name as key)\n                step._task_id = task_id  # type: ignore\n\n        return progress\n\n    def _create_log_panel(self) -> Panel:\n        \"\"\"Create panel showing recent log lines.\"\"\"\n        if self.log_handler:\n            logs = self.log_handler.get_recent_logs(self.log_lines)\n            log_text = \"\\n\".join(logs) if logs else \"[dim]No logs yet...[/dim]\"\n        else:\n            log_text = \"[dim]Logs not initialized...[/dim]\"\n\n        return Panel(\n            log_text,\n            title=\"[bold]Recent Logs[/bold]\",\n            border_style=\"blue\",\n            height=min(self.log_lines + 2, 20),  # Cap height\n        )\n\n    def render(self) -> RenderableType:\n        \"\"\"Render the complete dashboard.\"\"\"\n        self.overall_elapsed = time.time() - self.start_time\n\n        # Create layout\n        layout = Layout()\n\n        # Top: Status table\n        status_table = self._create_status_table()\n        status_panel = Panel(\n            status_table,\n            title=f\"[bold]CKX Pipeline[/bold]  {self._format_time(self.overall_elapsed)} elapsed\",\n            border_style=\"green\",\n        )\n\n        # Middle: Progress bars (only for running steps)\n        progress_bars = self._create_progress_bars()\n        progress_panel = Panel(progress_bars, border_style=\"yellow\", height=5)\n\n        # Bottom: Log tail\n        log_panel = self._create_log_panel()\n\n        # Arrange vertically\n        layout.split_column(\n            Layout(status_panel, name=\"status\", size=8),\n            Layout(progress_panel, name=\"progress\", size=7),\n            Layout(log_panel, name=\"logs\"),\n        )\n\n        return layout\n\n    def install_log_handler(self) -> None:\n        \"\"\"Install log handler to capture logs.\"\"\"\n        if self.log_handler is None:\n            self.log_handler = DashboardLogHandler(ring_buffer_size=self.log_lines * 2)\n            root_logger = logging.getLogger()\n            root_logger.addHandler(self.log_handler)\n\n    def remove_log_handler(self) -> None:\n        \"\"\"Remove log handler.\"\"\"\n        if self.log_handler:\n            root_logger = logging.getLogger()\n            root_logger.removeHandler(self.log_handler)\n            self.log_handler = None\n\n    def get_progress_callback(self, step_name: str) -> Callable[[int, int, Optional[dict]], None]:\n        \"\"\"Get a progress callback function for a step.\"\"\"\n\n        def callback(completed: int, total: int, context: Optional[dict] = None) -> None:\n            if context is None:\n                context = {}\n            current_item = context.get(\"conversation_id\") if context else None\n            # Update total on first call or when it changes\n            if total > 0:\n                self.set_step_total(step_name, total)\n            self.update_step_progress(step_name, completed, current_item, total=total if total > 0 else None)\n\n        return callback\n\n    def run_live(self, refresh_per_second: float = 4.0) -> Live:\n        \"\"\"Create Live context for updating dashboard.\"\"\"\n        return Live(\n            self,  # Pass self as the renderable, not self.render()\n            console=self.console,\n            refresh_per_second=refresh_per_second,\n            screen=False,\n        )\n    \n    def __rich__(self) -> RenderableType:\n        \"\"\"Rich protocol: render the dashboard.\"\"\"\n        return self.render()",
    "start_line": 82,
    "end_line": 313,
    "has_docstring": true,
    "docstring": "Persistent terminal dashboard showing pipeline progress.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PipelineDashboard",
    "component_id": "src.ck_exporter.ui.dashboard.PipelineDashboard"
  },
  "src.ck_exporter.utils.atom_candidates.normalize_statement": {
    "id": "src.ck_exporter.utils.atom_candidates.normalize_statement",
    "name": "normalize_statement",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/utils/atom_candidates.py",
    "relative_path": "src/ck_exporter/utils/atom_candidates.py",
    "depends_on": [],
    "source_code": "def normalize_statement(text: str) -> str:\n    \"\"\"\n    Normalize a statement/question for deduplication.\n\n    Args:\n        text: Statement or question text\n\n    Returns:\n        Normalized string (lowercase, stripped, basic whitespace normalization)\n    \"\"\"\n    return \" \".join(text.lower().strip().split())",
    "start_line": 7,
    "end_line": 17,
    "has_docstring": true,
    "docstring": "Normalize a statement/question for deduplication.\n\nArgs:\n    text: Statement or question text\n\nReturns:\n    Normalized string (lowercase, stripped, basic whitespace normalization)",
    "parameters": [
      "text"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function normalize_statement",
    "component_id": "src.ck_exporter.utils.atom_candidates.normalize_statement"
  },
  "src.ck_exporter.utils.atom_candidates.deduplicate_candidates": {
    "id": "src.ck_exporter.utils.atom_candidates.deduplicate_candidates",
    "name": "deduplicate_candidates",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/utils/atom_candidates.py",
    "relative_path": "src/ck_exporter/utils/atom_candidates.py",
    "depends_on": [
      "src.ck_exporter.utils.atom_candidates.normalize_statement"
    ],
    "source_code": "def deduplicate_candidates(\n    all_candidates: dict[str, list[dict[str, Any]]],\n    max_evidence_per_item: int | None = None,\n) -> dict[str, list[dict[str, Any]]]:\n    \"\"\"\n    Deduplicate candidate atoms before Pass 2 refinement.\n\n    This reduces the payload size sent to the LLM and improves throughput.\n\n    Args:\n        all_candidates: Dict with \"facts\", \"decisions\", \"open_questions\" lists\n        max_evidence_per_item: Optional cap on evidence count per item (None = no cap)\n\n    Returns:\n        Deduplicated candidates dict with same structure\n    \"\"\"\n    # Get evidence cap from env or use provided default\n    if max_evidence_per_item is None:\n        max_evidence_per_item = int(os.getenv(\"CKX_MAX_EVIDENCE_PER_ITEM\", \"0\")) or None\n\n    result = {\"facts\": [], \"decisions\": [], \"open_questions\": []}\n\n    # Deduplicate facts\n    seen_facts: dict[tuple[str, str, str], dict[str, Any]] = {}\n    for fact in all_candidates.get(\"facts\", []):\n        key = (\n            fact.get(\"type\", \"\"),\n            fact.get(\"topic\", \"\"),\n            normalize_statement(fact.get(\"statement\", \"\")),\n        )\n        if key in seen_facts:\n            # Merge evidence arrays\n            existing_evidence = seen_facts[key].get(\"evidence\", [])\n            new_evidence = fact.get(\"evidence\", [])\n            # Deduplicate evidence by message_id\n            evidence_map = {ev.get(\"message_id\"): ev for ev in existing_evidence}\n            for ev in new_evidence:\n                msg_id = ev.get(\"message_id\")\n                if msg_id and msg_id not in evidence_map:\n                    evidence_map[msg_id] = ev\n            merged_evidence = list(evidence_map.values())\n            # Apply evidence cap if configured\n            if max_evidence_per_item and len(merged_evidence) > max_evidence_per_item:\n                merged_evidence = merged_evidence[:max_evidence_per_item]\n            seen_facts[key][\"evidence\"] = merged_evidence\n        else:\n            # Apply evidence cap if configured\n            evidence = fact.get(\"evidence\", [])\n            if max_evidence_per_item and len(evidence) > max_evidence_per_item:\n                fact = {**fact, \"evidence\": evidence[:max_evidence_per_item]}\n            seen_facts[key] = fact\n    result[\"facts\"] = list(seen_facts.values())\n\n    # Deduplicate decisions\n    seen_decisions: dict[tuple[str, str, str], dict[str, Any]] = {}\n    for decision in all_candidates.get(\"decisions\", []):\n        key = (\n            decision.get(\"type\", \"\"),\n            decision.get(\"topic\", \"\"),\n            normalize_statement(decision.get(\"statement\", \"\")),\n        )\n        if key in seen_decisions:\n            # Merge evidence arrays\n            existing_evidence = seen_decisions[key].get(\"evidence\", [])\n            new_evidence = decision.get(\"evidence\", [])\n            evidence_map = {ev.get(\"message_id\"): ev for ev in existing_evidence}\n            for ev in new_evidence:\n                msg_id = ev.get(\"message_id\")\n                if msg_id and msg_id not in evidence_map:\n                    evidence_map[msg_id] = ev\n            merged_evidence = list(evidence_map.values())\n            if max_evidence_per_item and len(merged_evidence) > max_evidence_per_item:\n                merged_evidence = merged_evidence[:max_evidence_per_item]\n            seen_decisions[key][\"evidence\"] = merged_evidence\n        else:\n            evidence = decision.get(\"evidence\", [])\n            if max_evidence_per_item and len(evidence) > max_evidence_per_item:\n                decision = {**decision, \"evidence\": evidence[:max_evidence_per_item]}\n            seen_decisions[key] = decision\n    result[\"decisions\"] = list(seen_decisions.values())\n\n    # Deduplicate open questions\n    seen_questions: dict[tuple[str, str], dict[str, Any]] = {}\n    for question in all_candidates.get(\"open_questions\", []):\n        key = (\n            question.get(\"topic\", \"\"),\n            normalize_statement(question.get(\"question\", \"\")),\n        )\n        if key in seen_questions:\n            # Merge evidence arrays\n            existing_evidence = seen_questions[key].get(\"evidence\", [])\n            new_evidence = question.get(\"evidence\", [])\n            evidence_map = {ev.get(\"message_id\"): ev for ev in existing_evidence}\n            for ev in new_evidence:\n                msg_id = ev.get(\"message_id\")\n                if msg_id and msg_id not in evidence_map:\n                    evidence_map[msg_id] = ev\n            merged_evidence = list(evidence_map.values())\n            if max_evidence_per_item and len(merged_evidence) > max_evidence_per_item:\n                merged_evidence = merged_evidence[:max_evidence_per_item]\n            seen_questions[key][\"evidence\"] = merged_evidence\n        else:\n            evidence = question.get(\"evidence\", [])\n            if max_evidence_per_item and len(evidence) > max_evidence_per_item:\n                question = {**question, \"evidence\": evidence[:max_evidence_per_item]}\n            seen_questions[key] = question\n    result[\"open_questions\"] = list(seen_questions.values())\n\n    return result",
    "start_line": 20,
    "end_line": 128,
    "has_docstring": true,
    "docstring": "Deduplicate candidate atoms before Pass 2 refinement.\n\nThis reduces the payload size sent to the LLM and improves throughput.\n\nArgs:\n    all_candidates: Dict with \"facts\", \"decisions\", \"open_questions\" lists\n    max_evidence_per_item: Optional cap on evidence count per item (None = no cap)\n\nReturns:\n    Deduplicated candidates dict with same structure",
    "parameters": [
      "all_candidates",
      "max_evidence_per_item"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function deduplicate_candidates",
    "component_id": "src.ck_exporter.utils.atom_candidates.deduplicate_candidates"
  },
  "src.ck_exporter.utils.chunking.estimate_tokens": {
    "id": "src.ck_exporter.utils.chunking.estimate_tokens",
    "name": "estimate_tokens",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/utils/chunking.py",
    "relative_path": "src/ck_exporter/utils/chunking.py",
    "depends_on": [],
    "source_code": "def estimate_tokens(text: str, model: str = \"gpt-5.2\") -> int:\n    \"\"\"Estimate token count for text.\"\"\"\n    if HAS_TIKTOKEN:\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n            return len(encoding.encode(text))\n        except Exception:\n            pass\n\n    # Fallback: rough estimate (1 token  4 characters)\n    return len(text) // 4",
    "start_line": 12,
    "end_line": 22,
    "has_docstring": true,
    "docstring": "Estimate token count for text.",
    "parameters": [
      "text",
      "model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function estimate_tokens",
    "component_id": "src.ck_exporter.utils.chunking.estimate_tokens"
  },
  "src.ck_exporter.utils.chunking.chunk_text": {
    "id": "src.ck_exporter.utils.chunking.chunk_text",
    "name": "chunk_text",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/utils/chunking.py",
    "relative_path": "src/ck_exporter/utils/chunking.py",
    "depends_on": [
      "src.ck_exporter.utils.chunking.estimate_tokens"
    ],
    "source_code": "def chunk_text(\n    text: str,\n    max_tokens: int = 8000,\n    overlap_tokens: int = 200,\n    model: str = \"gpt-5.2\",\n) -> List[str]:\n    \"\"\"\n    Split text into chunks with token-based sizing.\n\n    Args:\n        text: Text to chunk\n        max_tokens: Maximum tokens per chunk\n        overlap_tokens: Token overlap between chunks\n        model: Model name for tokenization\n\n    Returns:\n        List of text chunks\n    \"\"\"\n    if not text.strip():\n        return []\n\n    # Estimate total tokens\n    total_tokens = estimate_tokens(text, model)\n\n    if total_tokens <= max_tokens:\n        return [text]\n\n    chunks = []\n    current_pos = 0\n    text_length = len(text)\n\n    while current_pos < text_length:\n        # Try to find a good break point\n        end_pos = min(current_pos + max_tokens * 4, text_length)  # Rough char estimate\n\n        # If not at end, try to break at sentence boundary\n        if end_pos < text_length:\n            # Look for sentence endings\n            for punct in [\".\\n\\n\", \".\\n\", \". \", \"!\\n\\n\", \"!\\n\", \"! \", \"?\\n\\n\", \"?\\n\", \"? \"]:\n                last_break = text.rfind(punct, current_pos, end_pos)\n                if last_break > current_pos:\n                    end_pos = last_break + len(punct)\n                    break\n\n        chunk = text[current_pos:end_pos].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move forward with overlap\n        if end_pos >= text_length:\n            break\n        current_pos = max(current_pos + 1, end_pos - overlap_tokens * 4)\n\n    return chunks",
    "start_line": 25,
    "end_line": 78,
    "has_docstring": true,
    "docstring": "Split text into chunks with token-based sizing.\n\nArgs:\n    text: Text to chunk\n    max_tokens: Maximum tokens per chunk\n    overlap_tokens: Token overlap between chunks\n    model: Model name for tokenization\n\nReturns:\n    List of text chunks",
    "parameters": [
      "text",
      "max_tokens",
      "overlap_tokens",
      "model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function chunk_text",
    "component_id": "src.ck_exporter.utils.chunking.chunk_text"
  },
  "src.ck_exporter.utils.chunking.chunk_messages": {
    "id": "src.ck_exporter.utils.chunking.chunk_messages",
    "name": "chunk_messages",
    "component_type": "function",
    "file_path": "/Users/stephen/Projects/scratch/chatgpt-conversation-knowledge-exporter/src/ck_exporter/utils/chunking.py",
    "relative_path": "src/ck_exporter/utils/chunking.py",
    "depends_on": [
      "src.ck_exporter.utils.chunking.estimate_tokens"
    ],
    "source_code": "def chunk_messages(\n    messages: List[dict],\n    max_tokens: int = 8000,\n    overlap_tokens: int = 200,\n    model: str = \"gpt-5.2\",\n) -> List[List[dict]]:\n    \"\"\"\n    Chunk a list of messages into groups that fit within token limits.\n\n    Returns:\n        List of message groups (each group is a list of messages)\n    \"\"\"\n    if not messages:\n        return []\n\n    chunks = []\n    current_chunk = []\n    current_tokens = 0\n\n    for msg in messages:\n        msg_text = msg.get(\"text\", \"\")\n        msg_tokens = estimate_tokens(msg_text, model)\n\n        # If single message exceeds limit, add it alone\n        if msg_tokens > max_tokens:\n            if current_chunk:\n                chunks.append(current_chunk)\n                current_chunk = []\n            chunks.append([msg])\n            current_tokens = 0\n            continue\n\n        # Check if adding this message would exceed limit\n        if current_tokens + msg_tokens > max_tokens and current_chunk:\n            chunks.append(current_chunk)\n            current_chunk = []\n            current_tokens = 0\n\n        current_chunk.append(msg)\n        current_tokens += msg_tokens\n\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks",
    "start_line": 81,
    "end_line": 125,
    "has_docstring": true,
    "docstring": "Chunk a list of messages into groups that fit within token limits.\n\nReturns:\n    List of message groups (each group is a list of messages)",
    "parameters": [
      "messages",
      "max_tokens",
      "overlap_tokens",
      "model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function chunk_messages",
    "component_id": "src.ck_exporter.utils.chunking.chunk_messages"
  }
}