# ChatGPT Conversation Knowledge Exporter - Environment Variables
# Copy this file to .env and configure as needed

# ==============================================================================
# API Keys (Required)
# ==============================================================================
# At least one of these must be set:
OPENAI_API_KEY=your_openai_api_key_here
# OR
OPENROUTER_API_KEY=your_openrouter_api_key_here

# ==============================================================================
# OpenRouter Settings (Optional)
# ==============================================================================
# Optional attribution headers (recommended by OpenRouter)
OPENROUTER_HTTP_REFERER=https://your-domain.com
OPENROUTER_X_TITLE=ChatGPT Conversation Knowledge Exporter

# ==============================================================================
# Model Selection
# ==============================================================================
# Models for extraction (Pass 1 = fast, Pass 2 = refinement)
CKX_FAST_MODEL=z-ai/glm-4.7
CKX_BIG_MODEL=z-ai/glm-4.7

# Models for DSPy operations (if using DSPy adapters)
CKX_DSPY_LABEL_MODEL=z-ai/glm-4.7
CKX_DSPY_REFINE_MODEL=z-ai/glm-4.7
CKX_DSPY_MEETING_EXTRACT_MODEL=z-ai/glm-4.7

# ==============================================================================
# Concurrency Settings
# ==============================================================================
# Maximum concurrent conversations to process
CKX_MAX_CONCURRENCY=8

# Maximum concurrent chunks per conversation (Pass 1 extraction)
CKX_CHUNK_MAX_CONCURRENCY=4

# Maximum in-flight LLM requests (defaults to CKX_MAX_CONCURRENCY * 4)
CKX_LLM_MAX_INFLIGHT=

# ==============================================================================
# Token Limits (max_tokens) - Optional
# ==============================================================================
# Per-step token caps for OpenRouter/OpenAI LLM calls
# If unset, no cap is applied (backward compatible)

# Global default for all OpenRouter/OpenAI calls
CKX_LLM_MAX_TOKENS_DEFAULT=

# Per-step limits (override global default):
CKX_LLM_MAX_TOKENS_TOPIC_LABEL=300
CKX_LLM_MAX_TOKENS_EXTRACT_PASS1=1200
CKX_LLM_MAX_TOKENS_EXTRACT_PASS1_REPAIR=800
CKX_LLM_MAX_TOKENS_EXTRACT_PASS2=1600

# Per-step token caps for DSPy LM calls
# If unset, no cap is applied (backward compatible)

# Global default for all DSPy calls
CKX_DSPY_MAX_TOKENS_DEFAULT=

# Per-step limits (override global default):
CKX_DSPY_MAX_TOKENS_TOPIC_LABEL=300
CKX_DSPY_MAX_TOKENS_REFINE_ATOMS=1600
CKX_DSPY_MAX_TOKENS_MEETING_EXTRACT=2000

# ==============================================================================
# Adapter Selection
# ==============================================================================
# Atom extractor implementation: "openrouter" (default) or "dspy" (hybrid)
CKX_ATOM_REFINER_IMPL=openrouter

# Topic labeler implementation: "openrouter" (default) or "dspy"
CKX_TOPIC_LABELER_IMPL=openrouter

# ==============================================================================
# Logging Configuration
# ==============================================================================
# Logging mode: human, hybrid, machine, or auto (default: auto)
CKX_LOG_MODE=auto

# Log format: json, rich, or plain (default: json)
CKX_LOG_FORMAT=json

# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
CKX_LOG_LEVEL=INFO

# Optional file path for log output
CKX_LOG_FILE=

# Third-party library log level (default: WARNING)
CKX_THIRD_PARTY_LOG_LEVEL=WARNING

# Enable/disable hybrid UI mode (default: true)
CKX_HYBRID_UI=true

# ==============================================================================
# LangSmith Tracing (Optional)
# ==============================================================================
# Enable LangSmith tracing: true, 1, or yes (default: disabled)
LANGSMITH_TRACING=false

# LangSmith API key (required if tracing enabled)
LANGSMITH_API_KEY=

# LangSmith project name (default: chatgpt-conversation-knowledge-exporter)
LANGSMITH_PROJECT=chatgpt-conversation-knowledge-exporter

# LangSmith workspace ID (optional, for org-scoped keys)
LANGSMITH_WORKSPACE_ID=

# LangSmith endpoint (default: https://api.smith.langchain.com)
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# ==============================================================================
# Temperature Settings - Optional
# ==============================================================================
# Sampling temperature for LLM generation (0.0-2.0, lower = more deterministic)
# If unset, defaults are used (backward compatible)

# Global default for all OpenRouter/OpenAI calls (default: 0.3)
CKX_LLM_TEMPERATURE_DEFAULT=0.3

# Per-step temperature overrides (override global default):
CKX_LLM_TEMPERATURE_TOPIC_LABEL=0.3
CKX_LLM_TEMPERATURE_EXTRACT_PASS1=0.3
CKX_LLM_TEMPERATURE_EXTRACT_PASS1_REPAIR=0.1
CKX_LLM_TEMPERATURE_EXTRACT_PASS2=0.2
